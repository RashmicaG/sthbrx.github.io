<!DOCTYPE html>
<html lang="en">
<head>
        <title>Store Half Byte-Reverse Indexed - Articles by Daniel Axtens</title>
        <meta charset="utf-8" />
        <link href="https://sthbrx.github.io/atom.xml" type="application/atom+xml" rel="alternate" title="Store Half Byte-Reverse Indexed Full Atom Feed" />
        <link href="https://sthbrx.github.io/rss.xml" type="application/rss+xml" rel="alternate" title="Store Half Byte-Reverse Indexed RSS Feed" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://sthbrx.github.io/">Store Half Byte-Reverse Indexed <strong>A Power Technical Blog</strong></a></h1>
        </header><!-- /#banner -->
        <nav id="menu"><ul>
        </ul></nav><!-- /#menu -->
<section id="content">
<h2>Articles by Daniel Axtens</h2>

<ol id="post-list">
        <li><article class="hentry">
                <header> <h2 class="entry-title"><a href="https://sthbrx.github.io/blog/2017/01/31/linuxconfau-2017-review/" rel="bookmark" title="Permalink to linux.conf.au 2017 review">linux.conf.au 2017 review</a></h2> </header>
                <footer class="post-info">
                    <time class="published" datetime="2017-01-31T16:07:00+11:00"> Tue 31 January 2017 </time>
                    <address class="vcard author">By
                        <a class="url fn" href="https://sthbrx.github.io/author/daniel-axtens.html">Daniel Axtens</a>
                    </address>
                </footer><!-- /.post-info -->
                <div class="entry-content"> <p>I recently attended LCA 2017, where I gave a talk at the Linux Kernel miniconf (run by fellow sthbrx blogger Andrew Donnellan!) and a talk at the main conference.</p>
<p>I received some really interesting feedback so I've taken the opportunity to write some of it down to complement the talk videos and slides that are online. (And to remind me to follow up on it!)</p>
<h2>Miniconf talk: Sparse Warnings</h2>
<p>My kernel miniconf talk was on sparse warnings (<a href="https://github.com/daxtens/sparse-warnings-talk/blob/master/talk.pdf">pdf slides</a>, <a href="https://www.youtube.com/watch?v=hmCukzpevUc">23m video</a>).</p>
<p>The abstract read (in part):</p>
<blockquote>
<p>sparse is a semantic parser for C, and is one of the static analysis tools available to kernel devs.</p>
<p>Sparse is a powerful tool with good integration into the kernel build system. However, we suffer from warning overload - there are too many sparse warnings to spot the serious issues amongst the trivial. This makes it difficult to use, both for developers and maintainers.</p>
</blockquote>
<p>Happily, I received some feedback that suggests it's not all doom and gloom like I had thought!</p>
<ul>
<li>
<p>Dave Chinner told me that the xfs team uses sparse regularly to make sure that the file system is endian-safe. This is good news - we really would like that to be endian-safe!</p>
</li>
<li>
<p>Paul McKenney let me know that the 0day bot does do some sparse checking - it would just seem that it's not done on PowerPC.</p>
</li>
</ul>
<h2>Main talk: 400,000 Ephemeral Containers</h2>
<p>My main talk was entitled "400,000 Ephemeral Containers: testing entire ecosystems with Docker". You can read the <a href="https://linux.conf.au/schedule/presentation/81/">abstract</a> for full details, but it boils down to:</p>
<blockquote>
<p>What if you want to test how <em>all</em> the packages in a given ecosystem work in a given situation?</p>
</blockquote>
<p>My main example was testing how many of the Ruby packages successfully install on Power, but I also talk about other languages and other cool tests you could run.</p>
<p>The <a href="https://www.youtube.com/watch?v=v7wSqOQeGhA">44m video</a> is online. I haven't put the slides up yet but they should be available <a href="https://github.com/daxtens/400000-ephemeral-containers">on GitHub</a> soonish.</p>
<p>Unlike with the kernel talk, I didn't catch the names of most of the people with feedback.</p>
<h3>Docker memory issues</h3>
<p>One of the questions I received during the talk was about running into memory issues in Docker. I attempted to answer that during the Q&amp;A. The person who asked the question then had a chat with me afterwards, and it turns out I had completely misunderstood the question. I thought it was about memory usage of running containers in parallel. It was actually about memory usage in the docker daemon when running lots of containers in serial. Apparently the docker daemon doesn't free memory during the life of the process, and the question was whether or not I had observed that during my runs.</p>
<p>I didn't have a good answer for this at the time other than "it worked for me", so I have gone back and looked at the docker daemon memory usage.</p>
<p>After a full Ruby run, the daemon is using about 13.9G of virtual memory, and 1.975G of resident memory. If I restart it, the memory usage drops to 1.6G of virtual and 43M of resident memory. So it would appear that the person asking the question was right, and I'm just not seeing it have an effect.</p>
<h3>Other interesting feedback</h3>
<ul>
<li>
<p>Someone was quite interested in testing on Sparc, once they got their Go runtime nailed down.</p>
</li>
<li>
<p>A Rackspacer was quite interested in Python testing for OpenStack - this has some intricacies around Py2/Py3, but we had an interesting discussion around just testing to see if packages that claim Py3 support provide Py3 support.</p>
</li>
<li>
<p>A large jobs site mentioned using this technique to help them migrate their dependencies between versions of Go.</p>
</li>
<li>
<p>I was 'gently encouraged' to try to do better with how long the process takes to run - if for no other reason than to avoid burning more coal. This is a fair point. I did not explain very well what I meant with diminishing returns in the talk: there's <em>lots</em> you could do to make the process faster, it's just comes at the cost of the simplicity that I really wanted when I first started the project. I am working (on and off) on better ways to deal with this by considering the dependency graph.</p>
</li>
</ul> </div><!-- /.entry-content -->
        </article></li>
        <li><article class="hentry">
                <header> <h2 class="entry-title"><a href="https://sthbrx.github.io/blog/2016/07/27/get-off-my-lawn-separating-docker-workloads-using-cgroups/" rel="bookmark" title="Permalink to Get off my lawn: separating Docker workloads using cgroups">Get off my lawn: separating Docker workloads using cgroups</a></h2> </header>
                <footer class="post-info">
                    <time class="published" datetime="2016-07-27T13:30:00+10:00"> Wed 27 July 2016 </time>
                    <address class="vcard author">By
                        <a class="url fn" href="https://sthbrx.github.io/author/daniel-axtens.html">Daniel Axtens</a>
                    </address>
                </footer><!-- /.post-info -->
                <div class="entry-content"> <p>On my team, we do two different things in our Continuous Integration setup: build/functional tests, and performance tests. Build tests simply test whether a project builds, and, if the project provides a functional test suite, that the tests pass. We do a lot of MySQL/MariaDB testing this way. The other type of testing we do is performance tests: we build a project and then run a set of benchmarks against it. Python is a good example here.</p>
<p>Build tests want as much grunt as possible. Performance tests, on the other hand, want a stable, isolated environment. Initially, we set up Jenkins so that performance and build tests never ran at the same time. Builds would get the entire machine, and performance tests would never have to share with anyone.</p>
<p>This, while simple and effective, has some downsides. In POWER land, our machines are quite beefy. For example, one of the boxes I use - an S822L - has 4 sockets, each with 4 cores. At SMT-8 (an 8 way split of each core) that gives us 4 x 4 x 8 = 128 threads. It seems wasteful to lock this entire machine - all 128 threads - just so as to isolate a single-threaded test.<sup id="fnref-1"><a class="footnote-ref" href="#fn-1">1</a></sup></p>
<p>So, <strong>can we partition our machine so that we can be running two different sorts of processes in a sufficiently isolated way?</strong></p>
<p>What counts as 'sufficiently isolated'? Well, my performance tests are CPU bound, so I want CPU isolation. I also want memory, and in particular memory bandwith to be isolated. I don't particularly care about IO isolation as my tests aren't IO heavy. Lastly, I have a couple of tests that are very multithreaded, so I'd like to have enough of a machine for those test results to be interesting.</p>
<p>For CPU isolation we have CPU affinity. We can also do something similar with memory. On a POWER8 system, memory is connected to individual P8s, not to some central point. This is a 'Non-Uniform Memory Architecture' (NUMA) setup: the directly attached memory will be very fast for a processor to access, and memory attached to other processors will be slower to access. An accessible guide (with very helpful diagrams!) is <a href="http://www.redbooks.ibm.com/redpapers/pdfs/redp5098.pdf">the relevant RedBook (PDF)</a>, chapter 2.</p>
<p>We could achieve the isolation we want by dividing up CPUs and NUMA nodes between the competing workloads. Fortunately, all of the hardware NUMA information is plumbed nicely into Linux. Each P8 socket gets a corresponding NUMA node. <code>lscpu</code> will tell you what CPUs correspond to which NUMA nodes (although what it calls a CPU we would call a hardware thread). If you install <code>numactl</code>, you can use <code>numactl -H</code> to get even more details.</p>
<p>In our case, the relevant <code>lscpu</code> output is thus:</p>
<div class="highlight"><pre><span></span>NUMA node0 CPU(s):     0-31
NUMA node1 CPU(s):     96-127
NUMA node16 CPU(s):    32-63
NUMA node17 CPU(s):    64-95
</pre></div>


<p>Now all we have to do is find some way to tell Linux to restrict a group of processes to a particular NUMA node and the corresponding CPUs. How? Enter control groups, or <code>cgroups</code> for short. Processes can be put into a cgroup, and then a cgroup controller can control the resouces allocated to the cgroup. Cgroups are hierarchical, and there are controllers for a number of different ways you could control a group of processes. Most helpfully for us, there's one called <code>cpuset</code>, which can control CPU affinity, and restrict memory allocation to a NUMA node.</p>
<p>We then just have to get the processes into the relevant cgroup. Fortunately, Docker is incredibly helpful for this!<sup id="fnref-2"><a class="footnote-ref" href="#fn-2">2</a></sup> Docker containers are put in the <code>docker</code> cgroup. Each container gets it's own cgroup under the docker cgroup, and fortunately Docker deals well with the somewhat broken state of cpuset inheritance.<sup id="fnref-3"><a class="footnote-ref" href="#fn-3">3</a></sup> So it suffices to create a cpuset cgroup for docker, and allocate some resources to it, and Docker will do the rest. Here we'll allocate the last 3 sockets and NUMA nodes to Docker containers:</p>
<div class="highlight"><pre><span></span>cgcreate -g cpuset:docker
<span class="nb">echo</span> <span class="m">32</span>-127 &gt; /sys/fs/cgroup/cpuset/docker/cpuset.cpus
<span class="nb">echo</span> <span class="m">1</span>,16-17 &gt; /sys/fs/cgroup/cpuset/docker/cpuset.mems
<span class="nb">echo</span> <span class="m">1</span> &gt; /sys/fs/cgroup/cpuset/docker/cpuset.mem_hardwall
</pre></div>


<p><code>mem_hardwall</code> prevents memory allocations under docker from spilling over into the one remaining NUMA node.</p>
<p>So, does this work? I created a container with sysbench and then ran the following:</p>
<div class="highlight"><pre><span></span>root@0d3f339d4181:/# sysbench --test<span class="o">=</span>cpu --num-threads<span class="o">=</span><span class="m">128</span> --max-requests<span class="o">=</span><span class="m">10000000</span> run
</pre></div>


<p>Now I've asked for 128 threads, but the cgroup only has CPUs/hwthreads 32-127 allocated. So If I run htop, I shouldn't see any load on CPUs 0-31. What do I actually see?</p>
<p><img alt="htop screenshot, showing load only on CPUs 32-127" src="/images/dja/cgroup1.png"></p>
<p>It works! Now, we create a cgroup for performance tests using the first socket and NUMA node:</p>
<div class="highlight"><pre><span></span>cgcreate -g cpuset:perf-cgroup
<span class="nb">echo</span> <span class="m">0</span>-31 &gt; /sys/fs/cgroup/cpuset/perf-cgroup/cpuset.cpus
<span class="nb">echo</span> <span class="m">0</span> &gt; /sys/fs/cgroup/cpuset/perf-cgroup/cpuset.mems
<span class="nb">echo</span> <span class="m">1</span> &gt; /sys/fs/cgroup/cpuset/perf-cgroup/cpuset.mem_hardwall
</pre></div>


<p>Docker conveniently lets us put new containers under a different cgroup, which means we can simply do:</p>
<div class="highlight"><pre><span></span>dja@p88 ~&gt; docker run -it --rm --cgroup-parent<span class="o">=</span>/perf-cgroup/ ppc64le/ubuntu bash
root@b037049f94de:/# <span class="c1"># ... install sysbench</span>
root@b037049f94de:/# sysbench --test<span class="o">=</span>cpu --num-threads<span class="o">=</span><span class="m">128</span> --max-requests<span class="o">=</span><span class="m">10000000</span> run
</pre></div>


<p>And the result?</p>
<p><img alt="htop screenshot, showing load only on CPUs 0-31" src="/images/dja/cgroup2.png"></p>
<p>It works! My benchmark results also suggest this is sufficient isolation, and the rest of the team is happy to have more build resources to play with.</p>
<p>There are some boring loose ends to tie up: if a build job does anything outside of docker (like clone a git repo), that doesn't come under the docker cgroup, and we have to interact with systemd. Because systemd doesn't know about cpuset, this is <em>quite</em> fiddly. We also want this in a systemd unit so it runs on start up, and we want some code to tear it down. But I'll spare you the gory details.</p>
<p>In summary, cgroups are surprisingly powerful and simple to work with, especially in conjunction with Docker and NUMA on Power!</p>
<div class="footnote">
<hr>
<ol>
<li id="fn-1">
<p>It gets worse! Before the performance test starts, all the running build jobs must drain. If we have 8 Jenkins executors running on the box, and a performance test job is the next in the queue, we have to wait for 8 running jobs to clear. If they all started at different times and have different runtimes, we will inevitably spend a fair chunk of time with the machine at less than full utilisation while we're waiting.&#160;<a class="footnote-backref" href="#fnref-1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn-2">
<p>At least, on Ubuntu 16.04. I haven't tested if this is true anywhere else.&#160;<a class="footnote-backref" href="#fnref-2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn-3">
<p>I hear this is getting better. It is also why systemd hasn't done cpuset inheritance yet.&#160;<a class="footnote-backref" href="#fnref-3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
</ol>
</div> </div><!-- /.entry-content -->
        </article></li>
        <li><article class="hentry">
                <header> <h2 class="entry-title"><a href="https://sthbrx.github.io/blog/2016/03/15/and-now-for-something-completely-different-approximate-computing/" rel="bookmark" title="Permalink to And now for something completely different: approximate computing">And now for something completely different: approximate computing</a></h2> </header>
                <footer class="post-info">
                    <time class="published" datetime="2016-03-15T11:30:00+11:00"> Tue 15 March 2016 </time>
                    <address class="vcard author">By
                        <a class="url fn" href="https://sthbrx.github.io/author/daniel-axtens.html">Daniel Axtens</a>
                    </address>
                </footer><!-- /.post-info -->
                <div class="entry-content"> <p>In early February I had the opportunity to go the the NICTA Systems Summer School, where Cyril and I were invited to represent IBM. There were a number of excellent talks across a huge range of systems related subjects, but the one that has stuck with me the most was a talk given by <a href="http://homes.cs.washington.edu/~luisceze/">Luis Ceze</a>  on a topic called approximate computing. So here, in hopes that you too find it interesting, is a brief run-down on what I learned.</p>
<p>Approximate computing is fundamentally about trading off accuracy for something else - often speed or power consumption. Initially this sounded like a very weird proposition: computers do things like 'running your operating system' and 'reading from and writing to disks': things you need to always be absolutely correct if you want anything vaguely resembling reliability. It turns out that this is actually not as big a roadblock as I had assumed - you can work around it fairly easily.</p>
<p>The model proposed for approximate computing is as follows. You divide your computation up into two classes: 'precise', and 'approximate'. You use 'precise' computations when you need to get exact answers: so for example if you are constructing a JPEG file, you want the JPEG header to be exact. Then you have approximate computations: so for example the contents of your image can be approximate.</p>
<p>For correctness, you have to establish some boundaries: you say that precise data can be used in approximate calculations, but that approximate data isn't allowed to cross back over and pollute precise calculations. This, while intuitively correct, poses some problems in practise: when you want to write out your approximate JPEG data, you need an operation that allows you to 'bless' (or in their terms 'endorse') some approximate data so it can be used in the precise file system operations.</p>
<p>In the talk we were shown an implementation of this model in Java, called <a href="http://sampa.cs.washington.edu/research/approximation/enerj.html">EnerJ</a>. EnerJ allows you to label variables with either <code>@Precise</code> if you're dealing with precise data, or <code>@Approx</code> if you're dealing with approximate data. The compiler was modified so that it would do all sorts of weird things when it knew it was dealing with approximate data: for example, drop loop iterations entirely, do things in entirely non-determistic ways - all sorts of fun stuff. It turns out this works surprisingly well.</p>
<p>However, the approximate computing really shines when you can bring it all the way down to the hardware level. The first thing they tried was a CPU with both 'approximate' and precise execution engines, but this turned out not to have the power savings hoped for. What seemed to work really well was a model where some approximate calculations could be identified ahead of time, and then replaced with neural networks in hardware. These neural networks approximated the calculations, but did so at significantly lower power levels. This sounded like a really promising concept, and it will be interesting to see if this goes anywhere over the next few years.</p>
<p>There's a lot of work evaluating the quality of the approximate result, for cases where the set of inputs is known, and when the inputs is not known. This is largely beyond my understanding, so I'll simply refer you to some of the papers <a href="http://sampa.cs.washington.edu/research/approximation/enerj.html">listed on the website</a>.</p>
<p>The final thing covered in the talk was bringing approximate computing into current paradigms by just being willing to accept higher user-visible error rates. For example, they hacked up a network stack to accept packets with invalid checksums. This has had mixed results so far. A question I had (but didn't get around to asking!) would be whether the mathematical properties of checksums (i.e. that they can correct a certain number of bit errors) could be used to correct some of the errors, rather than just accepting/rejecting them blindly. Perhaps by first attempting to correct errors using the checksums, we will be able to fix the simpler errors, reducing the error rate visible to the user.</p>
<p>Overall, I found the NICTA Systems Summer School to be a really interesting experience (and I hope to blog more about it soon). If you're a university student in Australia, or an academic, see if you can make it in 2017!</p> </div><!-- /.entry-content -->
        </article></li>
        <li><article class="hentry">
                <header> <h2 class="entry-title"><a href="https://sthbrx.github.io/blog/2016/03/15/linuxconfau-2016-a-set-of-brief-thoughts/" rel="bookmark" title="Permalink to linux.conf.au 2016: A set of brief thoughts">linux.conf.au 2016: A set of brief thoughts</a></h2> </header>
                <footer class="post-info">
                    <time class="published" datetime="2016-03-15T11:30:00+11:00"> Tue 15 March 2016 </time>
                    <address class="vcard author">By
                        <a class="url fn" href="https://sthbrx.github.io/author/daniel-axtens.html">Daniel Axtens</a>
                    </address>
                </footer><!-- /.post-info -->
                <div class="entry-content"> <p>Recently most of us attended LCA2016. This is one set of reflections on what we heard and what we've thought since. (Hopefully not the only set of reflections that will be posted on this blog either!)</p>
<p>LCA was 2 days of miniconferences plus 3 days of talks. Here, I've picked some of the more interesting talks I attended, and I've written down some thoughts. If you find the thoughts interesting, you can click through and watch the whole talk video, because LCA is awesome like that.</p>
<h4>Life is better with Rust's community automation</h4>
<p><a href="https://www.youtube.com/watch?v=dIageYT0Vgg">This talk</a> is probably the one that's had the biggest impact on our team so far. We were really impressed by the community automation that Rust has: the way they can respond to pull requests from new community members in a way that lets them keep their code quality high and be nice to everyone at the same time.</p>
<p>The system that they've developed is fascinating (and seems fantastic). However, their system uses pull requests, while we use mailing lists. Pull requests are easy, because github has good hook support, but how do we link mailing lists to an automatic test system?</p>
<p>As it turns out, this is something we're working on: we already have <a href="http://patchwork.ozlabs.org/">Patchwork</a>, and <a href="https://openpower.xyz/">Jenkins</a>: how do we link them? We have something brewing, which we'll open source real soon now - stay tuned!</p>
<h4>Usable formal methods - are we there yet?</h4>
<p>I liked <a href="https://www.youtube.com/watch?v=RxHjhBVOCSU">this talk</a>, as I have a soft spot for formal methods (as I have a soft spot for maths). It covers applying a bunch of static analysis and some of the less intrusive formal methods (in particular <a href="http://www.cprover.org/cbmc/">cbmc</a>) to an operating system kernel. They were looking at eChronos rather than Linux, but it's still quite an interesting set of results.</p>
<p>We've also tried to increase our use of static analysis, which has already found a <a href="http://patchwork.ozlabs.org/patch/580629/">real bug</a>. We're hoping to scale this up, especially the use of sparse and cppcheck, but we're a bit short on developer cycles for it at the moment.</p>
<h4>Adventures in OpenPower Firmware</h4>
<p>Stewart Smith - another OzLabber - gave <a href="https://www.youtube.com/watch?v=a4XGvssR-ag">this talk</a> about, well, OpenPOWER firmware. This is a large part of our lives in OzLabs, so it's a great way to get a picture of what we do each day. It's also a really good explanation of the open source stack we have: a POWER8 CPU runs open-source from the first cycle.</p>
<h4>What Happens When 4096 Cores <code>All Do synchronize_rcu_expedited()</code>?</h4>
<p>Paul McKenney is a parallel programming genius - he literally <a href="https://www.kernel.org/pub/linux/kernel/people/paulmck/perfbook/perfbook.html">'wrote the book'</a> (or at least, wrote <em>a</em> book!) on it. <a href="https://www.youtube.com/watch?v=1nfpjHTWaUc">His talk</a> is - as always - a brain-stretching look at parallel programming within the RCU subsystem of the Linux kernel. In particular, the tree structure for locking that he presents is really interesting and quite a clever way of scaling what at first seems to be a necessarily global lock.</p>
<p>I'd also really recommed <a href="https://www.youtube.com/watch?v=tFmajPt0_hI">RCU Mutation Testing</a>, from the kernel miniconf, also by Paul.</p>
<h4>What I've learned as the kernel docs maintainer</h4>
<p>As an extra bonus: I mention <a href="https://www.youtube.com/watch?v=gsJXf6oSbAE">this talk</a>, just to say "why on earth have we still not fixed the Linux kernel <a href="https://www.kernel.org/doc/linux/README">README</a>"?!!?</p> </div><!-- /.entry-content -->
        </article></li>
        <li><article class="hentry">
                <header> <h2 class="entry-title"><a href="https://sthbrx.github.io/blog/2015/10/30/docker-just-stop-using-aufs/" rel="bookmark" title="Permalink to Docker: Just Stop Using AUFS">Docker: Just Stop Using AUFS</a></h2> </header>
                <footer class="post-info">
                    <time class="published" datetime="2015-10-30T13:30:00+11:00"> Fri 30 October 2015 </time>
                    <address class="vcard author">By
                        <a class="url fn" href="https://sthbrx.github.io/author/daniel-axtens.html">Daniel Axtens</a>
                    </address>
                </footer><!-- /.post-info -->
                <div class="entry-content"> <p>Docker's default storage driver on most Ubuntu installs is AUFS.</p>
<p>Don't use it. Use Overlay instead. Here's why.</p>
<p>First, some background. I'm testing the performance of the basic LAMP
stack on POWER. (LAMP is Linux + Apache + MySQL/MariaDB + PHP, by the
way.) To do more reliable and repeatable tests, I do my builds and
tests in Docker containers. (See <a href="/blog/2015/10/12/a-tale-of-two-dockers/">my previous post</a> for more info.)</p>
<p>Each test downloads the source of Apache, MariaDB and PHP, and builds
them. This should be quick: the POWER8 system I'm building on has 160
hardware threads and 128 GB of memory. But I was finding that it was
only just keeping pace with a 2 core Intel VM on BlueMix.</p>
<p>Why? Well, my first point of call was to observe a compilation under
<code>top</code>. The header is below.</p>
<p><img alt="top header, showing over 70 percent of CPU time spent in the kernel" src="/images/dja/aufs/top-bad.png"></p>
<p>Over 70% of CPU time is spent in the kernel?! That's weird. Let's dig
deeper.</p>
<p>My next port of call for analysis of CPU-bound workloads is
<code>perf</code>. <code>perf top</code> reports astounding quantities of time in
spin-locks:</p>
<p><img alt="display from perf top, showing 80 percent of time in a spinlock" src="/images/dja/aufs/perf-top-spinlock.png"></p>
<p><code>perf top -g</code> gives us some more information: the time is in system
calls. <code>open()</code> and <code>stat()</code> are the key culprits, and we can see a
number of file system functions are in play in the call-chains of the
spinlocks.</p>
<p><img alt="display from perf top -g, showing syscalls and file ops" src="/images/dja/aufs/perf-top-syscalls.png"></p>
<p>Why are open and stat slow? Well, I know that the files are on an AUFS
mount. (<code>docker info</code> will tell you what you're using if you're not
sure.) So, being something of a kernel hacker, I set out to find out
why. This did not go well. AUFS isn't upstream, it's a separate patch
set. Distros have been trying to deprecate it for years. Indeed, RHEL
doesn't ship it. (To it's credit, Docker seems to be trying to move
away from it.)</p>
<p>Wanting to avoid the minor nightmare that is an out-of-tree patchset,
I looked at other storage drivers for Docker. <a href="https://jpetazzo.github.io/assets/2015-03-03-not-so-deep-dive-into-docker-storage-drivers.html">This presentation is particularly good.</a>
My choices are pretty simple: AUFS, btrfs, device-mapper or
Overlay. Overlay was an obvious choice: it doesn't need me to set up
device mapper on a cloud VM, or reformat things as btrfs.</p>
<p>It's also easy to set up on Ubuntu:</p>
<ul>
<li>
<p>export/save any docker containers you care about.</p>
</li>
<li>
<p>add <code>--storage-driver=overlay</code> option to <code>DOCKER_OPTS</code> in <code>/etc/default/docker</code>, and restart docker (<code>service docker restart</code>)</p>
</li>
<li>
<p>import/load the containters you exported</p>
</li>
<li>
<p>verify that things work, then clear away your old storage directory (<code>/var/lib/docker/aufs</code>). </p>
</li>
</ul>
<p>Having moved my base container across, I set off another build.</p>
<p>The first thing I noticed is that images are much slower to create with Overlay. But once that finishes, and a compile starts, things run much better:</p>
<p><img alt="top, showing close to zero system time, and around 90 percent user time" src="/images/dja/aufs/top-good.png"></p>
<p>The compiles went from taking painfully long to astonishingly fast. Winning.</p>
<p>So in conclusion:</p>
<ul>
<li>
<p>If you use Docker for something that involves open()ing or stat()ing files</p>
</li>
<li>
<p>If you want your machine to do real work, rather than spin in spinlocks</p>
</li>
<li>
<p>If you want to use code that's upstream and thus much better supported</p>
</li>
<li>
<p>If you want something less disruptive than the btrfs or dm storage drivers</p>
</li>
</ul>
<p>...then drop AUFS and switch to Overlay today.</p> </div><!-- /.entry-content -->
        </article></li>
</ol><!-- /#posts-list -->
<p class="paginator">
        <a href="https://sthbrx.github.io/author/daniel-axtens.html">&laquo;</a>
    Page 2 / 3
        <a href="https://sthbrx.github.io/author/daniel-axtens3.html">&raquo;</a>
</p>
</section><!-- /#content -->
        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>,
                which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->
        </footer><!-- /#contentinfo -->
</body>
</html>