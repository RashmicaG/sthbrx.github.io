<!DOCTYPE html>
<html lang="en">
<head>
        <title>Store Half Byte-Reverse Indexed - Articles by Rashmica Gupta</title>
        <meta charset="utf-8" />
        <link href="https://sthbrx.github.io/atom.xml" type="application/atom+xml" rel="alternate" title="Store Half Byte-Reverse Indexed Full Atom Feed" />
        <link href="https://sthbrx.github.io/rss.xml" type="application/rss+xml" rel="alternate" title="Store Half Byte-Reverse Indexed RSS Feed" />
</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://sthbrx.github.io/">Store Half Byte-Reverse Indexed <strong>A Power Technical Blog</strong></a></h1>
        </header><!-- /#banner -->
        <nav id="menu"><ul>
        </ul></nav><!-- /#menu -->
<section id="content">
<h2>Articles by Rashmica Gupta</h2>

<ol id="post-list">
        <li><article class="hentry">
                <header> <h2 class="entry-title"><a href="https://sthbrx.github.io/blog/2018/07/27/improving-phoronix-benchmarks/" rel="bookmark" title="Permalink to Improving Phoronix Benchmarks">Improving Phoronix Benchmarks</a></h2> </header>
                <footer class="post-info">
                    <time class="published" datetime="2018-07-27T22:22:00+10:00"> Fri 27 July 2018 </time>
                    <address class="vcard author">By
                        <a class="url fn" href="https://sthbrx.github.io/author/rashmica-gupta.html">Rashmica Gupta</a>
                    </address>
                </footer><!-- /.post-info -->
                <div class="entry-content"> <h3>Intro</h3>
<p><a href="https://www.phoronix.com/scan.php?page=article&amp;item=power9-talos-2&amp;num=1">Benchmarks</a></p>
<h3>OpenMP / Parboil</h3>
<h3>x264 Video Encoding</h3>
<h3>Primesieve</h3>
<h3>FLAC</h3>
<h3>Lame</h3>
<h3>OpenSSL</h3>
<h3>SciKit-Learn</h3>
<h3>Blender</h3> </div><!-- /.entry-content -->
        </article></li>
        <li><article class="hentry">
                <header> <h2 class="entry-title"><a href="https://sthbrx.github.io/blog/2017/02/13/high-power-lustre/" rel="bookmark" title="Permalink to High Power Lustre">High Power Lustre</a></h2> </header>
                <footer class="post-info">
                    <time class="published" datetime="2017-02-13T16:29:00+11:00"> Mon 13 February 2017 </time>
                    <address class="vcard author">By
                        <a class="url fn" href="https://sthbrx.github.io/author/daniel-axtens.html">Daniel Axtens</a>
                        <a class="url fn" href="https://sthbrx.github.io/author/rashmica-gupta.html">Rashmica Gupta</a>
                    </address>
                </footer><!-- /.post-info -->
                <div class="entry-content"> <p>(Most of the hard work here was done by fellow blogger Rashmica - I just verified her instructions and wrote up this post.)</p>
<p><a href="http://lustre.org/">Lustre</a> is a high-performance clustered file system. Traditionally the Lustre client and server have run on x86, but both the server and client will also work on Power. Here's how to get them running.</p>
<h1>Server</h1>
<p>Lustre normally requires a patched 'enterprise' kernel - normally an old RHEL, CentOS or SUSE kernel. We tested with a CentOS 7.3 kernel. We tried to follow <a href="https://wiki.hpdd.intel.com/pages/viewpage.action?pageId=52104622">the Intel instructions</a> for building the kernel as much as possible - any deviations we had to make are listed below.</p>
<h2>Setup quirks</h2>
<p>We are told to edit <code>~/kernel/rpmbuild/SPEC/kernel.spec</code>. This doesn't exist because the directory is <code>SPECS</code> not <code>SPEC</code>: you need to edit <code>~/kernel/rpmbuild/SPECS/kernel.spec</code>.</p>
<p>I also found there was an extra quote mark in the supplied patch script after <code>-lustre.patch</code>. I removed that and ran this instead:</p>
<div class="highlight"><pre><span></span>for patch in $(<span class="err">&lt;</span>&quot;3.10-rhel7.series&quot;); do \
      patch_file=&quot;<span class="nv">$HOME</span>/lustre-release/lustre/kernel_patches/patches/<span class="cp">${</span><span class="n">patch</span><span class="cp">}</span>&quot; \
      cat &quot;<span class="cp">${</span><span class="n">patch_file</span><span class="cp">}</span>&quot; &gt;&gt; <span class="nv">$HOME</span>/lustre-kernel-x86_64-lustre.patch \
done
</pre></div>


<p>The fact that there is 'x86_64' in the patch name doesn't matter as you're about to copy it under a different name to a place where it will be included by the spec file.</p>
<h2>Building for ppc64le</h2>
<p>Building for ppc64le was reasonably straight-forward. I had one small issue:</p>
<div class="highlight"><pre><span></span>[build@dja-centos-guest rpmbuild]$ rpmbuild -bp --target=`uname -m` ./SPECS/kernel.spec
Building target platforms: ppc64le
Building for target ppc64le
error: Failed build dependencies:
       net-tools is needed by kernel-3.10.0-327.36.3.el7.ppc64le
</pre></div>


<p>Fixing this was as simple as a <code>yum install net-tools</code>.</p>
<p>This was sufficient to build the kernel RPMs. I installed them and booted to my patched kernel - so far so good!</p>
<h1>Building the client packages: CentOS</h1>
<p>I then tried to build and install the RPMs from <a href="https://git.hpdd.intel.com/?p=fs/lustre-release.git;a=summary"><code>lustre-release</code></a>. This repository provides the sources required to build the client and utility binaries.</p>
<p><code>./configure</code> and <code>make</code> succeeded, but when I went to install the packages with <code>rpm</code>, I found I was missing some dependencies:</p>
<div class="highlight"><pre><span></span><span class="n">error</span><span class="o">:</span> <span class="n">Failed</span> <span class="n">dependencies</span><span class="o">:</span>
        <span class="n">ldiskfsprogs</span> <span class="o">&gt;=</span> <span class="mf">1.42</span><span class="o">.</span><span class="mi">7</span><span class="o">.</span><span class="na">wc1</span> <span class="k">is</span> <span class="n">needed</span> <span class="n">by</span> <span class="n">kmod</span><span class="o">-</span><span class="n">lustre</span><span class="o">-</span><span class="n">osd</span><span class="o">-</span><span class="n">ldiskfs</span><span class="o">-</span><span class="mf">2.9</span><span class="o">.</span><span class="mi">52</span><span class="n">_60_g1d2fbad_dirty</span><span class="o">-</span><span class="mi">1</span><span class="o">.</span><span class="na">el7</span><span class="o">.</span><span class="na">centos</span><span class="o">.</span><span class="na">ppc64le</span>
    <span class="n">sg3_utils</span> <span class="k">is</span> <span class="n">needed</span> <span class="n">by</span> <span class="n">lustre</span><span class="o">-</span><span class="n">iokit</span><span class="o">-</span><span class="mf">2.9</span><span class="o">.</span><span class="mi">52</span><span class="n">_60_g1d2fbad_dirty</span><span class="o">-</span><span class="mi">1</span><span class="o">.</span><span class="na">el7</span><span class="o">.</span><span class="na">centos</span><span class="o">.</span><span class="na">ppc64le</span>
        <span class="n">attr</span> <span class="k">is</span> <span class="n">needed</span> <span class="n">by</span> <span class="n">lustre</span><span class="o">-</span><span class="n">tests</span><span class="o">-</span><span class="mf">2.9</span><span class="o">.</span><span class="mi">52</span><span class="n">_60_g1d2fbad_dirty</span><span class="o">-</span><span class="mi">1</span><span class="o">.</span><span class="na">el7</span><span class="o">.</span><span class="na">centos</span><span class="o">.</span><span class="na">ppc64le</span>
        <span class="n">lsof</span> <span class="k">is</span> <span class="n">needed</span> <span class="n">by</span> <span class="n">lustre</span><span class="o">-</span><span class="n">tests</span><span class="o">-</span><span class="mf">2.9</span><span class="o">.</span><span class="mi">52</span><span class="n">_60_g1d2fbad_dirty</span><span class="o">-</span><span class="mi">1</span><span class="o">.</span><span class="na">el7</span><span class="o">.</span><span class="na">centos</span><span class="o">.</span><span class="na">ppc64le</span>
</pre></div>


<p>I was able to install <code>sg3_utils</code>, <code>attr</code> and <code>lsof</code>, but I was still missing <code>ldiskfsprogs</code>.</p>
<p>It seems we need the lustre-patched version of <code>e2fsprogs</code> - I found a <a href="https://groups.google.com/forum/#!topic/lustre-discuss-list/U93Ja6Xkxfk">mailing list post</a> to that effect.</p>
<p>So, following the instructions on the walkthrough, I grabbed <a href="https://downloads.hpdd.intel.com/public/e2fsprogs/latest/el7/SRPMS/">the SRPM</a> and installed the dependencies: <code>yum install -y texinfo libblkid-devel libuuid-devel</code></p>
<p>I then tried <code>rpmbuild -ba SPECS/e2fsprogs-RHEL-7.spec</code>. This built but failed tests. Some failed because I ran out of disk space - they were using 10s of gigabytes. I found that there were some comments in the spec file about this with suggested tests to disable, so I did that. Even with that fix, I was still failing two tests:</p>
<ul>
<li><code>f_pgsize_gt_blksize</code>: Intel added this to their fork, and no equivalent exists in the master e2fsprogs branches. This relates to Intel specific assumptions about page sizes which don't hold on Power.</li>
<li><code>f_eofblocks</code>: This may need fixing for large page sizes, see <a href="https://jira.hpdd.intel.com/browse/LU-4677?focusedCommentId=78814&amp;page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-78814">this bug</a>.</li>
</ul>
<p>I disabled the tests by adding the following two lines to the spec file, just before <code>make %{?_smp_mflags} check</code>.</p>
<div class="highlight"><pre><span></span>rm -rf tests/f_pgsize_gt_blksize
rm -rf tests/f_eofblocks
</pre></div>


<p>With those tests disabled I was able to build the packages successfully. I installed them with <code>yum localinstall *1.42.13.wc5*</code> (I needed that rather weird pattern to pick up important RPMs that didn't fit the <code>e2fs*</code> pattern - things like <code>libcom_err</code> and <code>libss</code>)</p>
<p>Following that I went back to the <code>lustre-release</code> build products and was able to successfully run <code>yum localinstall *ppc64le.rpm</code>!</p>
<h1>Testing the server</h1>
<p>After disabling SELinux and rebooting, I ran the test script:</p>
<div class="highlight"><pre><span></span>sudo /usr/lib64/lustre/tests/llmount.sh
</pre></div>


<p>This spat out one scary warning:</p>
<div class="highlight"><pre><span></span>mount.lustre FATAL: unhandled/unloaded fs type 0 &#39;ext3&#39;
</pre></div>


<p>The test did seem to succeed overall, and it would seem that is a <a href="https://jira.hpdd.intel.com/browse/LU-9059">known problem</a>, so I pressed on undeterred.</p>
<p>I then attached a couple of virtual harddrives for the metadata and object store volumes, and having set them up, proceeded to try to mount my freshly minted lustre volume from some clients.</p>
<h1>Testing with a ppc64le client</h1>
<p>My first step was to test whether another ppc64le machine would work as a client.</p>
<p>I tried with an existing Ubuntu 16.04 VM that I use for much of my day to day development.</p>
<p>A quick google suggested that I could grab the <code>lustre-release</code> repository and run <code>make debs</code> to get Debian packages for my system.</p>
<p>I needed the following dependencies:</p>
<div class="highlight"><pre><span></span>sudo apt install module-assistant debhelper dpatch libsnmp-dev quilt
</pre></div>


<p>With those the packages built successfully, and could be easily installed:</p>
<div class="highlight"><pre><span></span>dpkg -i lustre-client-modules-4.4.0-57-generic_2.9.52-60-g1d2fbad-dirty-1_ppc64el.deblustre-utils_2.9.52-60-g1d2fbad-dirty-1_ppc64el.deb
</pre></div>


<p>I tried to connect to the server:</p>
<div class="highlight"><pre><span></span>sudo mount -t lustre $SERVER_IP@tcp:/lustre /lustre/
</pre></div>


<p>Initially I wasn't able to connect to the server at all. I remembered that (unlike Ubuntu), CentOS comes with quite an aggressive firewall by default. I ran the following on the server:</p>
<div class="highlight"><pre><span></span>systemctl stop firewalld
</pre></div>


<p>And voila! I was able to connect, mount the lustre volume, and successfully read and write to it. This is very much an over-the-top hack - I should have poked holes in the firewall to allow just the ports lustre needed. This is left as an exercise for the reader.</p>
<h1>Testing with an x86_64 client</h1>
<p>I then tried to run <code>make debs</code> on my Ubuntu 16.10 x86_64 laptop.</p>
<p>This did not go well - I got the following error:</p>
<div class="highlight"><pre><span></span>liblustreapi.c: In function ‘llapi_get_poollist’:
liblustreapi.c:1201:3: error: ‘readdir_r’ is deprecated [-Werror=deprecated-declarations]
</pre></div>


<p>This looks like one of the new errors introduced in recent GCC versions, and is <a href="https://jira.hpdd.intel.com/browse/LU-8724?focusedCommentId=175244&amp;page=com.atlassian.jira.plugin.system.issuetabpanels%3Acomment-tabpanel#comment-175244">a known bug</a>. To work around it, I found the following stanza in a <code>lustre/autoconf/lustre-core.m4</code>, and removed the <code>-Werror</code>:</p>
<div class="highlight"><pre><span></span>AS_IF([test $target_cpu == &quot;i686&quot; -o $target_cpu == &quot;x86_64&quot;],
        [CFLAGS=&quot;$CFLAGS -Wall -Werror&quot;])
</pre></div>


<p>Even this wasn't enough: I got the following errors:</p>
<div class="highlight"><pre><span></span>/home/dja/dev/lustre-release/debian/tmp/modules-deb/usr_src/modules/lustre/lustre/llite/dcache.c:387:22: error: initialization from incompatible pointer type [-Werror=incompatible-pointer-types]
         .d_compare = ll_dcompare,
                  ^~~~~~~~~~~
/home/dja/dev/lustre-release/debian/tmp/modules-deb/usr_src/modules/lustre/lustre/llite/dcache.c:387:22: note: (near initialization for ‘ll_d_ops.d_compare’)
</pre></div>


<p>I figured this was probably because Ubuntu 16.10 has a 4.8 kernel, and Ubuntu 16.04 has a 4.4 kernel. Work on supporting 4.8 <a href="https://jira.hpdd.intel.com/browse/LU-9003">is ongoing</a>.</p>
<p>Sure enough, when I fired up a 16.04 x86_64 VM with a 4.4 kernel, I was able to build and install fine.</p>
<p>Connecting didn't work first time - the guest failed to mount, but I did get the following helpful error on the server:</p>
<div class="highlight"><pre><span></span><span class="n">LNetError</span><span class="o">:</span> <span class="mi">2595</span><span class="o">:</span><span class="mi">0</span><span class="o">:(</span><span class="n">acceptor</span><span class="o">.</span><span class="na">c</span><span class="o">:</span><span class="mi">406</span><span class="o">:</span><span class="n">lnet_acceptor</span><span class="o">())</span> <span class="n">Refusing</span> <span class="n">connection</span> <span class="n">from</span> <span class="mf">10.61</span><span class="o">.</span><span class="mf">2.227</span><span class="o">:</span> <span class="n">insecure</span> <span class="n">port</span> <span class="mi">1024</span>
</pre></div>


<p>Refusing insecure port 1024 made me thing that perhaps the NATing that qemu was performing for me was interfering - perhaps the server expected to get a connection where the source port was privileged, and qemu wouldn't be able to do that with NAT.</p>
<p>Sure enough, switching NAT to bridging was enough to get the x86 VM to talk to the ppc64le server. I verified that <code>ls</code>, reading and writing all succeeded.</p>
<h1>Next steps</h1>
<p>The obvious next steps are following up the disabled tests in e2fsprogs, and doing a lot of internal performance and functionality testing.</p>
<p>Happily, it looks like Lustre might be in the mainline kernel before too long - parts have already started to go in to staging. This will make our lives a lot easier: for example, the breakage between 4.4 and 4.8 would probably have already been picked up and fixed if it was the main kernel tree rather than an out-of-tree patch set.</p>
<p>In the long run, we'd like to make Lustre on Power just as easy as Lustre on x86. (And, of course, more performant!) We'll keep you up to date!</p>
<p>(Thanks to fellow bloggers Daniel Black and Andrew Donnellan for useful feedback on this post.)</p> </div><!-- /.entry-content -->
        </article></li>
        <li><article class="hentry">
                <header> <h2 class="entry-title"><a href="https://sthbrx.github.io/blog/2017/02/01/namd-on-nvlink/" rel="bookmark" title="Permalink to NAMD on NVLink">NAMD on NVLink</a></h2> </header>
                <footer class="post-info">
                    <time class="published" datetime="2017-02-01T08:32:00+11:00"> Wed 01 February 2017 </time>
                    <address class="vcard author">By
                        <a class="url fn" href="https://sthbrx.github.io/author/daniel-axtens.html">Daniel Axtens</a>
                        <a class="url fn" href="https://sthbrx.github.io/author/rashmica-gupta.html">Rashmica Gupta</a>
                        <a class="url fn" href="https://sthbrx.github.io/author/daniel-black.html">Daniel Black</a>
                    </address>
                </footer><!-- /.post-info -->
                <div class="entry-content"> <p>NAMD is a molecular dynamics program that can use GPU acceleration to speed up its calculations. Recent OpenPOWER machines like the IBM Power Systems S822LC for High Performance Computing (Minsky) come with a new interconnect for GPUs called NVLink, which offers extremely high bandwidth to a number of very powerful Nvidia Pascal P100 GPUs. So they're ideal machines for this sort of workload.</p>
<p>Here's how to set up NAMD 2.12 on your Minsky, and how to debug some common issues. We've targeted this script for CentOS, but we've successfully compiled NAMD on Ubuntu as well.</p>
<h2>Prerequisites</h2>
<h3>GPU Drivers and CUDA</h3>
<p>Firstly, you'll need CUDA and the NVidia drivers.</p>
<p>You can install CUDA by following the instructions on NVidia's <a href="https://developer.nvidia.com/cuda-downloads">CUDA Downloads</a> page.</p>
<div class="highlight"><pre><span></span>yum install epel-release
yum install dkms
# download the rpm from the NVidia website
rpm -i cuda-repo-rhel7-8-0-local-ga2-8.0.54-1.ppc64le.rpm
yum clean expire-cache
yum install cuda
# this will take a while...
</pre></div>


<p>Then, we set up a profile file to automatically load CUDA into our path:</p>
<div class="highlight"><pre><span></span>cat &gt;  /etc/profile.d/cuda_path.sh <span class="err">&lt;</span><span class="nt">&lt;EOF</span>
<span class="err">#</span> <span class="err">From</span> <span class="err">http://developer.download.nvidia.com/compute/cuda/8.0/secure/prod/docs/sidebar/CUDA_Quick_Start_Guide.pdf</span> <span class="err">-</span> <span class="err">4.4.2.1</span>
<span class="err">export</span> <span class="na">PATH=</span><span class="s">/usr/local/cuda-8.0/bin${PATH:+:${PATH}}</span>
<span class="err">export</span> <span class="na">LD_LIBRARY_PATH=</span><span class="s">/usr/local/cuda-8.0/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}</span>
<span class="err">EOF</span>
</pre></div>


<p>Now, open a new terminal session and check to see if it works:</p>
<div class="highlight"><pre><span></span>cuda-install-samples-8.0.sh ~
cd ~/NVIDIA_CUDA-8.0_Samples/1_Utilities/bandwidthTest
make &amp;&amp; ./bandwidthTest
</pre></div>


<p>If you see a figure of ~32GB/s, that means NVLink is working as expected. A figure of ~7-8GB indicates that only PCI is working, and more debugging is required.</p>
<h3>Compilers</h3>
<p>You need a c++ compiler:</p>
<div class="highlight"><pre><span></span>yum install gcc-c++
</pre></div>


<h2>Building NAMD</h2>
<p>Once CUDA and the compilers are installed, building NAMD is reasonably straightforward. The one hitch is that because we're using CUDA 8.0, and the NAMD build scripts assume CUDA 7.5, we need to supply an updated <a href="/images/namd/Linux-POWER.cuda">Linux-POWER.cuda file</a>. (We also enable code generation for the Pascal in this file.)</p>
<p>We've documented the entire process as a script which you can <a href="/images/namd/install-namd.sh">download</a>. We'd recommend executing the commands one by one, but if you're brave you can run the script directly.</p>
<p>The script will fetch NAMD 2.12 and build it for you, but won't install it. It will look for the CUDA override file in the directory you are running the script from, and will automatically move it into the correct place so it is picked up by the build system..</p>
<p>The script compiles for a single multicore machine setup, rather than for a cluster. However, it should be a good start for an Ethernet or Infiniband setup.</p>
<p>If you're doing things by hand, you may see some errors during the compilation of charm - as long as you get <code>charm++ built successfully.</code> at the end, you should be OK.</p>
<h2>Testing NAMD</h2>
<p>We have been testing NAMD using the STMV files available from the <a href="http://www.ks.uiuc.edu/Research/namd/utilities/">NAMD website</a>:</p>
<div class="highlight"><pre><span></span>cd NAMD_2.12_Source/Linux-POWER-g++
wget http://www.ks.uiuc.edu/Research/namd/utilities/stmv.tar.gz
tar -xf stmv.tar.gz
sudo ./charmrun +p80 ./namd2 +pemap 0-159:2 +idlepoll +commthread stmv/stmv.namd
</pre></div>


<p>This binds a namd worker thread to every second hardware thread. This is because hardware threads share resources, so using every hardware thread costs overhead and doesn't give us access to any more physical resources.</p>
<p>You should see messages about finding and using GPUs:</p>
<div class="highlight"><pre><span></span>Pe 0 physical rank 0 binding to CUDA device 0 on &lt;hostname&gt;: &#39;Graphics Device&#39;  Mem: 4042MB  Rev: 6.0
</pre></div>


<p>This should be <em>significantly</em> faster than on non-NVLink machines - we saw a gain of about 2x in speed going from a machine with Nvidia K80s to a Minsky. If things aren't faster for you, let us know!</p>
<h2>Downloads</h2>
<ul>
<li><a href="/images/namd/install-namd.sh">Install script for CentOS</a></li>
<li><a href="/images/namd/Linux-POWER.cuda">Linux-POWER.cuda file</a></li>
</ul>
<h2>Other notes</h2>
<p>Namd requires some libraries, some of which they supply as binary downloads on <a href="http://www.ks.uiuc.edu/Research/namd/libraries/">their website</a>.
Make sure you get the ppc64le versions, not the ppc64 versions, otherwise you'll get errors like:</p>
<div class="highlight"><pre><span></span>/bin/ld: failed to merge target specific data of file .rootdir/tcl/lib/libtcl8.5.a(regfree.o)
/bin/ld: .rootdir/tcl/lib/libtcl8.5.a(regerror.o): compiled for a big endian system and target is little endian
/bin/ld: failed to merge target specific data of file .rootdir/tcl/lib/libtcl8.5.a(regerror.o)
/bin/ld: .rootdir/tcl/lib/libtcl8.5.a(tclAlloc.o): compiled for a big endian system and target is little endian
</pre></div>


<p>The script we supply should get these right automatically.</p> </div><!-- /.entry-content -->
        </article></li>
        <li><article class="hentry">
                <header> <h2 class="entry-title"><a href="https://sthbrx.github.io/blog/2016/06/08/interning-at-ozlabs/" rel="bookmark" title="Permalink to Interning at Ozlabs">Interning at Ozlabs</a></h2> </header>
                <footer class="post-info">
                    <time class="published" datetime="2016-06-08T22:22:00+10:00"> Wed 08 June 2016 </time>
                    <address class="vcard author">By
                        <a class="url fn" href="https://sthbrx.github.io/author/rashmica-gupta.html">Rashmica Gupta</a>
                    </address>
                </footer><!-- /.post-info -->
                <div class="entry-content"> <p>I am sadly coming to the end of my six(ish) month internship with Ozlabs (funded by <a href="https://www.acs.org.au">ACS</a>). So here I am writing about my experience in the hopes that future prospective interns can read about how they should come and work with the previously dubbed Linux Gods.</p>
<h3>What is your background?</h3>
<p>Despite embracing being a nerd at school, my opinion of computers prior to starting my Engineering degree was that they were boring and for geeky boys who didn't want to interact with the 'real' world. However when having to choose a specialisation of Engineering I was drawn towards Computer Systems as everything else seemed obvious * but Computer Systems was this great mystical unknown. </p>
<p>Fast forward three years, and I had seen glimpses into the workings of this magical computer world. I had learnt about transistors, logic gates and opamps; I had designed circuits that actually worked; and I had bashed my head against a wall trying to find obscure bugs. I had dabbled in a range of languages from the low levels of VHDL and embedded C, to the abstract world of Python and Java and delved into the obscure world of declarative prologs and relational reinforcement learning. Now it was time to solidify some of these concepts and get some experience under my belt so I could feel less like a monkey bashing random keys on my keyboard. Enter Ozlabs!</p>
<h3>What did you do at Ozlabs?</h3>
<p>After being handed a nice laptop and the root passwords, I faced the inevitable battle of getting everything setup. With the help of my mentor, the prestigious <a href="http://mpe.github.io/">Michael Ellerman</a>, and various other Ozlabs residents I picked off some low hanging fruit such as removing unused code and tidying up a few things. This allowed me to get familiar with the open-source workflow, the kernel building process, IRC, do more with Git then just push and pull, and <strong>finally</strong> come face-to-face with the seemingly impossible: Vim and virtual machines.</p>
<p>I then got to learn about Transactional Memory (TM) - a way of making a bunch of instructions on one processor appear to be one atomic operation to other processors. I took some old TM tests from Mikey and checked that they did indeed pass and fail when they were supposed to and refurbished them a little, learning how to run kernel self-tests and a bit about powerpc assembly along the way.</p>
<p>Eventually my fear of shell scripts was no match for my desire to be able to build and install a kernel with one command and so I finally got around to writing a build script. Accidentally rebooting a bare-metal machine instead of my VM running on it may have had a significant contribution to this...</p>
<p>The next interesting task I got to tackle was to implement a virtual memory dump that other architectures like x86 have, so we can see how the pages in memory are laid out along with information about these pages. This involved understanding x86's implementation and relating that to POWER's memory management. At Uni I never quite understood the fuss about pages and virtual memory and so it was great to be able to build up an appreciation and play around with page tables, virtual to real addresses, and hashtable.</p>
<p>I then moved onto <a href="https://sthbrx.github.io/blog/2016/05/13/srop-mitigation/">SROP mitigation</a>! After a lot of reading and re-reading, I decided to first understand how to use SROP to make an exploit on POWER which meant some assembly, diving into the signal code and finally meeting and spending time with GDB.  Once again I had x86 code to port over to POWER, the main issue being making sure that I didn't break existing things - aka hours and hours of running the kernel self-tests and the Linux Test Project tests and some more scripting, with the help of <a href="http://blog.christophersmart.com/">Chris Smart</a>, to collate the results.</p>
<p>You can judge all my submitted patches <a href="https://patchwork.ozlabs.org/project/linuxppc-dev/list/?submitter=67695&amp;state=*">here</a>.</p>
<h3>What was your overall experience like at Ozlabs?</h3>
<p>I moved to Canberra shortly after finishing exams and so hadn't had the time to ponder expectations of Ozlabs. Everyone was super friendly and despite being, not just the only female but, the only kiwi among a whoooole lot of Aussies I experienced a distinct lack of discrimination (apart from a bit of banter about accents).</p>
<p>Could I wear my normal clothes (and not stuffy business clothes)? Check. Did I get to work on interesting things? Check. Could I do my work without having to go through lots of unnecessary hoops and what not? Check. Could I develop my own workflow and learn all the things? Check. Did I get to delve into a few different areas? Check. Was I surrounded by super smart people who were willing to help me learn? Check. </p>
<p>All in all, I have had a great time here, learnt so much and you should definitely come and work at Ozlabs! Hopefully you'll see me back on this blog in a few months :)</p>
<p>* <em>My pre-university, perhaps somewhat naiive, opinion: Civil and Mechanical is just physics. Chemical and Materials is just chemistry. Electrical seems interesting but who wants to work with power lines? Biomedical is just math and biology. Software is just abstract high level nonsense. But how a computer works?? That is some magical stuff.</em></p> </div><!-- /.entry-content -->
        </article></li>
        <li><article class="hentry">
                <header> <h2 class="entry-title"><a href="https://sthbrx.github.io/blog/2016/05/13/srop-mitigation/" rel="bookmark" title="Permalink to SROP Mitigation">SROP Mitigation</a></h2> </header>
                <footer class="post-info">
                    <time class="published" datetime="2016-05-13T22:22:00+10:00"> Fri 13 May 2016 </time>
                    <address class="vcard author">By
                        <a class="url fn" href="https://sthbrx.github.io/author/rashmica-gupta.html">Rashmica Gupta</a>
                    </address>
                </footer><!-- /.post-info -->
                <div class="entry-content"> <h2>What is SROP?</h2>
<p>Sigreturn Oriented Programming - a general technique that can be used as an exploit, or as a backdoor to exploit another vulnerability.</p>
<h2>Okay, but what is it?</h2>
<p>Yeah... Let me take you through some relevant background info, where I skimp on the details and give you the general picture.</p>
<p>In Linux, software interrupts are called signals. More about signals <a href="http://www.thegeekstuff.com/2012/03/linux-signals-fundamentals/">here</a>! Generally a signal will convey some information from the kernel and so most signals will have a specific signal handler (some code that deals with the signal) setup.</p>
<p>Signals are asynchronous - ie they can be sent to a process/program at anytime. When a signal arrives for a process, the kernel suspends the process. The kernel then saves the 'context' of the process - all the general purpose registers (GPRs), the stack pointer, the next-instruction pointer etc - into a structure called a 'sigframe'. The sigframe is stored on the stack, and then the kernel runs the signal handler. At the very end of the signal handler, it calls a special system call called 'sigreturn' - indicating to the kernel that the signal has been dealt with. The kernel then grabs the sigframe from the stack, restores the process's context and resumes the execution of the process.</p>
<p>This is the rough mental picture you should have:</p>
<p><img alt="Double Format" src="/images/rashmica/picture.png"></p>
<h2>Okay... but you still haven't explained what SROP is..?</h2>
<p>Well, if you insist...</p>
<p>The above process was designed so that the kernel does not need to keep track of what signals it has delivered. The kernel assumes that the sigframe it takes off the stack was legitimately put there by the kernel because of a signal. This is where we can trick the kernel!</p>
<p>If we can construct a fake sigframe, put it on the stack, and call sigreturn, the kernel will assume that the sigframe is one it put there before and will load the contents of the fake context into the CPU's registers and 'resume' execution from where the fake sigframe tells it to. And that is what SROP is!</p>
<h2>Well that sounds cool, show me!</h2>
<p><strong>Firstly</strong> we have to set up a (valid) sigframe:</p>
<p>By valid sigframe, I mean a sigframe that the kernel will not reject. Luckily most architectures only examine a few parts of the sigframe to determine the validity of it. Unluckily, you will have to dive into the source code to find out which parts of the sigframe you need to set up for your architecture. Have a look in the function which deals with the syscall sigreturn (probably something like sys_sigreturn() ).</p>
<p>For a real time signal on a little endian powerpc 64bit machine, the sigframe looks something like this:</p>
<div class="highlight"><pre><span></span><span class="k">struct</span> <span class="n">rt_sigframe</span> <span class="p">{</span>
        <span class="k">struct</span> <span class="n">ucontext</span> <span class="n">uc</span><span class="p">;</span>
        <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">_unused</span><span class="p">[</span><span class="mi">2</span><span class="p">];</span>
        <span class="kt">unsigned</span> <span class="kt">int</span> <span class="n">tramp</span><span class="p">[</span><span class="n">TRAMP_SIZE</span><span class="p">];</span>
        <span class="k">struct</span> <span class="n">siginfo</span> <span class="n">__user</span> <span class="o">*</span><span class="n">pinfo</span><span class="p">;</span>
        <span class="kt">void</span> <span class="n">__user</span> <span class="o">*</span><span class="n">puc</span><span class="p">;</span>
        <span class="k">struct</span> <span class="n">siginfo</span> <span class="n">info</span><span class="p">;</span>
        <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">user_cookie</span><span class="p">;</span>
        <span class="cm">/* New 64 bit little-endian ABI allows redzone of 512 bytes below sp */</span>
        <span class="kt">char</span> <span class="n">abigap</span><span class="p">[</span><span class="n">USER_REDZONE_SIZE</span><span class="p">];</span>
<span class="p">}</span> <span class="n">__attribute__</span> <span class="p">((</span><span class="n">aligned</span> <span class="p">(</span><span class="mi">16</span><span class="p">)));</span>
</pre></div>


<p>The most important part of the sigframe is the context or ucontext as this contains all the register values that will be written into the CPU's registers when the kernel loads in the sigframe. To minimise potential issues we can copy valid values from the current GPRs into our fake ucontext:</p>
<div class="highlight"><pre><span></span><span class="k">register</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">r1</span> <span class="nf">asm</span><span class="p">(</span><span class="s">&quot;r1&quot;</span><span class="p">);</span>
<span class="k">register</span> <span class="kt">unsigned</span> <span class="kt">long</span> <span class="n">r13</span> <span class="nf">asm</span><span class="p">(</span><span class="s">&quot;r13&quot;</span><span class="p">);</span>
<span class="k">struct</span> <span class="n">ucontext</span> <span class="n">ctx</span> <span class="o">=</span> <span class="p">{</span> <span class="mi">0</span> <span class="p">};</span>

<span class="cm">/* We need a system thread id so copy the one from this process */</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">uc_mcontext</span><span class="p">.</span><span class="n">gp_regs</span><span class="p">[</span><span class="n">PT_R13</span><span class="p">]</span> <span class="o">=</span> <span class="n">r13</span><span class="p">;</span>

<span class="cm">/*  Set the context&#39;s stack pointer to where the current stack pointer is pointing */</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">uc_mcontext</span><span class="p">.</span><span class="n">gp_regs</span><span class="p">[</span><span class="n">PT_R1</span><span class="p">]</span> <span class="o">=</span> <span class="n">r1</span><span class="p">;</span>
</pre></div>


<p>We also need to tell the kernel where to resume execution from. As this is just a test to see if we can successfully get the kernel to resume execution from a fake sigframe we will just point it to a function that prints out some text.</p>
<div class="highlight"><pre><span></span><span class="cm">/* Set the next instruction pointer (NIP) to the code that we want executed */</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">uc_mcontext</span><span class="p">.</span><span class="n">gp_regs</span><span class="p">[</span><span class="n">PT_NIP</span><span class="p">]</span> <span class="o">=</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span><span class="p">)</span> <span class="n">test_function</span><span class="p">;</span>
</pre></div>


<p>For some reason the sys_rt_sigreturn() on little endian powerpc 64bit checks the endianess bit of the ucontext's MSR register, so we need to set that:</p>
<div class="highlight"><pre><span></span><span class="cm">/* Set MSR bit if LE */</span>
<span class="n">ctx</span><span class="p">.</span><span class="n">uc_mcontext</span><span class="p">.</span><span class="n">gp_regs</span><span class="p">[</span><span class="n">PT_MSR</span><span class="p">]</span> <span class="o">=</span> <span class="mh">0x01</span><span class="p">;</span>
</pre></div>


<p>Fun fact: not doing this or setting it to 0 results in the CPU switching from little endian to big endian! For a powerpc machine sys_rt_sigreturn() only examines ucontext, so we do not need to set up a full sigframe.</p>
<p><strong>Secondly</strong> we have to put it on the stack:</p>
<div class="highlight"><pre><span></span><span class="cm">/* Set current stack pointer to our fake context */</span>
<span class="n">r1</span> <span class="o">=</span> <span class="p">(</span><span class="kt">unsigned</span> <span class="kt">long</span><span class="p">)</span> <span class="o">&amp;</span><span class="n">ctx</span><span class="p">;</span>
</pre></div>


<p><strong>Thirdly</strong>, we call sigreturn:</p>
<div class="highlight"><pre><span></span><span class="cm">/* Syscall - NR_rt_sigreturn */</span>
<span class="k">asm</span><span class="p">(</span><span class="s">&quot;li 0, 172</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
<span class="k">asm</span><span class="p">(</span><span class="s">&quot;sc</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">);</span>
</pre></div>


<p>When the kernel receives the sigreturn call, it looks at the userspace stack pointer for the ucontext and loads this in. As we have put valid values in the ucontext, the kernel assumes that this is a valid sigframe that it set up earlier and loads the contents of the ucontext in the CPU's registers "and resumes" execution of the process from the address we pointed the NIP to.</p>
<p>Obviously, you need something worth executing at this address, but sadly that next part is not in my job description. This is a nice gateway into the kernel though and would pair nicely with another kernel vulnerability.  If you are interested in some more in depth examples, have a read of <a href="http://www.cs.vu.nl/~herbertb/papers/srop_sp14.pdf">this</a> paper.</p>
<h2>So how can we mitigate this?</h2>
<p>Well, I'm glad you asked. We need some way of distinguishing between sigframes that were put there legitimately by the kernel and 'fake' sigframes. The current idea that is being thrown around is cookies, and you can see the x86 discussion <a href="https://lkml.org/lkml/2016/3/29/788">here</a>.</p>
<p>The proposed solution is to give every sighand struct a randomly generated value. When the kernel constructs a sigframe for a process, it stores a 'cookie' with the sigframe. The cookie is a hash of the cookie's location and the random value stored in the sighand struct for the process. When the kernel receives a sigreturn, it hashes the location where the cookie should be with the randomly generated number in sighand struct - if this matches the cookie, the cookie is zeroed,  the sigframe is valid and the kernel will restore this context.  If the cookies do not match, the sigframe is not restored.</p>
<p>Potential issues:</p>
<ul>
<li>Multithreading: Originally the random number was suggested to be stored in the task struct. However, this would break multi-threaded applications as every thread has its own task struct. As the sighand struct is shared by threads, this should not adversely affect multithreaded applications.</li>
<li>Cookie location: At first I put the cookie on top of the sigframe. However some code in userspace assumed that all the space between the signal handler and the sigframe  was essentially up for grabs and would zero the cookie before I could read the cookie value. Putting the cookie below the sigframe was also a no-go due to the ABI-gap (a gap below the stack pointer that signal code cannot touch) being a part of the sigframe. Putting the cookie inside the sigframe, just above the ABI gap has been fine with all the tests I have run so far!</li>
<li>Movement of sigframe: If you move the sigframe on the stack, the cookie value will no longer be valid... I don't think that this is something that you should be doing, and have not yet come across a scenario that does this. </li>
</ul>
<p>For a more in-depth explanation of SROP, click <a href="https://lwn.net/Articles/676803/">here</a>.</p> </div><!-- /.entry-content -->
        </article></li>
</ol><!-- /#posts-list -->
</section><!-- /#content -->
        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>,
                which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->
        </footer><!-- /#contentinfo -->
</body>
</html>