<!DOCTYPE html>
<html lang="en">
<head>
          <title>Store Half Byte-Reverse Indexed</title>
        <meta charset="utf-8" />
        <link href="https://sthbrx.github.io/atom.xml" type="application/atom+xml" rel="alternate" title="Store Half Byte-Reverse Indexed Full Atom Feed" />
        <link href="https://sthbrx.github.io/rss.xml" type="application/rss+xml" rel="alternate" title="Store Half Byte-Reverse Indexed RSS Feed" />


    <meta name="tags" content="ceph" />
    <meta name="tags" content="raid" />
    <meta name="tags" content="storage" />

</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://sthbrx.github.io/">Store Half Byte-Reverse Indexed <strong>A Power Technical Blog</strong></a></h1>
        </header><!-- /#banner -->
        <nav id="menu"><ul>
        </ul></nav><!-- /#menu -->
<section id="content" class="body">
  <header>
    <h2 class="entry-title">
      <a href="https://sthbrx.github.io/blog/2017/03/29/evaluating-cephfs-on-power/" rel="bookmark"
         title="Permalink to Evaluating CephFS on Power">Evaluating CephFS on Power</a></h2>
 
  </header>
  <footer class="post-info">
    <time class="published" datetime="2017-03-29T00:00:00+11:00">
      Wed 29 March 2017
    </time>
    <address class="vcard author">
      By           <a class="url fn" href="https://sthbrx.github.io/author/alastair-dsilva.html">Alastair D'Silva</a>
    </address>
  </footer><!-- /.post-info -->
  <div class="entry-content">
    <h2>Methodology</h2>
<p>To evaluate CephFS, we will create a ppc64le virtual machine, with sufficient
space to compile the software, as well as 3 sparse 1TB disks to create the
object store.</p>
<p>We will then build &amp; install the Ceph packages, after adding the PowerPC
optimisiations to the code. This is done, as ceph-deploy will fetch prebuilt
packages that do not have the performance patches if the packages are not
installed.</p>
<p>Finally, we will use the ceph-deploy to deploy the instance. We will ceph-deploy
via pip, to avoid file conflicts with the packages that we built.</p>
<p>For more information on what each command does, visit the following tutorial,
upon which which this is based:
<a href="http://palmerville.github.io/2016/04/30/single-node-ceph-install.html">http://palmerville.github.io/2016/04/30/single-node-ceph-install.html</a></p>
<h3>Virtual Machine Config</h3>
<p>Create a virtual machine with at least the following:
 - 16GB of memory
 - 16 CPUs
 - 64GB disk for the root filesystem
 - 3 x 1TB for the Ceph object store
 - Ubuntu 16.04 default install (only use the 64GB disk, leave the others unpartitioned)</p>
<h3>Initial config</h3>
<ul>
<li>Enable ssh</li>
</ul>
<div class="highlight"><pre><span></span>    sudo apt install openssh-server
    sudo apt update
    sudo apt upgrade
    sudo reboot
</pre></div>


<ul>
<li>Install build tools</li>
</ul>
<div class="highlight"><pre><span></span>    sudo apt install git debhelper
</pre></div>


<h3>Build Ceph</h3>
<ul>
<li>Clone the Ceph repo by following the instructions here: <a href="http://docs.ceph.com/docs/master/install/clone-source/">http://docs.ceph.com/docs/master/install/clone-source/</a></li>
</ul>
<div class="highlight"><pre><span></span>    mkdir $HOME/src
    cd $HOME/src
    git clone --recursive https://github.com/ceph/ceph.git  # This may take a while
    cd ceph
    git checkout master
    git submodule update --force --init --recursive
</pre></div>


<ul>
<li>Cherry-pick the Power performance patches:</li>
</ul>
<div class="highlight"><pre><span></span>    git remote add kestrels https://github.com/kestrels/ceph.git
    git fetch --all
    git cherry-pick 59bed55a676ebbe3ad97d8ec005c2088553e4e11
</pre></div>


<ul>
<li>Install prerequisites</li>
</ul>
<div class="highlight"><pre><span></span>    ./install-deps.sh
    sudo apt install python-requests python-flask resource-agents curl python-cherrypy python3-pip python-django python-dateutil python-djangorestframework
    sudo pip3 install ceph-deploy
</pre></div>


<ul>
<li>Build the packages as per the instructions: <a href="http://docs.ceph.com/docs/master/install/build-ceph/">http://docs.ceph.com/docs/master/install/build-ceph/</a></li>
</ul>
<div class="highlight"><pre><span></span>    cd $HOME/src/ceph
    sudo dpkg-buildpackage -J$(nproc) # This will take a couple of hours (16 cpus)
</pre></div>


<ul>
<li>Install the packages (note that python3-ceph-argparse will fail, but is safe to ignore)</li>
</ul>
<div class="highlight"><pre><span></span>    cd $HOME/src
    sudo dpkg -i *.deb
</pre></div>


<h3>Create the ceph-deploy user</h3>
<div class="highlight"><pre><span></span>    sudo adduser ceph-deploy
    echo &quot;ceph-deploy ALL = (root) NOPASSWD:ALL&quot; | sudo tee /etc/sudoers.d/ceph-deploy
    sudo chmod 0440 /etc/sudoers.d/ceph-deploy
</pre></div>


<h3>Configure the ceph-deploy user environment</h3>
<div class="highlight"><pre><span></span>    su - ceph-deploy
    ssh-keygen
    node=`hostname`
    ssh-copy-id ceph-deploy@$node
    mkdir $HOME/ceph-cluster
    cd $HOME/ceph-cluster
    ceph-deploy new $node # If this fails, remove the bogus 127.0.1.1 entry from /etc/hosts
    echo &#39;osd pool default size = 2&#39; &gt;&gt; ceph.conf
    echo &#39;osd crush chooseleaf type = 0&#39; &gt;&gt; ceph.conf
</pre></div>


<h3>Complete the Ceph deployment</h3>
<div class="highlight"><pre><span></span>    ceph-deploy install <span class="nv">$node</span>
    ceph-deploy mon create-initial
    drives=&quot;vda vdb vdc&quot;  # the 1TB drives - check that these are correct for your system
    for drive in <span class="nv">$drives</span>; do ceph-deploy disk zap <span class="nv">$node</span>:<span class="nv">$drive</span>; ceph-deploy osd prepare <span class="nv">$node</span>:<span class="nv">$drive</span>; done
    for drive in <span class="nv">$drives</span>; do ceph-deploy osd activate <span class="nv">$node</span>:/dev/<span class="cp">${</span><span class="n">drive</span><span class="cp">}</span>1; done
    ceph-deploy admin <span class="nv">$node</span>
    sudo chmod +r /etc/ceph/ceph.client.admin.keyring
    ceph -s # Check the state of the cluster
</pre></div>


<h3>Configure CephFS</h3>
<div class="highlight"><pre><span></span>    ceph-deploy mds create $node
    ceph osd pool create cephfs_data 128
    ceph osd pool create cephfs_metadata 128
    ceph fs new cephfs cephfs_metadata cephfs_data
    sudo systemctl status ceph\*.service ceph\*.target # Ensure the ceph-osd, ceph-mon &amp; ceph-mds daemons are running
    sudo mkdir /mnt/cephfs
    key=`grep key ~/ceph-cluster/ceph.client.admin.keyring | cut -d &#39; &#39; -f 3`
    sudo mount -t ceph $node:6789:/ /mnt/cephfs -o name=admin,secret=$key
</pre></div>


<h2>References</h2>
<ol>
<li><a href="http://docs.ceph.com/docs/master/install/clone-source/">http://docs.ceph.com/docs/master/install/clone-source/</a></li>
<li><a href="http://docs.ceph.com/docs/master/install/build-ceph/">http://docs.ceph.com/docs/master/install/build-ceph/</a></li>
<li><a href="http://palmerville.github.io/2016/04/30/single-node-ceph-install.html">http://palmerville.github.io/2016/04/30/single-node-ceph-install.html</a></li>
</ol>
  </div><!-- /.entry-content -->
</section>
        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>,
                which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->
        </footer><!-- /#contentinfo -->
</body>
</html>