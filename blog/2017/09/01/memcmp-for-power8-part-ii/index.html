<!DOCTYPE html>
<html lang="en">
<head>
          <title>Store Half Byte-Reverse Indexed</title>
        <meta charset="utf-8" />
        <link href="https://sthbrx.github.io/atom.xml" type="application/atom+xml" rel="alternate" title="Store Half Byte-Reverse Indexed Full Atom Feed" />
        <link href="https://sthbrx.github.io/rss.xml" type="application/rss+xml" rel="alternate" title="Store Half Byte-Reverse Indexed RSS Feed" />


    <meta name="tags" content="performance" />
    <meta name="tags" content="power" />

</head>

<body id="index" class="home">
        <header id="banner" class="body">
                <h1><a href="https://sthbrx.github.io/">Store Half Byte-Reverse Indexed <strong>A Power Technical Blog</strong></a></h1>
        </header><!-- /#banner -->
        <nav id="menu"><ul>
        </ul></nav><!-- /#menu -->
<section id="content" class="body">
  <header>
    <h2 class="entry-title">
      <a href="https://sthbrx.github.io/blog/2017/09/01/memcmp-for-power8-part-ii/" rel="bookmark"
         title="Permalink to memcmp() for POWER8 - part II">memcmp() for POWER8 - part II</a></h2>
 
  </header>
  <footer class="post-info">
    <time class="published" datetime="2017-09-01T12:00:00+10:00">
      Fri 01 September 2017
    </time>
    <time class="modified" datetime="2017-09-01T12:00:00+10:00">
      Fri 01 September 2017
    </time>
    <address class="vcard author">
      By           <a class="url fn" href="https://sthbrx.github.io/author/cyril-bur.html">Cyril Bur</a>
    </address>
  </footer><!-- /.post-info -->
  <div class="entry-content">
    <p>This entry is a followup to part I which you should absolutely read
<a href="https://sthbrx.github.io/blog/2017/08/07/memcmp-for-power8/">here</a> before continuing
on.</p>
<h2>Where we left off</h2>
<p>We concluded that while a vectorised <code>memcmp()</code> is a win, there are
some cases where it won't quite perform.</p>
<h2>The overhead of enabling ALTIVEC</h2>
<p>In the kernel we explicitly don't touch ALTIVEC unless we need to,
this means that in the general case we can leave the userspace
registers in place and not have do anything to service a syscall for a
process.</p>
<p>This means that if we do want to use ALTIVEC in the kernel, there is
some setup that must be done. Notably, we must enable the facility (a
potentially time consuming move to MSR), save off the registers (if
userspace we using them) and an inevitable restore later on.</p>
<p>If all this needs to be done for a <code>memcmp()</code> in the order of tens of
bytes then it really wasn't worth it.</p>
<p>There are two reasons that <code>memcmp()</code> might go for a small number of
bytes, firstly and trivially detectable is simply that parameter n is
small. The other is harder to detect, if the memcmp() is going to fail
(return non zero) early then it also wasn't worth enabling ALTIVEC.</p>
<h2>Detecting early failures</h2>
<p>Right at the start of <code>memcmp()</code>, before enabling ALTIVEC, the first
64 bytes are checked using general purpose registers. Why the first 64
bytes, well why not? In a strange twist of fate 64 bytes happens to be
the amount of bytes in four ALTIVEC registers (128 bits per register,
so 16 bytes multiplied by 4) and by utter coincidence that happens to
be the stride of the ALTIVEC compare loop.</p>
<h2>What does this all look like</h2>
<p>Well unlike part I the results appear slightly less consistent across
three runs of measurement but there are some very key differences with
part I. The trends do appear to be the same across all three runs,
just less pronounced - why this is is unclear.</p>
<p>The difference between run two and run three clipped at deltas of
1000ns is interesting:
<img alt="Sample 2: Deltas below 1000ns" src="/images/power8_memcmp/v2deltas2-1000.png" title="Sample 2: Deltas below 1000ns"></p>
<p>vs</p>
<p><img alt="Sample 3: Deltas below 1000ns" src="/images/power8_memcmp/v2deltas3-1000.png" title="Sample 3: Deltas below 1000ns"></p>
<p>The results are similar except for a spike in the amount of deltas in
the unpatched kernel at around 600ns. This is not present in the first
sample (deltas1) of data. There are a number of reasons why this spike
could have appeared here, it is possible that the kernel or hardware
did something under the hood, prefetch could have brought deltas for a
<code>memcmp()</code> that would otherwise have yielded a greater delta into the
600ns range.</p>
<p>What these two graphs do both demonstrate quite clearly is that
optimisations down at the sub 100ns end have resulted in more sub
100ns deltas for the patched kernel, a significant win over the
original data. Zooming out and looking at a graph which includes
deltas up to 5000ns shows that the sub 100ns delta optimisations
haven't noticeably slowed the performance of long duration <code>memcmp()</code>,
<img alt="Samply 2: Deltas below 5000ns" src="/images/power8_memcmp/v2deltas2-5000.png" title="Sample 2: Deltas below 5000ns">.</p>
<h2>Conclusion</h2>
<p>The small amount of extra development effort has yielded tangible
results in reducing the low end <code>memcmp()</code> times. This second round of
data collection and performance analysis only confirms the that for
any significant amount of comparison, a vectorised loop is
significantly quicker.</p>
<p>The results obtained here show no downside to adopting this approach
for all power8 and onwards chips as this new version of the patch
solves the performance regression for small compares.</p>
  </div><!-- /.entry-content -->
</section>
        <footer id="contentinfo" class="body">
                <address id="about" class="vcard body">
                Proudly powered by <a href="http://getpelican.com/">Pelican</a>,
                which takes great advantage of <a href="http://python.org">Python</a>.
                </address><!-- /#about -->
        </footer><!-- /#contentinfo -->
</body>
</html>