<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom"><title>Store Half Byte-Reverse Indexed - Development</title><link href="http://sthbrx.github.io/" rel="alternate"></link><link href="https://sthbrx.github.io/feeds/development.atom.xml" rel="self"></link><id>http://sthbrx.github.io/</id><updated>2017-07-17T10:08:00+10:00</updated><entry><title>XDP on Power</title><link href="http://sthbrx.github.io/blog/2017/07/17/xdp-on-power/" rel="alternate"></link><published>2017-07-17T10:08:00+10:00</published><updated>2017-07-17T10:08:00+10:00</updated><author><name>Daniel Axtens</name></author><id>tag:sthbrx.github.io,2017-07-17:/blog/2017/07/17/xdp-on-power/</id><summary type="html">&lt;p&gt;This post is a bit of a break from the standard IBM fare of this blog,
as I now work for Canonical. But I have a soft spot for Power from my
time at IBM - and Canonical officially supports 64-bit, little-endian
Power - so when I get a spare moment I try to make sure that cool,
officially-supported technologies work on Power &lt;em&gt;before&lt;/em&gt; we end up
with a customer emergency! So, without further ado, this is the story
of XDP on Power.&lt;/p&gt;
&lt;h2&gt;XDP&lt;/h2&gt;
&lt;p&gt;eXpress Data Path (XDP) is a cool Linux technology to allow really
fast processing of network packets.&lt;/p&gt;
&lt;p&gt;Normally in Linux, a packet is received by the network card, an SKB
(&lt;a href="http://vger.kernel.org/~davem/skb.html"&gt;socket buffer&lt;/a&gt;) is
allocated, and the packet is passed up through the networking stack.&lt;/p&gt;
&lt;p&gt;This introduces an inescapable latency penalty: we have to allocate
some memory and copy stuff around. XDP allows some network cards and
drivers to process packets early - even before the allocation of the
SKB. This is much faster, and so has applications in DDOS mitigation
and other high-speed networking use-cases. The IOVisor project has
&lt;a href="https://www.iovisor.org/technology/xdp"&gt;much more information&lt;/a&gt; if you
want to learn more.&lt;/p&gt;
&lt;h2&gt;eBPF&lt;/h2&gt;
&lt;p&gt;XDP processing is done by an eBPF program. eBPF - the extended
Berkeley Packet Filter - is an in-kernel virtual machine with a
limited set of instructions. The kernel can statically validate eBPF
programs to ensure that they terminate and are memory safe. From this
it follows that the programs cannot be Turing-complete: they do not
have backward branches, so they cannot do fancy things like
loops. Nonetheless, they're surprisingly powerful for packet
processing and tracing. eBPF programs are translated into efficient
machine code using in-kernel JIT compilers on many platforms, and
interpreted on platforms that do not have a JIT. (Yes, there are
multiple JIT implementations in the kernel. I find this a terrifying
thought.)&lt;/p&gt;
&lt;p&gt;Rather than requiring people to write raw eBPF programs, you can write
them in a somewhat-restricted subset of C, and use Clang's eBPF target
to translate them. This is super handy, as it gives you access to the
kernel headers - which define a number of useful data structures like
headers for various network protocols.&lt;/p&gt;
&lt;h2&gt;Trying it&lt;/h2&gt;
&lt;p&gt;There are a few really interesting project that are already up and
running that allow you to explore XDP without learning the innards of
both eBPF and the kernel networking stack. I explored the samples in
the &lt;a href="https://github.com/iovisor/bcc"&gt;bcc compiler collection&lt;/a&gt; and also
the samples from the &lt;a href="https://github.com/netoptimizer/prototype-kernel/"&gt;netoptimizer/prototype-kernel repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The easiest way to get started with these is with a virtual machine,
as recent virtio network drivers support XDP. If you are using Ubuntu,
you can use the &lt;a href="https://help.ubuntu.com/lts/serverguide/cloud-images-and-uvtool.html"&gt;uvt-kvm
tooling&lt;/a&gt;
to trivially set up a VM running Ubuntu Zesty on your local machine.&lt;/p&gt;
&lt;p&gt;Once your VM is installed, you need to shut it down and edit the virsh XML. &lt;/p&gt;
&lt;p&gt;You need 2 vCPUs (or more) and a virtio+vhost network card. You also
need to edit the 'interface' section and add the following snippet
(with thanks to the &lt;a href="https://www.spinics.net/lists/xdp-newbies/msg00029.html"&gt;xdp-newbies
list&lt;/a&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;driver&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;vhost&amp;#39;&lt;/span&gt; &lt;span class="na"&gt;queues=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;4&amp;#39;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;host&lt;/span&gt; &lt;span class="na"&gt;tso4=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;off&amp;#39;&lt;/span&gt; &lt;span class="na"&gt;tso6=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;off&amp;#39;&lt;/span&gt; &lt;span class="na"&gt;ecn=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;off&amp;#39;&lt;/span&gt; &lt;span class="na"&gt;ufo=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;off&amp;#39;&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;guest&lt;/span&gt; &lt;span class="na"&gt;tso4=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;off&amp;#39;&lt;/span&gt; &lt;span class="na"&gt;tso6=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;off&amp;#39;&lt;/span&gt; &lt;span class="na"&gt;ecn=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;off&amp;#39;&lt;/span&gt; &lt;span class="na"&gt;ufo=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;off&amp;#39;&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/driver&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(If you have more than 2 vCPUs, set the queues parameter to 2x the
number of vCPUs.)&lt;/p&gt;
&lt;p&gt;Then, install a modern clang (we've had issues with 3.8 - I recommend
v4+), and the usual build tools.&lt;/p&gt;
&lt;p&gt;I recommend testing with the prototype-kernel tools - the DDOS
prevention tool is a good demo. Then - on x86 - you just follow their
instructions. I'm not going to repeat that here.&lt;/p&gt;
&lt;h2&gt;POWERful XDP&lt;/h2&gt;
&lt;p&gt;What happens when you try this on Power? Regular readers of my posts
will know to expect some
&lt;a href="https://sthbrx.github.io/blog/2017/02/13/high-power-lustre/"&gt;minor&lt;/a&gt;
&lt;a href="https://sthbrx.github.io/blog/2017/02/01/namd-on-nvlink/"&gt;hitches&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;XDP does not disappoint.&lt;/p&gt;
&lt;p&gt;Firstly, the prototype-kernel repository &lt;a href="https://github.com/netoptimizer/prototype-kernel/blob/master/kernel/samples/bpf/Makefile#L92"&gt;hard codes x86&lt;/a&gt;
as the architecture for kernel headers. You need to change it for
powerpc.&lt;/p&gt;
&lt;p&gt;Then, once you get the stuff compiled, and try to run it on a
current-at-time-of-writing Zesty kernel, you'll hit a massive debug
splat ending in:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;32: (61) r1 = *(u32 *)(r8 +12)
misaligned packet access off 0+18+12 size 4
load_bpf_file: Permission denied
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It turns out this is because in Ubuntu's Zesty kernel,
CONFIG_HAS_EFFICIENT_UNALIGNED_ACCESS is not set on ppc64el. Because
of that, the eBPF verifier will check that all loads are aligned - and
this load (part of checking some packet header) is not, and so the
verifier rejects the program. Unaligned access is not enabled because
the Zesty kernel is being compiled for CPU_POWER7 instead of
CPU_POWER8, and we don't have efficient unaligned access on POWER7.&lt;/p&gt;
&lt;p&gt;As it turns out, IBM never released any officially supported Power7 LE
systems - LE was only ever supported on Power8. So, I &lt;a href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1699627"&gt;filed a bug&lt;/a&gt; and
&lt;a href="https://lists.ubuntu.com/archives/kernel-team/2017-June/085074.html"&gt;sent a patch&lt;/a&gt;
to build Zesty kernels for POWER8 instead, and that has been accepted
and will be part of the next stable update due real soon now.&lt;/p&gt;
&lt;p&gt;Sure enough, if you install a kernel with that config change, you can
verify the XDP program and load it into the kernel!&lt;/p&gt;
&lt;p&gt;If you have real powerpc hardware, that's enough to use XDP on Power!
Thanks to &lt;a href="http://michael.ellerman.id.au/"&gt;Michael Ellerman&lt;/a&gt;,
maintainer extraordinaire, for verifying this for me.&lt;/p&gt;
&lt;p&gt;If - like me - you don't have ready access to Power hardware, you're
stuffed. You can't use qemu in TCG mode: to use XDP with a VM, you
need multi-queue support, which only exists in the vhost driver, which
is only available for KVM guests. Maybe IBM should release a developer
workstation. (Hint, hint!)&lt;/p&gt;
&lt;p&gt;Overall, I was pleasantly surprised by how easy things were for people
with real ppc hardware - it's encouraging to see something not require
kernel changes!&lt;/p&gt;
&lt;p&gt;eBPF and XDP are definitely growing technologies - as &lt;a href="https://twitter.com/brendangregg/status/866078955530444800"&gt;Brendan Gregg notes&lt;/a&gt;,
now is a good time to learn them! (And those on Power have no excuse
either!)&lt;/p&gt;</summary><content type="html">&lt;p&gt;This post is a bit of a break from the standard IBM fare of this blog,
as I now work for Canonical. But I have a soft spot for Power from my
time at IBM - and Canonical officially supports 64-bit, little-endian
Power - so when I get a spare moment I try to make sure that cool,
officially-supported technologies work on Power &lt;em&gt;before&lt;/em&gt; we end up
with a customer emergency! So, without further ado, this is the story
of XDP on Power.&lt;/p&gt;
&lt;h2&gt;XDP&lt;/h2&gt;
&lt;p&gt;eXpress Data Path (XDP) is a cool Linux technology to allow really
fast processing of network packets.&lt;/p&gt;
&lt;p&gt;Normally in Linux, a packet is received by the network card, an SKB
(&lt;a href="http://vger.kernel.org/~davem/skb.html"&gt;socket buffer&lt;/a&gt;) is
allocated, and the packet is passed up through the networking stack.&lt;/p&gt;
&lt;p&gt;This introduces an inescapable latency penalty: we have to allocate
some memory and copy stuff around. XDP allows some network cards and
drivers to process packets early - even before the allocation of the
SKB. This is much faster, and so has applications in DDOS mitigation
and other high-speed networking use-cases. The IOVisor project has
&lt;a href="https://www.iovisor.org/technology/xdp"&gt;much more information&lt;/a&gt; if you
want to learn more.&lt;/p&gt;
&lt;h2&gt;eBPF&lt;/h2&gt;
&lt;p&gt;XDP processing is done by an eBPF program. eBPF - the extended
Berkeley Packet Filter - is an in-kernel virtual machine with a
limited set of instructions. The kernel can statically validate eBPF
programs to ensure that they terminate and are memory safe. From this
it follows that the programs cannot be Turing-complete: they do not
have backward branches, so they cannot do fancy things like
loops. Nonetheless, they're surprisingly powerful for packet
processing and tracing. eBPF programs are translated into efficient
machine code using in-kernel JIT compilers on many platforms, and
interpreted on platforms that do not have a JIT. (Yes, there are
multiple JIT implementations in the kernel. I find this a terrifying
thought.)&lt;/p&gt;
&lt;p&gt;Rather than requiring people to write raw eBPF programs, you can write
them in a somewhat-restricted subset of C, and use Clang's eBPF target
to translate them. This is super handy, as it gives you access to the
kernel headers - which define a number of useful data structures like
headers for various network protocols.&lt;/p&gt;
&lt;h2&gt;Trying it&lt;/h2&gt;
&lt;p&gt;There are a few really interesting project that are already up and
running that allow you to explore XDP without learning the innards of
both eBPF and the kernel networking stack. I explored the samples in
the &lt;a href="https://github.com/iovisor/bcc"&gt;bcc compiler collection&lt;/a&gt; and also
the samples from the &lt;a href="https://github.com/netoptimizer/prototype-kernel/"&gt;netoptimizer/prototype-kernel repository&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The easiest way to get started with these is with a virtual machine,
as recent virtio network drivers support XDP. If you are using Ubuntu,
you can use the &lt;a href="https://help.ubuntu.com/lts/serverguide/cloud-images-and-uvtool.html"&gt;uvt-kvm
tooling&lt;/a&gt;
to trivially set up a VM running Ubuntu Zesty on your local machine.&lt;/p&gt;
&lt;p&gt;Once your VM is installed, you need to shut it down and edit the virsh XML. &lt;/p&gt;
&lt;p&gt;You need 2 vCPUs (or more) and a virtio+vhost network card. You also
need to edit the 'interface' section and add the following snippet
(with thanks to the &lt;a href="https://www.spinics.net/lists/xdp-newbies/msg00029.html"&gt;xdp-newbies
list&lt;/a&gt;):&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="nt"&gt;&amp;lt;driver&lt;/span&gt; &lt;span class="na"&gt;name=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;vhost&amp;#39;&lt;/span&gt; &lt;span class="na"&gt;queues=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;4&amp;#39;&lt;/span&gt;&lt;span class="nt"&gt;&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;host&lt;/span&gt; &lt;span class="na"&gt;tso4=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;off&amp;#39;&lt;/span&gt; &lt;span class="na"&gt;tso6=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;off&amp;#39;&lt;/span&gt; &lt;span class="na"&gt;ecn=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;off&amp;#39;&lt;/span&gt; &lt;span class="na"&gt;ufo=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;off&amp;#39;&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
    &lt;span class="nt"&gt;&amp;lt;guest&lt;/span&gt; &lt;span class="na"&gt;tso4=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;off&amp;#39;&lt;/span&gt; &lt;span class="na"&gt;tso6=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;off&amp;#39;&lt;/span&gt; &lt;span class="na"&gt;ecn=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;off&amp;#39;&lt;/span&gt; &lt;span class="na"&gt;ufo=&lt;/span&gt;&lt;span class="s"&gt;&amp;#39;off&amp;#39;&lt;/span&gt;&lt;span class="nt"&gt;/&amp;gt;&lt;/span&gt;
&lt;span class="nt"&gt;&amp;lt;/driver&amp;gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;(If you have more than 2 vCPUs, set the queues parameter to 2x the
number of vCPUs.)&lt;/p&gt;
&lt;p&gt;Then, install a modern clang (we've had issues with 3.8 - I recommend
v4+), and the usual build tools.&lt;/p&gt;
&lt;p&gt;I recommend testing with the prototype-kernel tools - the DDOS
prevention tool is a good demo. Then - on x86 - you just follow their
instructions. I'm not going to repeat that here.&lt;/p&gt;
&lt;h2&gt;POWERful XDP&lt;/h2&gt;
&lt;p&gt;What happens when you try this on Power? Regular readers of my posts
will know to expect some
&lt;a href="https://sthbrx.github.io/blog/2017/02/13/high-power-lustre/"&gt;minor&lt;/a&gt;
&lt;a href="https://sthbrx.github.io/blog/2017/02/01/namd-on-nvlink/"&gt;hitches&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;XDP does not disappoint.&lt;/p&gt;
&lt;p&gt;Firstly, the prototype-kernel repository &lt;a href="https://github.com/netoptimizer/prototype-kernel/blob/master/kernel/samples/bpf/Makefile#L92"&gt;hard codes x86&lt;/a&gt;
as the architecture for kernel headers. You need to change it for
powerpc.&lt;/p&gt;
&lt;p&gt;Then, once you get the stuff compiled, and try to run it on a
current-at-time-of-writing Zesty kernel, you'll hit a massive debug
splat ending in:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;32: (61) r1 = *(u32 *)(r8 +12)
misaligned packet access off 0+18+12 size 4
load_bpf_file: Permission denied
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It turns out this is because in Ubuntu's Zesty kernel,
CONFIG_HAS_EFFICIENT_UNALIGNED_ACCESS is not set on ppc64el. Because
of that, the eBPF verifier will check that all loads are aligned - and
this load (part of checking some packet header) is not, and so the
verifier rejects the program. Unaligned access is not enabled because
the Zesty kernel is being compiled for CPU_POWER7 instead of
CPU_POWER8, and we don't have efficient unaligned access on POWER7.&lt;/p&gt;
&lt;p&gt;As it turns out, IBM never released any officially supported Power7 LE
systems - LE was only ever supported on Power8. So, I &lt;a href="https://bugs.launchpad.net/ubuntu/+source/linux/+bug/1699627"&gt;filed a bug&lt;/a&gt; and
&lt;a href="https://lists.ubuntu.com/archives/kernel-team/2017-June/085074.html"&gt;sent a patch&lt;/a&gt;
to build Zesty kernels for POWER8 instead, and that has been accepted
and will be part of the next stable update due real soon now.&lt;/p&gt;
&lt;p&gt;Sure enough, if you install a kernel with that config change, you can
verify the XDP program and load it into the kernel!&lt;/p&gt;
&lt;p&gt;If you have real powerpc hardware, that's enough to use XDP on Power!
Thanks to &lt;a href="http://michael.ellerman.id.au/"&gt;Michael Ellerman&lt;/a&gt;,
maintainer extraordinaire, for verifying this for me.&lt;/p&gt;
&lt;p&gt;If - like me - you don't have ready access to Power hardware, you're
stuffed. You can't use qemu in TCG mode: to use XDP with a VM, you
need multi-queue support, which only exists in the vhost driver, which
is only available for KVM guests. Maybe IBM should release a developer
workstation. (Hint, hint!)&lt;/p&gt;
&lt;p&gt;Overall, I was pleasantly surprised by how easy things were for people
with real ppc hardware - it's encouraging to see something not require
kernel changes!&lt;/p&gt;
&lt;p&gt;eBPF and XDP are definitely growing technologies - as &lt;a href="https://twitter.com/brendangregg/status/866078955530444800"&gt;Brendan Gregg notes&lt;/a&gt;,
now is a good time to learn them! (And those on Power have no excuse
either!)&lt;/p&gt;</content><category term="xdp"></category><category term="power"></category><category term="networking"></category><category term="remoteposts"></category></entry><entry><title>Evaluating CephFS on Power</title><link href="http://sthbrx.github.io/blog/2017/03/29/evaluating-cephfs-on-power/" rel="alternate"></link><published>2017-03-29T00:00:00+11:00</published><updated>2017-03-29T00:00:00+11:00</updated><author><name>Alastair D'Silva</name></author><id>tag:sthbrx.github.io,2017-03-29:/blog/2017/03/29/evaluating-cephfs-on-power/</id><summary type="html">&lt;h2&gt;Methodology&lt;/h2&gt;
&lt;p&gt;To evaluate CephFS, we will create a ppc64le virtual machine, with sufficient
space to compile the software, as well as 3 sparse 1TB disks to create the
object store.&lt;/p&gt;
&lt;p&gt;We will then build &amp;amp; install the Ceph packages, after adding the PowerPC
optimisiations to the code. This is done, as ceph-deploy will fetch prebuilt
packages that do not have the performance patches if the packages are not
installed.&lt;/p&gt;
&lt;p&gt;Finally, we will use the ceph-deploy to deploy the instance. We will ceph-deploy
via pip, to avoid file conflicts with the packages that we built.&lt;/p&gt;
&lt;p&gt;For more information on what each command does, visit the following tutorial,
upon which which this is based:
&lt;a href="http://palmerville.github.io/2016/04/30/single-node-ceph-install.html"&gt;http://palmerville.github.io/2016/04/30/single-node-ceph-install.html&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Virtual Machine Config&lt;/h3&gt;
&lt;p&gt;Create a virtual machine with at least the following:
 - 16GB of memory
 - 16 CPUs
 - 64GB disk for the root filesystem
 - 3 x 1TB for the Ceph object store
 - Ubuntu 16.04 default install (only use the 64GB disk, leave the others unpartitioned)&lt;/p&gt;
&lt;h3&gt;Initial config&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Enable ssh&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    sudo apt install openssh-server
    sudo apt update
    sudo apt upgrade
    sudo reboot
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Install build tools&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    sudo apt install git debhelper
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Build Ceph&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Clone the Ceph repo by following the instructions here: &lt;a href="http://docs.ceph.com/docs/master/install/clone-source/"&gt;http://docs.ceph.com/docs/master/install/clone-source/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    mkdir $HOME/src
    cd $HOME/src
    git clone --recursive https://github.com/ceph/ceph.git  # This may take a while
    cd ceph
    git checkout master
    git submodule update --force --init --recursive
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Cherry-pick the Power performance patches:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    git remote add kestrels https://github.com/kestrels/ceph.git
    git fetch --all
    git cherry-pick 59bed55a676ebbe3ad97d8ec005c2088553e4e11
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Install prerequisites&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    ./install-deps.sh
    sudo apt install python-requests python-flask resource-agents curl python-cherrypy python3-pip python-django python-dateutil python-djangorestframework
    sudo pip3 install ceph-deploy
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Build the packages as per the instructions: &lt;a href="http://docs.ceph.com/docs/master/install/build-ceph/"&gt;http://docs.ceph.com/docs/master/install/build-ceph/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    cd $HOME/src/ceph
    sudo dpkg-buildpackage -J$(nproc) # This will take a couple of hours (16 cpus)
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Install the packages (note that python3-ceph-argparse will fail, but is safe to ignore)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    cd $HOME/src
    sudo dpkg -i *.deb
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Create the ceph-deploy user&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    sudo adduser ceph-deploy
    echo &amp;quot;ceph-deploy ALL = (root) NOPASSWD:ALL&amp;quot; | sudo tee /etc/sudoers.d/ceph-deploy
    sudo chmod 0440 /etc/sudoers.d/ceph-deploy
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Configure the ceph-deploy user environment&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    su - ceph-deploy
    ssh-keygen
    node=`hostname`
    ssh-copy-id ceph-deploy@$node
    mkdir $HOME/ceph-cluster
    cd $HOME/ceph-cluster
    ceph-deploy new $node # If this fails, remove the bogus 127.0.1.1 entry from /etc/hosts
    echo &amp;#39;osd pool default size = 2&amp;#39; &amp;gt;&amp;gt; ceph.conf
    echo &amp;#39;osd crush chooseleaf type = 0&amp;#39; &amp;gt;&amp;gt; ceph.conf
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Complete the Ceph deployment&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    ceph-deploy install &lt;span class="nv"&gt;$node&lt;/span&gt;
    ceph-deploy mon create-initial
    drives=&amp;quot;vda vdb vdc&amp;quot;  # the 1TB drives - check that these are correct for your system
    for drive in &lt;span class="nv"&gt;$drives&lt;/span&gt;; do ceph-deploy disk zap &lt;span class="nv"&gt;$node&lt;/span&gt;:&lt;span class="nv"&gt;$drive&lt;/span&gt;; ceph-deploy osd prepare &lt;span class="nv"&gt;$node&lt;/span&gt;:&lt;span class="nv"&gt;$drive&lt;/span&gt;; done
    for drive in &lt;span class="nv"&gt;$drives&lt;/span&gt;; do ceph-deploy osd activate &lt;span class="nv"&gt;$node&lt;/span&gt;:/dev/&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;drive&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;1; done
    ceph-deploy admin &lt;span class="nv"&gt;$node&lt;/span&gt;
    sudo chmod +r /etc/ceph/ceph.client.admin.keyring
    ceph -s # Check the state of the cluster
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Configure CephFS&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    ceph-deploy mds create $node
    ceph osd pool create cephfs_data 128
    ceph osd pool create cephfs_metadata 128
    ceph fs new cephfs cephfs_metadata cephfs_data
    sudo systemctl status ceph\*.service ceph\*.target # Ensure the ceph-osd, ceph-mon &amp;amp; ceph-mds daemons are running
    sudo mkdir /mnt/cephfs
    key=`grep key ~/ceph-cluster/ceph.client.admin.keyring | cut -d &amp;#39; &amp;#39; -f 3`
    sudo mount -t ceph $node:6789:/ /mnt/cephfs -o name=admin,secret=$key
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://docs.ceph.com/docs/master/install/clone-source/"&gt;http://docs.ceph.com/docs/master/install/clone-source/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://docs.ceph.com/docs/master/install/build-ceph/"&gt;http://docs.ceph.com/docs/master/install/build-ceph/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://palmerville.github.io/2016/04/30/single-node-ceph-install.html"&gt;http://palmerville.github.io/2016/04/30/single-node-ceph-install.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</summary><content type="html">&lt;h2&gt;Methodology&lt;/h2&gt;
&lt;p&gt;To evaluate CephFS, we will create a ppc64le virtual machine, with sufficient
space to compile the software, as well as 3 sparse 1TB disks to create the
object store.&lt;/p&gt;
&lt;p&gt;We will then build &amp;amp; install the Ceph packages, after adding the PowerPC
optimisiations to the code. This is done, as ceph-deploy will fetch prebuilt
packages that do not have the performance patches if the packages are not
installed.&lt;/p&gt;
&lt;p&gt;Finally, we will use the ceph-deploy to deploy the instance. We will ceph-deploy
via pip, to avoid file conflicts with the packages that we built.&lt;/p&gt;
&lt;p&gt;For more information on what each command does, visit the following tutorial,
upon which which this is based:
&lt;a href="http://palmerville.github.io/2016/04/30/single-node-ceph-install.html"&gt;http://palmerville.github.io/2016/04/30/single-node-ceph-install.html&lt;/a&gt;&lt;/p&gt;
&lt;h3&gt;Virtual Machine Config&lt;/h3&gt;
&lt;p&gt;Create a virtual machine with at least the following:
 - 16GB of memory
 - 16 CPUs
 - 64GB disk for the root filesystem
 - 3 x 1TB for the Ceph object store
 - Ubuntu 16.04 default install (only use the 64GB disk, leave the others unpartitioned)&lt;/p&gt;
&lt;h3&gt;Initial config&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Enable ssh&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    sudo apt install openssh-server
    sudo apt update
    sudo apt upgrade
    sudo reboot
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Install build tools&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    sudo apt install git debhelper
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Build Ceph&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Clone the Ceph repo by following the instructions here: &lt;a href="http://docs.ceph.com/docs/master/install/clone-source/"&gt;http://docs.ceph.com/docs/master/install/clone-source/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    mkdir $HOME/src
    cd $HOME/src
    git clone --recursive https://github.com/ceph/ceph.git  # This may take a while
    cd ceph
    git checkout master
    git submodule update --force --init --recursive
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Cherry-pick the Power performance patches:&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    git remote add kestrels https://github.com/kestrels/ceph.git
    git fetch --all
    git cherry-pick 59bed55a676ebbe3ad97d8ec005c2088553e4e11
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Install prerequisites&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    ./install-deps.sh
    sudo apt install python-requests python-flask resource-agents curl python-cherrypy python3-pip python-django python-dateutil python-djangorestframework
    sudo pip3 install ceph-deploy
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Build the packages as per the instructions: &lt;a href="http://docs.ceph.com/docs/master/install/build-ceph/"&gt;http://docs.ceph.com/docs/master/install/build-ceph/&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    cd $HOME/src/ceph
    sudo dpkg-buildpackage -J$(nproc) # This will take a couple of hours (16 cpus)
&lt;/pre&gt;&lt;/div&gt;


&lt;ul&gt;
&lt;li&gt;Install the packages (note that python3-ceph-argparse will fail, but is safe to ignore)&lt;/li&gt;
&lt;/ul&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    cd $HOME/src
    sudo dpkg -i *.deb
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Create the ceph-deploy user&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    sudo adduser ceph-deploy
    echo &amp;quot;ceph-deploy ALL = (root) NOPASSWD:ALL&amp;quot; | sudo tee /etc/sudoers.d/ceph-deploy
    sudo chmod 0440 /etc/sudoers.d/ceph-deploy
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Configure the ceph-deploy user environment&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    su - ceph-deploy
    ssh-keygen
    node=`hostname`
    ssh-copy-id ceph-deploy@$node
    mkdir $HOME/ceph-cluster
    cd $HOME/ceph-cluster
    ceph-deploy new $node # If this fails, remove the bogus 127.0.1.1 entry from /etc/hosts
    echo &amp;#39;osd pool default size = 2&amp;#39; &amp;gt;&amp;gt; ceph.conf
    echo &amp;#39;osd crush chooseleaf type = 0&amp;#39; &amp;gt;&amp;gt; ceph.conf
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Complete the Ceph deployment&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    ceph-deploy install &lt;span class="nv"&gt;$node&lt;/span&gt;
    ceph-deploy mon create-initial
    drives=&amp;quot;vda vdb vdc&amp;quot;  # the 1TB drives - check that these are correct for your system
    for drive in &lt;span class="nv"&gt;$drives&lt;/span&gt;; do ceph-deploy disk zap &lt;span class="nv"&gt;$node&lt;/span&gt;:&lt;span class="nv"&gt;$drive&lt;/span&gt;; ceph-deploy osd prepare &lt;span class="nv"&gt;$node&lt;/span&gt;:&lt;span class="nv"&gt;$drive&lt;/span&gt;; done
    for drive in &lt;span class="nv"&gt;$drives&lt;/span&gt;; do ceph-deploy osd activate &lt;span class="nv"&gt;$node&lt;/span&gt;:/dev/&lt;span class="cp"&gt;${&lt;/span&gt;&lt;span class="n"&gt;drive&lt;/span&gt;&lt;span class="cp"&gt;}&lt;/span&gt;1; done
    ceph-deploy admin &lt;span class="nv"&gt;$node&lt;/span&gt;
    sudo chmod +r /etc/ceph/ceph.client.admin.keyring
    ceph -s # Check the state of the cluster
&lt;/pre&gt;&lt;/div&gt;


&lt;h3&gt;Configure CephFS&lt;/h3&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    ceph-deploy mds create $node
    ceph osd pool create cephfs_data 128
    ceph osd pool create cephfs_metadata 128
    ceph fs new cephfs cephfs_metadata cephfs_data
    sudo systemctl status ceph\*.service ceph\*.target # Ensure the ceph-osd, ceph-mon &amp;amp; ceph-mds daemons are running
    sudo mkdir /mnt/cephfs
    key=`grep key ~/ceph-cluster/ceph.client.admin.keyring | cut -d &amp;#39; &amp;#39; -f 3`
    sudo mount -t ceph $node:6789:/ /mnt/cephfs -o name=admin,secret=$key
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;References&lt;/h2&gt;
&lt;ol&gt;
&lt;li&gt;&lt;a href="http://docs.ceph.com/docs/master/install/clone-source/"&gt;http://docs.ceph.com/docs/master/install/clone-source/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://docs.ceph.com/docs/master/install/build-ceph/"&gt;http://docs.ceph.com/docs/master/install/build-ceph/&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href="http://palmerville.github.io/2016/04/30/single-node-ceph-install.html"&gt;http://palmerville.github.io/2016/04/30/single-node-ceph-install.html&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;</content><category term="ceph"></category><category term="raid"></category><category term="storage"></category></entry><entry><title>Erasure Coding for Programmers, Part 2</title><link href="http://sthbrx.github.io/blog/2017/03/24/erasure-coding-for-programmers-part-2/" rel="alternate"></link><published>2017-03-24T10:08:00+11:00</published><updated>2017-03-24T10:08:00+11:00</updated><author><name>Daniel Axtens</name></author><id>tag:sthbrx.github.io,2017-03-24:/blog/2017/03/24/erasure-coding-for-programmers-part-2/</id><summary type="html">&lt;p&gt;We left &lt;a href="/blog/2017/03/20/erasure-coding-for-programmers-part-1/"&gt;part 1&lt;/a&gt; having explored GF(2^8) and RAID 6, and asking the question "what does all this have to do with Erasure Codes?"&lt;/p&gt;
&lt;p&gt;Basically, the thinking goes "RAID 6 is cool, but what if, instead of two parity disks, we had an arbitrary number of parity disks?"&lt;/p&gt;
&lt;p&gt;How would we do that? Well, let's introduce our new best friend: Coding Theory!&lt;/p&gt;
&lt;p&gt;Say we want to transmit some data across an error-prone medium. We don't know where the errors might occur, so we add some extra information to allow us to detect and possibly correct for errors. This is a code. Codes are a largish field of engineering, but rather than show off my knowledge about systematic linear block codes, let's press on.&lt;/p&gt;
&lt;p&gt;Today, our error-prone medium is an array of inexpensive disks. Now we make this really nice assumption about disks, namely that they are either perfectly reliable or completely missing. In other words, we consider that a disk will either be present or 'erased'. We come up with 'erasure codes' that are able to reconstruct data when it is known to be missing. (This is a slightly different problem to being able to verify and correct data that might or might not be subtly corrupted. Disks also have to deal with this problem, but it is &lt;em&gt;not&lt;/em&gt; something erasure codes address!)&lt;/p&gt;
&lt;p&gt;The particular code we use is a Reed-Solomon code. The specific details are unimportant, but there's a really good graphical outline of the broad concepts in sections 1 and 3 of &lt;a href="http://jerasure.org/jerasure-2.0/"&gt;the Jerasure paper/manual&lt;/a&gt;. (Don't go on to section 4.)&lt;/p&gt;
&lt;p&gt;That should give you some background on how this works at a pretty basic mathematical level. Implementation is a matter of mapping that maths (matrix multiplication) onto hardware primitives, and making it go fast.&lt;/p&gt;
&lt;h2&gt;Scope&lt;/h2&gt;
&lt;p&gt;I'm deliberately &lt;em&gt;not&lt;/em&gt; covering some pretty vast areas of what would be required to write your own erasure coding library from scratch. I'm not going to talk about how to compose the matricies, how to invert them, or anything like that. I'm not sure how that would be a helpful exercise - ISA-L and jerasure already exist and do that for you.&lt;/p&gt;
&lt;p&gt;What I want to cover is an efficient implementation of the some algorithms, once you have the matricies nailed down.&lt;/p&gt;
&lt;p&gt;I'm also going to assume your library already provides a generic multiplication function in GF(2^8). That's required to construct the matrices, so it's a pretty safe assumption.&lt;/p&gt;
&lt;h2&gt;The beginnings of an API&lt;/h2&gt;
&lt;p&gt;Let's make this a bit more concrete.&lt;/p&gt;
&lt;p&gt;This will be heavily based on the &lt;a href="https://01.org/intel%C2%AE-storage-acceleration-library-open-source-version/documentation/isa-l-open-source-api"&gt;ISA-L API&lt;/a&gt; but you probably want to plug into ISA-L anyway, so that shouldn't be a problem.&lt;/p&gt;
&lt;p&gt;What I want to do is build up from very basic algorithmic components into something useful.&lt;/p&gt;
&lt;p&gt;The first thing we want to do is to be able to is Galois Field multiplication of an entire region of bytes by an arbitrary constant.&lt;/p&gt;
&lt;p&gt;We basically want &lt;code&gt;gf_vect_mul(size_t len, &amp;lt;something representing the constant&amp;gt;, unsigned char * src, unsigned char * dest)&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;Simple and slow approach&lt;/h3&gt;
&lt;p&gt;The simplest way is to do something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;void gf_vect_mul_simple(size_t len, unsigned char c, unsigned char * src, unsigned char * dest) {

    size_t i;
    for (i=0; i&amp;lt;len; i++) {
        dest[i] = gf_mul(c, src[i]);
    }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That does multiplication element by element using the library's supplied &lt;code&gt;gf_mul&lt;/code&gt; function, which - as the name suggests - does GF(2^8) multiplication of a scalar by a scalar.&lt;/p&gt;
&lt;p&gt;This works. The problem is that it is very, painfully, slow - in the order of a few hundred megabytes per second.&lt;/p&gt;
&lt;h3&gt;Going faster&lt;/h3&gt;
&lt;p&gt;How can we make this faster?&lt;/p&gt;
&lt;p&gt;There are a few things we can try: if you want to explore a whole range of different ways to do this, check out the &lt;a href="http://jerasure.org/gf-complete-1.02/"&gt;gf-complete&lt;/a&gt; project. I'm going to assume we want to skip right to the end and know what is the fastest we've found.&lt;/p&gt;
&lt;p&gt;Cast your mind back to the &lt;a href="https://www.kernel.org/pub/linux/kernel/people/hpa/raid6.pdf"&gt;RAID 6 paper&lt;/a&gt; (PDF). I talked about in &lt;a href="/blog/2017/03/20/erasure-coding-for-programmers-part-1/"&gt;part 1&lt;/a&gt;. That had a way of doing an efficient multiplication in GF(2^8) using vector instructions.&lt;/p&gt;
&lt;p&gt;To refresh your memory, we split the multiplication into two parts - low bits and high bits, looked them up separately in a lookup table, and joined them with XOR. We then discovered that on modern Power chips, we could do that in one instruction with &lt;code&gt;vpermxor&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So, a very simple way to do this would be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;generate the table for &lt;code&gt;a&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;for each 16-byte chunk of our input:&lt;ul&gt;
&lt;li&gt;load the input&lt;/li&gt;
&lt;li&gt;do the &lt;code&gt;vpermxor&lt;/code&gt; with the table&lt;/li&gt;
&lt;li&gt;save it out&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generating the tables is reasonably straight-forward, in theory. Recall that the tables are &lt;code&gt;a&lt;/code&gt; * {{00},{01},...,{0f}} and &lt;code&gt;a&lt;/code&gt; * {{00},{10},..,{f0}} - a couple of loops in C will generate them without difficulty. ISA-L has a function to do this, as does gf-complete in split-table mode, so I won't repeat them here.&lt;/p&gt;
&lt;p&gt;So, let's recast our function to take the tables as an input rather than the constant &lt;code&gt;a&lt;/code&gt;. Assume we're provided the two tables concatenated into one 32-byte chunk. That would give us:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;void gf_vect_mul_v2(size_t len, unsigned char * table, unsigned char * src, unsigned char * dest)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here's how you would do it in C:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;void gf_vect_mul_v2(size_t len, unsigned char * table, unsigned char * src, unsigned char * dest) {
        vector unsigned char tbl1, tbl2, in, out;
        size_t i;

        /* Assume table, src, dest are aligned and len is a multiple of 16 */

        tbl1 = vec_ld(16, table);
        tbl2 = vec_ld(0, table);
        for (i=0; i&amp;lt;len; i+=16) {
            in = vec_ld(i, (unsigned char *)src);
            __asm__(&amp;quot;vpermxor %0, %1, %2, %3&amp;quot; : &amp;quot;=v&amp;quot;(out) : &amp;quot;v&amp;quot;(tbl1), &amp;quot;v&amp;quot;(tbl2), &amp;quot;v&amp;quot;(in)
            vec_st(out, i, (unsigned char *)dest);
        }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There's a few quirks to iron out - making sure the table is laid out in the vector register in the way you expect, etc, but that generally works and is quite fast - my Power 8 VM does about 17-18 GB/s with non-cache-contained data with this implementation.&lt;/p&gt;
&lt;p&gt;We can go a bit faster by doing larger chunks at a time:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    for (i=0; i&amp;lt;vlen; i+=64) {
            in1 = vec_ld(i, (unsigned char *)src);
            in2 = vec_ld(i+16, (unsigned char *)src);
            in3 = vec_ld(i+32, (unsigned char *)src);
            in4 = vec_ld(i+48, (unsigned char *)src);
            __asm__(&amp;quot;vpermxor %0, %1, %2, %3&amp;quot; : &amp;quot;=v&amp;quot;(out1) : &amp;quot;v&amp;quot;(tbl1), &amp;quot;v&amp;quot;(tbl2), &amp;quot;v&amp;quot;(in1));
            __asm__(&amp;quot;vpermxor %0, %1, %2, %3&amp;quot; : &amp;quot;=v&amp;quot;(out2) : &amp;quot;v&amp;quot;(tbl1), &amp;quot;v&amp;quot;(tbl2), &amp;quot;v&amp;quot;(in2));
            __asm__(&amp;quot;vpermxor %0, %1, %2, %3&amp;quot; : &amp;quot;=v&amp;quot;(out3) : &amp;quot;v&amp;quot;(tbl1), &amp;quot;v&amp;quot;(tbl2), &amp;quot;v&amp;quot;(in3));
            __asm__(&amp;quot;vpermxor %0, %1, %2, %3&amp;quot; : &amp;quot;=v&amp;quot;(out4) : &amp;quot;v&amp;quot;(tbl1), &amp;quot;v&amp;quot;(tbl2), &amp;quot;v&amp;quot;(in4));
            vec_st(out1, i, (unsigned char *)dest);
            vec_st(out2, i+16, (unsigned char *)dest);
            vec_st(out3, i+32, (unsigned char *)dest);
            vec_st(out4, i+48, (unsigned char *)dest);
    }
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This goes at about 23.5 GB/s.&lt;/p&gt;
&lt;p&gt;We can go one step further and do the core loop in assembler - that means we control the instruction layout and so on. I tried this: it turns out that for the basic vector multiply loop, if we turn off ASLR and pin to a particular CPU, we can see a improvement of a few percent (and a decrease in variability) over C code.&lt;/p&gt;
&lt;h2&gt;Building from vector multiplication&lt;/h2&gt;
&lt;p&gt;Once you're comfortable with the core vector multiplication, you can start to build more interesting routines.&lt;/p&gt;
&lt;p&gt;A particularly useful one on Power turned out to be the multiply and add routine: like gf_vect_mul, except that rather than overwriting the output, it loads the output and xors the product in. This is a simple extension of the gf_vect_mul function so is left as an exercise to the reader.&lt;/p&gt;
&lt;p&gt;The next step would be to start building erasure coding proper. Recall that to get an element of our output, we take a dot product: we take the corresponding input element of each disk, multiply it with the corresponding GF(2^8) coding matrix element and sum all those products. So all we need now is a dot product algorithm.&lt;/p&gt;
&lt;p&gt;One approach is the conventional dot product:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for each element&lt;ul&gt;
&lt;li&gt;zero accumulator&lt;/li&gt;
&lt;li&gt;for each source&lt;ul&gt;
&lt;li&gt;load &lt;code&gt;input[source][element]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;do GF(2^8) multiplication&lt;/li&gt;
&lt;li&gt;xor into accumulator&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;save accumulator to &lt;code&gt;output[element]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The other approach is multiply and add:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for each source&lt;ul&gt;
&lt;li&gt;for each element&lt;ul&gt;
&lt;li&gt;load &lt;code&gt;input[source][element]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;do GF(2^8) multiplication&lt;/li&gt;
&lt;li&gt;load &lt;code&gt;output[element]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;xor in product&lt;/li&gt;
&lt;li&gt;save &lt;code&gt;output[element]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The dot product approach has the advantage of fewer writes. The multiply and add approach has the advantage of better cache/prefetch performance. The approach you ultimately go with will probably depend on the characteristics of your machine and the length of data you are dealing with.&lt;/p&gt;
&lt;p&gt;For what it's worth, ISA-L ships with only the first approach in x86 assembler, and Jerasure leans heavily towards the second approach.&lt;/p&gt;
&lt;p&gt;Once you have a vector dot product sorted, you can build a full erasure coding setup: build your tables with your library, then do a dot product to generate each of your outputs!&lt;/p&gt;
&lt;p&gt;In ISA-L, this is implemented something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/*&lt;/span&gt;
&lt;span class="cm"&gt; * ec_encode_data_simple(length of each data input, number of inputs,&lt;/span&gt;
&lt;span class="cm"&gt; *                       number of outputs, pre-generated GF(2^8) tables,&lt;/span&gt;
&lt;span class="cm"&gt; *                       input data pointers, output code pointers)&lt;/span&gt;
&lt;span class="cm"&gt; */&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
void&lt;span class="w"&gt; &lt;/span&gt;ec_encode_data_simple&lt;span class="o"&gt;(&lt;/span&gt;int&lt;span class="w"&gt; &lt;/span&gt;len&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;int&lt;span class="w"&gt; &lt;/span&gt;k&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;int&lt;span class="w"&gt; &lt;/span&gt;rows&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;unsigned&lt;span class="w"&gt; &lt;/span&gt;char&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;g_tbls&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;                           &lt;/span&gt;unsigned&lt;span class="w"&gt; &lt;/span&gt;char&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;data&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;unsigned&lt;span class="w"&gt; &lt;/span&gt;char&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;coding&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="kr"&gt;while&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;rows&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;gf_vect_dot_prod&lt;span class="o"&gt;(&lt;/span&gt;len&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;k&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;g_tbls&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;data&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;coding&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="err"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;g_tbls&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;k&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;32&lt;/span&gt;&lt;span class="err"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;coding&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="err"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;rows&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="err"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="err"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Going faster still&lt;/h2&gt;
&lt;p&gt;Eagle eyed readers will notice that however we generate an output, we have to read all the input elements. This means that if we're doing a code with 10 data disks and 4 coding disks, we have to read each of the 10 inputs 4 times.&lt;/p&gt;
&lt;p&gt;We could do better if we could calculate multiple outputs for each pass through the inputs. This is a little fiddly to implement, but does lead to a speed improvement.&lt;/p&gt;
&lt;p&gt;ISA-L is an excellent example here. Intel goes up to 6 outputs at once: the number of outputs you can do is only limited by how many vector registers you have to put the various operands and results in.&lt;/p&gt;
&lt;h2&gt;Tips and tricks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Benchmarking is tricky. I do the following on a bare-metal, idle machine, with ASLR off and pinned to an arbitrary hardware thread. (Code is for the &lt;a href="https://fishshell.com/"&gt;fish shell&lt;/a&gt;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;for x in (seq 1 50)
    setarch ppc64le -R taskset -c 24 erasure_code/gf_vect_mul_perf
end | awk &amp;#39;/MB/ {sum+=$13} END {print sum/50, &amp;quot;MB/s&amp;quot;}&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Debugging is tricky; the more you can do in C and the less you do in assembly, the easier your life will be.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vector code is notoriously alignment-sensitive - if you can't figure out why something is wrong, check alignment. (Pro-tip: ISA-L does &lt;em&gt;not&lt;/em&gt; guarantee the alignment of the &lt;code&gt;gftbls&lt;/code&gt; parameter, and many of the tests supply an unaligned table from the stack. For testing &lt;code&gt;__attribute__((aligned(16)))&lt;/code&gt; is your friend!)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Related: GCC is moving towards assignment over vector intrinsics, at least on Power:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;vector unsigned char a;
unsigned char * data;
// good, also handles word-aligned data with VSX
a = *(vector unsigned char *)data;
// bad, requires special handling of non-16-byte aligned data
a = vec_ld(0, (unsigned char *) data);
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Hopefully by this point you're equipped to figure out how your erasure coding library of choice works, and write your own optimised implementation (or maintain an implementation written by someone else).&lt;/p&gt;
&lt;p&gt;I've referred to a number of resources throughout this series:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ISA-L &lt;a href="https://github.com/01org/isa-l"&gt;code&lt;/a&gt;, &lt;a href=""&gt;API description&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jerasure &lt;a href="http://jerasure.org/"&gt;code&lt;/a&gt;, &lt;a href="http://jerasure.org/jerasure-2.0/"&gt;docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;gf-complete &lt;a href="http://jerasure.org/"&gt;code&lt;/a&gt;, &lt;a href="http://jerasure.org/gf-complete-1.02/"&gt;docs&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kernel.org/pub/linux/kernel/people/hpa/raid6.pdf"&gt;The mathematics of RAID-6&lt;/a&gt; (PDF), H. Peter Anvin&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you want to go deeper, I also read the following and found them quite helpful in understanding Galois Fields and Reed-Solomon coding:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19900019023.pdf"&gt;Tutorial on Reed-Solomon Error Correction Coding&lt;/a&gt; (PDF), William A. Geisel, NASA&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19900019023.pdf"&gt;Reed-Solomon error correction&lt;/a&gt; (PDF), BBC R&amp;amp;D White Paper WHP 031, C. K. P. Clarke.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a more rigorous mathematical approach to rings and fields, a university mathematics course may be of interest. For more on coding theory, a university course in electronics engineering may be helpful.&lt;/p&gt;</summary><content type="html">&lt;p&gt;We left &lt;a href="/blog/2017/03/20/erasure-coding-for-programmers-part-1/"&gt;part 1&lt;/a&gt; having explored GF(2^8) and RAID 6, and asking the question "what does all this have to do with Erasure Codes?"&lt;/p&gt;
&lt;p&gt;Basically, the thinking goes "RAID 6 is cool, but what if, instead of two parity disks, we had an arbitrary number of parity disks?"&lt;/p&gt;
&lt;p&gt;How would we do that? Well, let's introduce our new best friend: Coding Theory!&lt;/p&gt;
&lt;p&gt;Say we want to transmit some data across an error-prone medium. We don't know where the errors might occur, so we add some extra information to allow us to detect and possibly correct for errors. This is a code. Codes are a largish field of engineering, but rather than show off my knowledge about systematic linear block codes, let's press on.&lt;/p&gt;
&lt;p&gt;Today, our error-prone medium is an array of inexpensive disks. Now we make this really nice assumption about disks, namely that they are either perfectly reliable or completely missing. In other words, we consider that a disk will either be present or 'erased'. We come up with 'erasure codes' that are able to reconstruct data when it is known to be missing. (This is a slightly different problem to being able to verify and correct data that might or might not be subtly corrupted. Disks also have to deal with this problem, but it is &lt;em&gt;not&lt;/em&gt; something erasure codes address!)&lt;/p&gt;
&lt;p&gt;The particular code we use is a Reed-Solomon code. The specific details are unimportant, but there's a really good graphical outline of the broad concepts in sections 1 and 3 of &lt;a href="http://jerasure.org/jerasure-2.0/"&gt;the Jerasure paper/manual&lt;/a&gt;. (Don't go on to section 4.)&lt;/p&gt;
&lt;p&gt;That should give you some background on how this works at a pretty basic mathematical level. Implementation is a matter of mapping that maths (matrix multiplication) onto hardware primitives, and making it go fast.&lt;/p&gt;
&lt;h2&gt;Scope&lt;/h2&gt;
&lt;p&gt;I'm deliberately &lt;em&gt;not&lt;/em&gt; covering some pretty vast areas of what would be required to write your own erasure coding library from scratch. I'm not going to talk about how to compose the matricies, how to invert them, or anything like that. I'm not sure how that would be a helpful exercise - ISA-L and jerasure already exist and do that for you.&lt;/p&gt;
&lt;p&gt;What I want to cover is an efficient implementation of the some algorithms, once you have the matricies nailed down.&lt;/p&gt;
&lt;p&gt;I'm also going to assume your library already provides a generic multiplication function in GF(2^8). That's required to construct the matrices, so it's a pretty safe assumption.&lt;/p&gt;
&lt;h2&gt;The beginnings of an API&lt;/h2&gt;
&lt;p&gt;Let's make this a bit more concrete.&lt;/p&gt;
&lt;p&gt;This will be heavily based on the &lt;a href="https://01.org/intel%C2%AE-storage-acceleration-library-open-source-version/documentation/isa-l-open-source-api"&gt;ISA-L API&lt;/a&gt; but you probably want to plug into ISA-L anyway, so that shouldn't be a problem.&lt;/p&gt;
&lt;p&gt;What I want to do is build up from very basic algorithmic components into something useful.&lt;/p&gt;
&lt;p&gt;The first thing we want to do is to be able to is Galois Field multiplication of an entire region of bytes by an arbitrary constant.&lt;/p&gt;
&lt;p&gt;We basically want &lt;code&gt;gf_vect_mul(size_t len, &amp;lt;something representing the constant&amp;gt;, unsigned char * src, unsigned char * dest)&lt;/code&gt;&lt;/p&gt;
&lt;h3&gt;Simple and slow approach&lt;/h3&gt;
&lt;p&gt;The simplest way is to do something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;void gf_vect_mul_simple(size_t len, unsigned char c, unsigned char * src, unsigned char * dest) {

    size_t i;
    for (i=0; i&amp;lt;len; i++) {
        dest[i] = gf_mul(c, src[i]);
    }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That does multiplication element by element using the library's supplied &lt;code&gt;gf_mul&lt;/code&gt; function, which - as the name suggests - does GF(2^8) multiplication of a scalar by a scalar.&lt;/p&gt;
&lt;p&gt;This works. The problem is that it is very, painfully, slow - in the order of a few hundred megabytes per second.&lt;/p&gt;
&lt;h3&gt;Going faster&lt;/h3&gt;
&lt;p&gt;How can we make this faster?&lt;/p&gt;
&lt;p&gt;There are a few things we can try: if you want to explore a whole range of different ways to do this, check out the &lt;a href="http://jerasure.org/gf-complete-1.02/"&gt;gf-complete&lt;/a&gt; project. I'm going to assume we want to skip right to the end and know what is the fastest we've found.&lt;/p&gt;
&lt;p&gt;Cast your mind back to the &lt;a href="https://www.kernel.org/pub/linux/kernel/people/hpa/raid6.pdf"&gt;RAID 6 paper&lt;/a&gt; (PDF). I talked about in &lt;a href="/blog/2017/03/20/erasure-coding-for-programmers-part-1/"&gt;part 1&lt;/a&gt;. That had a way of doing an efficient multiplication in GF(2^8) using vector instructions.&lt;/p&gt;
&lt;p&gt;To refresh your memory, we split the multiplication into two parts - low bits and high bits, looked them up separately in a lookup table, and joined them with XOR. We then discovered that on modern Power chips, we could do that in one instruction with &lt;code&gt;vpermxor&lt;/code&gt;.&lt;/p&gt;
&lt;p&gt;So, a very simple way to do this would be:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;generate the table for &lt;code&gt;a&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;for each 16-byte chunk of our input:&lt;ul&gt;
&lt;li&gt;load the input&lt;/li&gt;
&lt;li&gt;do the &lt;code&gt;vpermxor&lt;/code&gt; with the table&lt;/li&gt;
&lt;li&gt;save it out&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Generating the tables is reasonably straight-forward, in theory. Recall that the tables are &lt;code&gt;a&lt;/code&gt; * {{00},{01},...,{0f}} and &lt;code&gt;a&lt;/code&gt; * {{00},{10},..,{f0}} - a couple of loops in C will generate them without difficulty. ISA-L has a function to do this, as does gf-complete in split-table mode, so I won't repeat them here.&lt;/p&gt;
&lt;p&gt;So, let's recast our function to take the tables as an input rather than the constant &lt;code&gt;a&lt;/code&gt;. Assume we're provided the two tables concatenated into one 32-byte chunk. That would give us:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;void gf_vect_mul_v2(size_t len, unsigned char * table, unsigned char * src, unsigned char * dest)
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here's how you would do it in C:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;void gf_vect_mul_v2(size_t len, unsigned char * table, unsigned char * src, unsigned char * dest) {
        vector unsigned char tbl1, tbl2, in, out;
        size_t i;

        /* Assume table, src, dest are aligned and len is a multiple of 16 */

        tbl1 = vec_ld(16, table);
        tbl2 = vec_ld(0, table);
        for (i=0; i&amp;lt;len; i+=16) {
            in = vec_ld(i, (unsigned char *)src);
            __asm__(&amp;quot;vpermxor %0, %1, %2, %3&amp;quot; : &amp;quot;=v&amp;quot;(out) : &amp;quot;v&amp;quot;(tbl1), &amp;quot;v&amp;quot;(tbl2), &amp;quot;v&amp;quot;(in)
            vec_st(out, i, (unsigned char *)dest);
        }
}
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;There's a few quirks to iron out - making sure the table is laid out in the vector register in the way you expect, etc, but that generally works and is quite fast - my Power 8 VM does about 17-18 GB/s with non-cache-contained data with this implementation.&lt;/p&gt;
&lt;p&gt;We can go a bit faster by doing larger chunks at a time:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;    for (i=0; i&amp;lt;vlen; i+=64) {
            in1 = vec_ld(i, (unsigned char *)src);
            in2 = vec_ld(i+16, (unsigned char *)src);
            in3 = vec_ld(i+32, (unsigned char *)src);
            in4 = vec_ld(i+48, (unsigned char *)src);
            __asm__(&amp;quot;vpermxor %0, %1, %2, %3&amp;quot; : &amp;quot;=v&amp;quot;(out1) : &amp;quot;v&amp;quot;(tbl1), &amp;quot;v&amp;quot;(tbl2), &amp;quot;v&amp;quot;(in1));
            __asm__(&amp;quot;vpermxor %0, %1, %2, %3&amp;quot; : &amp;quot;=v&amp;quot;(out2) : &amp;quot;v&amp;quot;(tbl1), &amp;quot;v&amp;quot;(tbl2), &amp;quot;v&amp;quot;(in2));
            __asm__(&amp;quot;vpermxor %0, %1, %2, %3&amp;quot; : &amp;quot;=v&amp;quot;(out3) : &amp;quot;v&amp;quot;(tbl1), &amp;quot;v&amp;quot;(tbl2), &amp;quot;v&amp;quot;(in3));
            __asm__(&amp;quot;vpermxor %0, %1, %2, %3&amp;quot; : &amp;quot;=v&amp;quot;(out4) : &amp;quot;v&amp;quot;(tbl1), &amp;quot;v&amp;quot;(tbl2), &amp;quot;v&amp;quot;(in4));
            vec_st(out1, i, (unsigned char *)dest);
            vec_st(out2, i+16, (unsigned char *)dest);
            vec_st(out3, i+32, (unsigned char *)dest);
            vec_st(out4, i+48, (unsigned char *)dest);
    }
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This goes at about 23.5 GB/s.&lt;/p&gt;
&lt;p&gt;We can go one step further and do the core loop in assembler - that means we control the instruction layout and so on. I tried this: it turns out that for the basic vector multiply loop, if we turn off ASLR and pin to a particular CPU, we can see a improvement of a few percent (and a decrease in variability) over C code.&lt;/p&gt;
&lt;h2&gt;Building from vector multiplication&lt;/h2&gt;
&lt;p&gt;Once you're comfortable with the core vector multiplication, you can start to build more interesting routines.&lt;/p&gt;
&lt;p&gt;A particularly useful one on Power turned out to be the multiply and add routine: like gf_vect_mul, except that rather than overwriting the output, it loads the output and xors the product in. This is a simple extension of the gf_vect_mul function so is left as an exercise to the reader.&lt;/p&gt;
&lt;p&gt;The next step would be to start building erasure coding proper. Recall that to get an element of our output, we take a dot product: we take the corresponding input element of each disk, multiply it with the corresponding GF(2^8) coding matrix element and sum all those products. So all we need now is a dot product algorithm.&lt;/p&gt;
&lt;p&gt;One approach is the conventional dot product:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for each element&lt;ul&gt;
&lt;li&gt;zero accumulator&lt;/li&gt;
&lt;li&gt;for each source&lt;ul&gt;
&lt;li&gt;load &lt;code&gt;input[source][element]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;do GF(2^8) multiplication&lt;/li&gt;
&lt;li&gt;xor into accumulator&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;li&gt;save accumulator to &lt;code&gt;output[element]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The other approach is multiply and add:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;for each source&lt;ul&gt;
&lt;li&gt;for each element&lt;ul&gt;
&lt;li&gt;load &lt;code&gt;input[source][element]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;do GF(2^8) multiplication&lt;/li&gt;
&lt;li&gt;load &lt;code&gt;output[element]&lt;/code&gt;&lt;/li&gt;
&lt;li&gt;xor in product&lt;/li&gt;
&lt;li&gt;save &lt;code&gt;output[element]&lt;/code&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The dot product approach has the advantage of fewer writes. The multiply and add approach has the advantage of better cache/prefetch performance. The approach you ultimately go with will probably depend on the characteristics of your machine and the length of data you are dealing with.&lt;/p&gt;
&lt;p&gt;For what it's worth, ISA-L ships with only the first approach in x86 assembler, and Jerasure leans heavily towards the second approach.&lt;/p&gt;
&lt;p&gt;Once you have a vector dot product sorted, you can build a full erasure coding setup: build your tables with your library, then do a dot product to generate each of your outputs!&lt;/p&gt;
&lt;p&gt;In ISA-L, this is implemented something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/*&lt;/span&gt;
&lt;span class="cm"&gt; * ec_encode_data_simple(length of each data input, number of inputs,&lt;/span&gt;
&lt;span class="cm"&gt; *                       number of outputs, pre-generated GF(2^8) tables,&lt;/span&gt;
&lt;span class="cm"&gt; *                       input data pointers, output code pointers)&lt;/span&gt;
&lt;span class="cm"&gt; */&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
void&lt;span class="w"&gt; &lt;/span&gt;ec_encode_data_simple&lt;span class="o"&gt;(&lt;/span&gt;int&lt;span class="w"&gt; &lt;/span&gt;len&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;int&lt;span class="w"&gt; &lt;/span&gt;k&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;int&lt;span class="w"&gt; &lt;/span&gt;rows&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;unsigned&lt;span class="w"&gt; &lt;/span&gt;char&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;g_tbls&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;                           &lt;/span&gt;unsigned&lt;span class="w"&gt; &lt;/span&gt;char&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;data&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;unsigned&lt;span class="w"&gt; &lt;/span&gt;char&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;**&lt;/span&gt;coding&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="kr"&gt;while&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;(&lt;/span&gt;rows&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="err"&gt;{&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;gf_vect_dot_prod&lt;span class="o"&gt;(&lt;/span&gt;len&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;k&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;g_tbls&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;data&lt;span class="o"&gt;,&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;coding&lt;span class="o"&gt;)&lt;/span&gt;&lt;span class="err"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;g_tbls&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;+=&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;k&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="w"&gt; &lt;/span&gt;&lt;span class="m"&gt;32&lt;/span&gt;&lt;span class="err"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;coding&lt;span class="o"&gt;++&lt;/span&gt;&lt;span class="err"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;                &lt;/span&gt;rows&lt;span class="o"&gt;--&lt;/span&gt;&lt;span class="err"&gt;;&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="w"&gt;        &lt;/span&gt;&lt;span class="err"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;span class="err"&gt;}&lt;/span&gt;&lt;span class="w"&gt;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;h2&gt;Going faster still&lt;/h2&gt;
&lt;p&gt;Eagle eyed readers will notice that however we generate an output, we have to read all the input elements. This means that if we're doing a code with 10 data disks and 4 coding disks, we have to read each of the 10 inputs 4 times.&lt;/p&gt;
&lt;p&gt;We could do better if we could calculate multiple outputs for each pass through the inputs. This is a little fiddly to implement, but does lead to a speed improvement.&lt;/p&gt;
&lt;p&gt;ISA-L is an excellent example here. Intel goes up to 6 outputs at once: the number of outputs you can do is only limited by how many vector registers you have to put the various operands and results in.&lt;/p&gt;
&lt;h2&gt;Tips and tricks&lt;/h2&gt;
&lt;ul&gt;
&lt;li&gt;
&lt;p&gt;Benchmarking is tricky. I do the following on a bare-metal, idle machine, with ASLR off and pinned to an arbitrary hardware thread. (Code is for the &lt;a href="https://fishshell.com/"&gt;fish shell&lt;/a&gt;)&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;for x in (seq 1 50)
    setarch ppc64le -R taskset -c 24 erasure_code/gf_vect_mul_perf
end | awk &amp;#39;/MB/ {sum+=$13} END {print sum/50, &amp;quot;MB/s&amp;quot;}&amp;#39;
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Debugging is tricky; the more you can do in C and the less you do in assembly, the easier your life will be.&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Vector code is notoriously alignment-sensitive - if you can't figure out why something is wrong, check alignment. (Pro-tip: ISA-L does &lt;em&gt;not&lt;/em&gt; guarantee the alignment of the &lt;code&gt;gftbls&lt;/code&gt; parameter, and many of the tests supply an unaligned table from the stack. For testing &lt;code&gt;__attribute__((aligned(16)))&lt;/code&gt; is your friend!)&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Related: GCC is moving towards assignment over vector intrinsics, at least on Power:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;vector unsigned char a;
unsigned char * data;
// good, also handles word-aligned data with VSX
a = *(vector unsigned char *)data;
// bad, requires special handling of non-16-byte aligned data
a = vec_ld(0, (unsigned char *) data);
&lt;/pre&gt;&lt;/div&gt;


&lt;/li&gt;
&lt;/ul&gt;
&lt;h2&gt;Conclusion&lt;/h2&gt;
&lt;p&gt;Hopefully by this point you're equipped to figure out how your erasure coding library of choice works, and write your own optimised implementation (or maintain an implementation written by someone else).&lt;/p&gt;
&lt;p&gt;I've referred to a number of resources throughout this series:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;ISA-L &lt;a href="https://github.com/01org/isa-l"&gt;code&lt;/a&gt;, &lt;a href=""&gt;API description&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;Jerasure &lt;a href="http://jerasure.org/"&gt;code&lt;/a&gt;, &lt;a href="http://jerasure.org/jerasure-2.0/"&gt;docs&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;gf-complete &lt;a href="http://jerasure.org/"&gt;code&lt;/a&gt;, &lt;a href="http://jerasure.org/gf-complete-1.02/"&gt;docs&lt;/a&gt; &lt;/li&gt;
&lt;li&gt;&lt;a href="https://www.kernel.org/pub/linux/kernel/people/hpa/raid6.pdf"&gt;The mathematics of RAID-6&lt;/a&gt; (PDF), H. Peter Anvin&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;If you want to go deeper, I also read the following and found them quite helpful in understanding Galois Fields and Reed-Solomon coding:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href="https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19900019023.pdf"&gt;Tutorial on Reed-Solomon Error Correction Coding&lt;/a&gt; (PDF), William A. Geisel, NASA&lt;/li&gt;
&lt;li&gt;&lt;a href="https://ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/19900019023.pdf"&gt;Reed-Solomon error correction&lt;/a&gt; (PDF), BBC R&amp;amp;D White Paper WHP 031, C. K. P. Clarke.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a more rigorous mathematical approach to rings and fields, a university mathematics course may be of interest. For more on coding theory, a university course in electronics engineering may be helpful.&lt;/p&gt;</content><category term="erasure"></category><category term="raid"></category><category term="storage"></category></entry><entry><title>Erasure Coding for Programmers, Part 1</title><link href="http://sthbrx.github.io/blog/2017/03/20/erasure-coding-for-programmers-part-1/" rel="alternate"></link><published>2017-03-20T10:43:00+11:00</published><updated>2017-03-20T10:43:00+11:00</updated><author><name>Daniel Axtens</name></author><id>tag:sthbrx.github.io,2017-03-20:/blog/2017/03/20/erasure-coding-for-programmers-part-1/</id><summary type="html">&lt;p&gt;Erasure coding is an increasingly popular storage technology - allowing the same level of fault tolerance as replication with a significantly reduced storage footprint.&lt;/p&gt;
&lt;p&gt;Increasingly, erasure coding is available 'out of the box' on storage solutions such as Ceph and OpenStack Swift. Normally, you'd just pull in a library like &lt;a href="https://github.com/01org/isa-l"&gt;ISA-L&lt;/a&gt; or &lt;a href="http://jerasure.org"&gt;jerasure&lt;/a&gt;, and set some config options, and you'd be done.&lt;/p&gt;
&lt;p&gt;This post is not about that. This post is about how I went from knowing nothing about erasure coding to writing POWER optimised routines to make it go fast. (These are in the process of being polished for upstream at the moment.) If you want to understand how erasure coding works under the hood - and in particular if you're interested in writing optimised routines to make it run quickly in your platform - this is for you.&lt;/p&gt;
&lt;h2&gt;What are erasure codes anyway?&lt;/h2&gt;
&lt;p&gt;I think the easiest way to begin thinking about erasure codes is "RAID 6 on steroids". RAID 6 allows you to have up to 255 data disks and 2 parity disks (called P and Q), thus allowing you to tolerate the failure of up to 2 arbitrary disks without data loss.&lt;/p&gt;
&lt;p&gt;Erasure codes allow you to have k data disks and m 'parity' or coding disks. You then have a total of m + k disks, and you can tolerate the failure of up to m without losing data.&lt;/p&gt;
&lt;p&gt;The downside of erasure coding is that computing what to put on those parity disks is CPU intensive. Lets look at what we put on them.&lt;/p&gt;
&lt;h2&gt;RAID 6&lt;/h2&gt;
&lt;p&gt;RAID 6 is the easiest way to get started on understanding erasure codes for a number of reasons. H Peter Anvin's paper on RAID 6 in the Linux kernel is an excellent start, but does dive in a bit quickly to the underlying mathematics. So before reading that, read on!&lt;/p&gt;
&lt;h2&gt;Rings and Fields&lt;/h2&gt;
&lt;p&gt;As programmers we're pretty comfortable with modular arithmetic - the idea that if you have:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;unsigned char a = 255;
a++;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;the new value of &lt;code&gt;a&lt;/code&gt; will be 0, not 256.&lt;/p&gt;
&lt;p&gt;This is an example of an algebraic structure called a &lt;em&gt;ring&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Rings obey certain laws. For our purposes, we'll consider the following incomplete and somewhat simplified list:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is an addition operation.&lt;/li&gt;
&lt;li&gt;There is an additive identity (normally called 0), such that 'a + 0 = a'.&lt;/li&gt;
&lt;li&gt;Every element has an additive inverse, that is, for every element 'a', there is an element -a such that 'a + (-a) = 0'&lt;/li&gt;
&lt;li&gt;There is a multiplication operation.&lt;/li&gt;
&lt;li&gt;There is a multiplicative identity (normally called 1), such that 'a * 1 = a'.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These operations aren't necessarily addition or multiplication as we might expect from the integers or real numbers. For example, in our modular arithmetic example, we have 'wrap around'. (There are also certain rules the addition and multiplication rules must satisfy - we are glossing over them here.)&lt;/p&gt;
&lt;p&gt;One thing a ring doesn't have a 'multiplicative inverse'. The multiplicative inverse of some non-zero element of the ring (call it a), is the value b such that a * b = 1. (Often instead of b we write 'a^-1', but that looks bad in plain text, so we shall stick to b for now.)&lt;/p&gt;
&lt;p&gt;We do have some inverses in 'mod 256': the inverse of 3 is 171 as 3 * 171 = 513, and 513 = 1 mod 256, but there is no b such that 2 * b = 1 mod 256.&lt;/p&gt;
&lt;p&gt;If every non-zero element of our ring had a multiplicative inverse, we would have what is called a &lt;em&gt;field&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Now, let's look at a the integers modulo 2, that is, 0 and 1.&lt;/p&gt;
&lt;p&gt;We have this for addition:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;+&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Eagle-eyed readers will notice that this is the same as XOR.&lt;/p&gt;
&lt;p&gt;For multiplication: &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;*&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As we said, a field is a ring where every non-zero element has a multiplicative inverse. As we can see, the integers modulo 2 shown above is a field: it's a ring, and 1 is its own multiplicative inverse.&lt;/p&gt;
&lt;p&gt;So this is all well and good, but you can't really do very much in a field with 2 elements. This is sad, so we make bigger fields. For this application, we consider the Galois Field with 256 elements - GF(2^8). This field has some surprising and useful properties.&lt;/p&gt;
&lt;p&gt;Remember how we said that integers modulo 256 weren't a field because they didn't have multiplicative inverses? I also just said that GF(2^8) also has 256 elements, but is a field - i.e., it does have inverses! How does that work?&lt;/p&gt;
&lt;p&gt;Consider an element in GF(2^8). There are 2 ways to look at an element in GF(2^8). The first is to consider it as an 8-bit number. So, for example, let's take 100. We can express that as as an 8 bit binary number: 0b01100100.&lt;/p&gt;
&lt;p&gt;We can write that more explicitly as a sum of powers of 2:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;0 * 2^7 + 1 * 2^6 + 1 * 2^5 + 0 * 2^4 + 0 * 2^3 + 1 * 2^2 + 0 * 2 + 0 * 1
= 2^6 + 2^5 + 2^2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now the other way we can look at elements in GF(2^8) is to replace the '2's with 'x's, and consider them as polynomials. Each of our bits then represents the coefficient of a term of a polynomial, that is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;0 x^7 + 1 x^6 + 1 x^5 + 0 x^4 + 0 x^3 + 1 x^2 + 0 x + 0 * 1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;or more simply&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;x^6 + x^5 + x^2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, and this is &lt;strong&gt;important&lt;/strong&gt;: each of the coefficients are elements of the integers modulo 2: x + x = 2x = 0 as 2 mod 2 = 0. There is no concept of 'carrying' in this addition.&lt;/p&gt;
&lt;p&gt;Let's try: what's 100 + 79 in GF(2^8)?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;100 = 0b01100100 =&amp;gt; x^6 + x^5 +       x^2
 79 = 0b01001111 =&amp;gt; x^6 +       x^3 + x^2 + x + 1

100 + 79         =&amp;gt;   0 + x^5 + x^3 +   0 + x + 1
                 =    0b00101011 = 43
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So, 100 + 79 = 43 in GF(2^8)&lt;/p&gt;
&lt;p&gt;You may notice we could have done that much more efficiently: we can add numbers in GF(2^8) by just XORing their binary representations together. Subtraction, amusingly, is the same as addition: 0 + x = x =  0 - x, as -1 is congruent to 1 modulo 2.&lt;/p&gt;
&lt;p&gt;So at this point you might be wanting to explore a few additions yourself. Fortuantely there's a lovely tool that will allow you to do that:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo apt install gf-complete-tools
gf_add $A $B 8
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This will give you A + B in GF(2^8).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; gf_add 100 79 8
43
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Excellent!&lt;/p&gt;
&lt;p&gt;So, hold on to your hats, as this is where things get really weird. In modular arithmetic example, we considered the elements of our ring to be numbers, and we performed our addition and multiplication modulo 256. In GF(2^8), we consider our elements as polynomials and we perform our addition and multiplication modulo a polynomial. There is one conventional polynomial used in applications:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;0x11d =&amp;gt; 0b1 0001 1101 =&amp;gt; x^8 + x^4 + x^3 + x^2 + 1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It is possible to use other polynomials if they satisfy particular requirements, but for our applications we don't need to worry as we will always use 0x11d. I am not going to attempt to explain anything about this polynomial - take it as an article of faith.&lt;/p&gt;
&lt;p&gt;So when we multiply two numbers, we multiply their polynomial representations. Then, to find out what that is modulo 0x11d, we do polynomial long division by 0x11d, and take the remainder.&lt;/p&gt;
&lt;p&gt;Some examples will help.&lt;/p&gt;
&lt;p&gt;Let's multiply 100 by 3.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;100 = 0b01100100 =&amp;gt; x^6 + x^5 + x^2
  3 = 0b00000011 =&amp;gt; x + 1

(x^6 + x^5 + x^2)(x + 1) = x^7 + x^6 + x^3 + x^6 + x^5 + x^2
                         = x^7 + x^5 + x^3 + x^2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice that some of the terms have disappeared: x^6 + x^6 = 0.&lt;/p&gt;
&lt;p&gt;The degree (the largest power of a term) is 7. 7 is less than the degree of 0x11d, which is 8, so we don't need to do anything: the remainder modulo 0x11d is simply x^7 + x^5 + x^3 + x^2.&lt;/p&gt;
&lt;p&gt;In binary form, that is 0b10101100 = 172, so 100 * 3 = 172 in GF(2^8).&lt;/p&gt;
&lt;p&gt;Fortunately &lt;code&gt;gf-complete-tools&lt;/code&gt; also allows us to check multiplications:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; gf_mult 100 3 8
172
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Excellent!&lt;/p&gt;
&lt;p&gt;Now let's see what happens if we multiply by a larger number. Let's multiply 100 by 5.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;100 = 0b01100100 =&amp;gt; x^6 + x^5 + x^2
  5 = 0b00000101 =&amp;gt; x^2 + 1

(x^6 + x^5 + x^2)(x^2 + 1) = x^8 + x^7 + x^4 + x^6 + x^5 + x^2
                           = x^8 + x^7 + x^6 + x^5 + x^4 + x^2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here we have an x^8 term, so we have a degree of 8. This means will get a different remainder when we divide by our polynomial. We do this with polynomial long division, which you will hopefully remember if you did some solid algebra in high school.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;                              1
                           ---------------------------------------------
x^8 + x^4 + x^3 + x^2 + 1 | x^8 + x^7 + x^6 + x^5 + x^4       + x^2
                          - x^8                   + x^4 + x^3 + x^2 + 1
                            -------------------------------------------
                          =       x^7 + x^6 + x^5       + x^3       + 1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So we have that our original polynomial (x^8 + x^4 + x^3 + x^2 + 1) is congruent to (x^7 + x^6 + x^5 + x^3 + 1) modulo the polynomial 0x11d.
Looking at the binary representation of that new polynomial, we have 0b11101001 = 233.&lt;/p&gt;
&lt;p&gt;Sure enough:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; gf_mult 100 5 8
233
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Just to solidify the polynomial long division a bit, let's try a slightly larger example, 100 * 9:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;100 = 0b01100100 =&amp;gt; x^6 + x^5 + x^2
  9 = 0b00001001 =&amp;gt; x^3 + 1

(x^6 + x^5 + x^2)(x^3 + 1) = x^9 + x^8 + x^5 + x^6 + x^5 + x^2
                           = x^9 + x^8 + x^6 + x^2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Doing long division to reduce our result:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;                              x
                           -----------------------------------
x^8 + x^4 + x^3 + x^2 + 1 | x^9 + x^8       + x^6                   + x^2
                          - x^9                   + x^5 + x^4 + x^3       + x
                            -------------------------------------------------
                          =       x^8       + x^6 + x^5 + x^4 + x^3 + x^2 + x
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We still have a polynomial of degree 8, so we can do another step:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;                              x +   1
                           -----------------------------------
x^8 + x^4 + x^3 + x^2 + 1 | x^9 + x^8       + x^6                   + x^2
                          - x^9                   + x^5 + x^4 + x^3       + x
                            -------------------------------------------------
                          =       x^8       + x^6 + x^5 + x^4 + x^3 + x^2 + x
                          -       x^8                   + x^4 + x^3 + x^2     + 1
                                  -----------------------------------------------
                          =                   x^6 + x^5                   + x + 1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We now have a polynomial of degree less than 8 that is congruent to our original polynomial modulo 0x11d, and the binary form is 0x01100011 = 99.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; gf_mult 100 9 8
99
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This process can be done more efficiently, of course - but understanding what is going on will make you &lt;em&gt;much&lt;/em&gt; more comfortable with what is going on!&lt;/p&gt;
&lt;p&gt;I will not try to convince you that all multiplicative inverses exist in this magic shadow land of GF(2^8), but it's important for the rest of the algorithms to work that they do exist. Trust me on this.&lt;/p&gt;
&lt;h2&gt;Back to RAID 6&lt;/h2&gt;
&lt;p&gt;Equipped with this knowledge, you are ready to take on &lt;a href="https://www.kernel.org/pub/linux/kernel/people/hpa/raid6.pdf"&gt;RAID6 in the kernel&lt;/a&gt; (PDF) sections 1 - 2.&lt;/p&gt;
&lt;p&gt;Pause when you get to section 3 - this snippet is a bit magic and benefits from some explanation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Multiplication by {02} for a single byte can be implemeted using the C code:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;uint8_t c, cc;
cc = (c &amp;lt;&amp;lt; 1) ^ ((c &amp;amp; 0x80) ? 0x1d : 0);
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;How does this work? Well:&lt;/p&gt;
&lt;p&gt;Say you have a binary number 0bNMMM MMMM. Mutiplication by 2 gives you 0bNMMMMMMM0, which is 9 bits. Now, there are two cases to consider.&lt;/p&gt;
&lt;p&gt;If your leading bit (N) is 0, your product doesn't have an x^8 term, so we don't need to reduce it modulo the irreducible polynomial.&lt;/p&gt;
&lt;p&gt;If your leading bit is 1 however, your product is x^8 + something, which does need to be reduced. Fortunately, because we took an 8 bit number and multiplied it by 2, the largest term is x^8, so we only need to reduce it once. So we xor our number with our polynomial to subtract it.&lt;/p&gt;
&lt;p&gt;We implement this by letting the top bit overflow out and then xoring the lower 8 bits with the low 8 bits of the polynomial (0x1d)&lt;/p&gt;
&lt;p&gt;So, back to the original statement:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(c &amp;lt;&amp;lt; 1) ^ ((c &amp;amp; 0x80) ? 0x1d : 0)
    |          |          |     |
    &amp;gt; multiply by 2       |     |
               |          |     |
               &amp;gt; is the high bit set - will the product have an x^8 term?
                          |     |
                          &amp;gt; if so, reduce by the polynomial
                                |
                                &amp;gt; otherwise, leave alone
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Hopefully that makes sense.&lt;/p&gt;
&lt;h3&gt;Key points&lt;/h3&gt;
&lt;p&gt;It's critical you understand the section on Altivec (the vperm stuff), so let's cover it in a bit more detail.&lt;/p&gt;
&lt;p&gt;Say you want to do A * V, where A is a constant and V is an 8-bit variable. We can express V as V_a + V_b, where V_a is the top 4 bits of V, and V_b is the bottom 4 bits. A * V = A * V_a + A * V_b&lt;/p&gt;
&lt;p&gt;We can then make lookup tables for multiplication by A.&lt;/p&gt;
&lt;p&gt;If we did this in the most obvious way, we would need a 256 entry lookup table. But by splitting things into the top and bottom halves, we can reduce that to two 16 entry tables. For example, say A = 02.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;V_a&lt;/th&gt;
&lt;th&gt;A * V_a&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;00&lt;/td&gt;
&lt;td&gt;00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;01&lt;/td&gt;
&lt;td&gt;02&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;02&lt;/td&gt;
&lt;td&gt;04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0f&lt;/td&gt;
&lt;td&gt;1e&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;V_b&lt;/th&gt;
&lt;th&gt;A * V_b&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;00&lt;/td&gt;
&lt;td&gt;00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;f0&lt;/td&gt;
&lt;td&gt;fd&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We then use vperm to look up entries in these tables and vxor to combine our results.&lt;/p&gt;
&lt;p&gt;So - and this is a key point - for each A value we wish to multiply by, we need to generate a new lookup table.&lt;/p&gt;
&lt;p&gt;So if we wanted A = 03:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;V_a&lt;/th&gt;
&lt;th&gt;A * V_a&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;00&lt;/td&gt;
&lt;td&gt;00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;01&lt;/td&gt;
&lt;td&gt;03&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;02&lt;/td&gt;
&lt;td&gt;06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0f&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;V_b&lt;/th&gt;
&lt;th&gt;A * V_b&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;00&lt;/td&gt;
&lt;td&gt;00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;f0&lt;/td&gt;
&lt;td&gt;0d&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;One final thing is that Power8 adds a vpermxor instruction, so we can reduce the entire 4 instruction sequence in the paper:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;vsrb v1, v0, v14
vperm v2, v12, v12, v0
vperm v1, v13, v13, v1
vxor v1, v2, v1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;to 1 vpermxor:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;vpermxor v1, v12, v13, v0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Isn't POWER grand?&lt;/p&gt;
&lt;h2&gt;OK, but how does this relate to erasure codes?&lt;/h2&gt;
&lt;p&gt;I'm glad you asked.&lt;/p&gt;
&lt;p&gt;Galois Field arithmetic, and its application in RAID 6 is the basis for erasure coding. (It's also the basis for CRCs - two for the price of one!)&lt;/p&gt;
&lt;p&gt;But, that's all to come in part 2, which will definitely be published before 7 April!&lt;/p&gt;
&lt;p&gt;Many thanks to Sarah Axtens who reviewed the mathematical content of this post and suggested significant improvements. All errors and gross oversimplifications remain my own. Thanks also to the OzLabs crew for their feedback and comments.&lt;/p&gt;</summary><content type="html">&lt;p&gt;Erasure coding is an increasingly popular storage technology - allowing the same level of fault tolerance as replication with a significantly reduced storage footprint.&lt;/p&gt;
&lt;p&gt;Increasingly, erasure coding is available 'out of the box' on storage solutions such as Ceph and OpenStack Swift. Normally, you'd just pull in a library like &lt;a href="https://github.com/01org/isa-l"&gt;ISA-L&lt;/a&gt; or &lt;a href="http://jerasure.org"&gt;jerasure&lt;/a&gt;, and set some config options, and you'd be done.&lt;/p&gt;
&lt;p&gt;This post is not about that. This post is about how I went from knowing nothing about erasure coding to writing POWER optimised routines to make it go fast. (These are in the process of being polished for upstream at the moment.) If you want to understand how erasure coding works under the hood - and in particular if you're interested in writing optimised routines to make it run quickly in your platform - this is for you.&lt;/p&gt;
&lt;h2&gt;What are erasure codes anyway?&lt;/h2&gt;
&lt;p&gt;I think the easiest way to begin thinking about erasure codes is "RAID 6 on steroids". RAID 6 allows you to have up to 255 data disks and 2 parity disks (called P and Q), thus allowing you to tolerate the failure of up to 2 arbitrary disks without data loss.&lt;/p&gt;
&lt;p&gt;Erasure codes allow you to have k data disks and m 'parity' or coding disks. You then have a total of m + k disks, and you can tolerate the failure of up to m without losing data.&lt;/p&gt;
&lt;p&gt;The downside of erasure coding is that computing what to put on those parity disks is CPU intensive. Lets look at what we put on them.&lt;/p&gt;
&lt;h2&gt;RAID 6&lt;/h2&gt;
&lt;p&gt;RAID 6 is the easiest way to get started on understanding erasure codes for a number of reasons. H Peter Anvin's paper on RAID 6 in the Linux kernel is an excellent start, but does dive in a bit quickly to the underlying mathematics. So before reading that, read on!&lt;/p&gt;
&lt;h2&gt;Rings and Fields&lt;/h2&gt;
&lt;p&gt;As programmers we're pretty comfortable with modular arithmetic - the idea that if you have:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;unsigned char a = 255;
a++;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;the new value of &lt;code&gt;a&lt;/code&gt; will be 0, not 256.&lt;/p&gt;
&lt;p&gt;This is an example of an algebraic structure called a &lt;em&gt;ring&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Rings obey certain laws. For our purposes, we'll consider the following incomplete and somewhat simplified list:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;There is an addition operation.&lt;/li&gt;
&lt;li&gt;There is an additive identity (normally called 0), such that 'a + 0 = a'.&lt;/li&gt;
&lt;li&gt;Every element has an additive inverse, that is, for every element 'a', there is an element -a such that 'a + (-a) = 0'&lt;/li&gt;
&lt;li&gt;There is a multiplication operation.&lt;/li&gt;
&lt;li&gt;There is a multiplicative identity (normally called 1), such that 'a * 1 = a'.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These operations aren't necessarily addition or multiplication as we might expect from the integers or real numbers. For example, in our modular arithmetic example, we have 'wrap around'. (There are also certain rules the addition and multiplication rules must satisfy - we are glossing over them here.)&lt;/p&gt;
&lt;p&gt;One thing a ring doesn't have a 'multiplicative inverse'. The multiplicative inverse of some non-zero element of the ring (call it a), is the value b such that a * b = 1. (Often instead of b we write 'a^-1', but that looks bad in plain text, so we shall stick to b for now.)&lt;/p&gt;
&lt;p&gt;We do have some inverses in 'mod 256': the inverse of 3 is 171 as 3 * 171 = 513, and 513 = 1 mod 256, but there is no b such that 2 * b = 1 mod 256.&lt;/p&gt;
&lt;p&gt;If every non-zero element of our ring had a multiplicative inverse, we would have what is called a &lt;em&gt;field&lt;/em&gt;.&lt;/p&gt;
&lt;p&gt;Now, let's look at a the integers modulo 2, that is, 0 and 1.&lt;/p&gt;
&lt;p&gt;We have this for addition:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;+&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;Eagle-eyed readers will notice that this is the same as XOR.&lt;/p&gt;
&lt;p&gt;For multiplication: &lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;*&lt;/th&gt;
&lt;th&gt;0&lt;/th&gt;
&lt;th&gt;1&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;0&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;&lt;strong&gt;1&lt;/strong&gt;&lt;/td&gt;
&lt;td&gt;0&lt;/td&gt;
&lt;td&gt;1&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;As we said, a field is a ring where every non-zero element has a multiplicative inverse. As we can see, the integers modulo 2 shown above is a field: it's a ring, and 1 is its own multiplicative inverse.&lt;/p&gt;
&lt;p&gt;So this is all well and good, but you can't really do very much in a field with 2 elements. This is sad, so we make bigger fields. For this application, we consider the Galois Field with 256 elements - GF(2^8). This field has some surprising and useful properties.&lt;/p&gt;
&lt;p&gt;Remember how we said that integers modulo 256 weren't a field because they didn't have multiplicative inverses? I also just said that GF(2^8) also has 256 elements, but is a field - i.e., it does have inverses! How does that work?&lt;/p&gt;
&lt;p&gt;Consider an element in GF(2^8). There are 2 ways to look at an element in GF(2^8). The first is to consider it as an 8-bit number. So, for example, let's take 100. We can express that as as an 8 bit binary number: 0b01100100.&lt;/p&gt;
&lt;p&gt;We can write that more explicitly as a sum of powers of 2:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;0 * 2^7 + 1 * 2^6 + 1 * 2^5 + 0 * 2^4 + 0 * 2^3 + 1 * 2^2 + 0 * 2 + 0 * 1
= 2^6 + 2^5 + 2^2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now the other way we can look at elements in GF(2^8) is to replace the '2's with 'x's, and consider them as polynomials. Each of our bits then represents the coefficient of a term of a polynomial, that is:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;0 x^7 + 1 x^6 + 1 x^5 + 0 x^4 + 0 x^3 + 1 x^2 + 0 x + 0 * 1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;or more simply&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;x^6 + x^5 + x^2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Now, and this is &lt;strong&gt;important&lt;/strong&gt;: each of the coefficients are elements of the integers modulo 2: x + x = 2x = 0 as 2 mod 2 = 0. There is no concept of 'carrying' in this addition.&lt;/p&gt;
&lt;p&gt;Let's try: what's 100 + 79 in GF(2^8)?&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;100 = 0b01100100 =&amp;gt; x^6 + x^5 +       x^2
 79 = 0b01001111 =&amp;gt; x^6 +       x^3 + x^2 + x + 1

100 + 79         =&amp;gt;   0 + x^5 + x^3 +   0 + x + 1
                 =    0b00101011 = 43
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So, 100 + 79 = 43 in GF(2^8)&lt;/p&gt;
&lt;p&gt;You may notice we could have done that much more efficiently: we can add numbers in GF(2^8) by just XORing their binary representations together. Subtraction, amusingly, is the same as addition: 0 + x = x =  0 - x, as -1 is congruent to 1 modulo 2.&lt;/p&gt;
&lt;p&gt;So at this point you might be wanting to explore a few additions yourself. Fortuantely there's a lovely tool that will allow you to do that:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;sudo apt install gf-complete-tools
gf_add $A $B 8
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This will give you A + B in GF(2^8).&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; gf_add 100 79 8
43
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Excellent!&lt;/p&gt;
&lt;p&gt;So, hold on to your hats, as this is where things get really weird. In modular arithmetic example, we considered the elements of our ring to be numbers, and we performed our addition and multiplication modulo 256. In GF(2^8), we consider our elements as polynomials and we perform our addition and multiplication modulo a polynomial. There is one conventional polynomial used in applications:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;0x11d =&amp;gt; 0b1 0001 1101 =&amp;gt; x^8 + x^4 + x^3 + x^2 + 1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;It is possible to use other polynomials if they satisfy particular requirements, but for our applications we don't need to worry as we will always use 0x11d. I am not going to attempt to explain anything about this polynomial - take it as an article of faith.&lt;/p&gt;
&lt;p&gt;So when we multiply two numbers, we multiply their polynomial representations. Then, to find out what that is modulo 0x11d, we do polynomial long division by 0x11d, and take the remainder.&lt;/p&gt;
&lt;p&gt;Some examples will help.&lt;/p&gt;
&lt;p&gt;Let's multiply 100 by 3.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;100 = 0b01100100 =&amp;gt; x^6 + x^5 + x^2
  3 = 0b00000011 =&amp;gt; x + 1

(x^6 + x^5 + x^2)(x + 1) = x^7 + x^6 + x^3 + x^6 + x^5 + x^2
                         = x^7 + x^5 + x^3 + x^2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Notice that some of the terms have disappeared: x^6 + x^6 = 0.&lt;/p&gt;
&lt;p&gt;The degree (the largest power of a term) is 7. 7 is less than the degree of 0x11d, which is 8, so we don't need to do anything: the remainder modulo 0x11d is simply x^7 + x^5 + x^3 + x^2.&lt;/p&gt;
&lt;p&gt;In binary form, that is 0b10101100 = 172, so 100 * 3 = 172 in GF(2^8).&lt;/p&gt;
&lt;p&gt;Fortunately &lt;code&gt;gf-complete-tools&lt;/code&gt; also allows us to check multiplications:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; gf_mult 100 3 8
172
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Excellent!&lt;/p&gt;
&lt;p&gt;Now let's see what happens if we multiply by a larger number. Let's multiply 100 by 5.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;100 = 0b01100100 =&amp;gt; x^6 + x^5 + x^2
  5 = 0b00000101 =&amp;gt; x^2 + 1

(x^6 + x^5 + x^2)(x^2 + 1) = x^8 + x^7 + x^4 + x^6 + x^5 + x^2
                           = x^8 + x^7 + x^6 + x^5 + x^4 + x^2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Here we have an x^8 term, so we have a degree of 8. This means will get a different remainder when we divide by our polynomial. We do this with polynomial long division, which you will hopefully remember if you did some solid algebra in high school.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;                              1
                           ---------------------------------------------
x^8 + x^4 + x^3 + x^2 + 1 | x^8 + x^7 + x^6 + x^5 + x^4       + x^2
                          - x^8                   + x^4 + x^3 + x^2 + 1
                            -------------------------------------------
                          =       x^7 + x^6 + x^5       + x^3       + 1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;So we have that our original polynomial (x^8 + x^4 + x^3 + x^2 + 1) is congruent to (x^7 + x^6 + x^5 + x^3 + 1) modulo the polynomial 0x11d.
Looking at the binary representation of that new polynomial, we have 0b11101001 = 233.&lt;/p&gt;
&lt;p&gt;Sure enough:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; gf_mult 100 5 8
233
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Just to solidify the polynomial long division a bit, let's try a slightly larger example, 100 * 9:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;100 = 0b01100100 =&amp;gt; x^6 + x^5 + x^2
  9 = 0b00001001 =&amp;gt; x^3 + 1

(x^6 + x^5 + x^2)(x^3 + 1) = x^9 + x^8 + x^5 + x^6 + x^5 + x^2
                           = x^9 + x^8 + x^6 + x^2
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Doing long division to reduce our result:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;                              x
                           -----------------------------------
x^8 + x^4 + x^3 + x^2 + 1 | x^9 + x^8       + x^6                   + x^2
                          - x^9                   + x^5 + x^4 + x^3       + x
                            -------------------------------------------------
                          =       x^8       + x^6 + x^5 + x^4 + x^3 + x^2 + x
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We still have a polynomial of degree 8, so we can do another step:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;                              x +   1
                           -----------------------------------
x^8 + x^4 + x^3 + x^2 + 1 | x^9 + x^8       + x^6                   + x^2
                          - x^9                   + x^5 + x^4 + x^3       + x
                            -------------------------------------------------
                          =       x^8       + x^6 + x^5 + x^4 + x^3 + x^2 + x
                          -       x^8                   + x^4 + x^3 + x^2     + 1
                                  -----------------------------------------------
                          =                   x^6 + x^5                   + x + 1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We now have a polynomial of degree less than 8 that is congruent to our original polynomial modulo 0x11d, and the binary form is 0x01100011 = 99.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&amp;gt; gf_mult 100 9 8
99
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;This process can be done more efficiently, of course - but understanding what is going on will make you &lt;em&gt;much&lt;/em&gt; more comfortable with what is going on!&lt;/p&gt;
&lt;p&gt;I will not try to convince you that all multiplicative inverses exist in this magic shadow land of GF(2^8), but it's important for the rest of the algorithms to work that they do exist. Trust me on this.&lt;/p&gt;
&lt;h2&gt;Back to RAID 6&lt;/h2&gt;
&lt;p&gt;Equipped with this knowledge, you are ready to take on &lt;a href="https://www.kernel.org/pub/linux/kernel/people/hpa/raid6.pdf"&gt;RAID6 in the kernel&lt;/a&gt; (PDF) sections 1 - 2.&lt;/p&gt;
&lt;p&gt;Pause when you get to section 3 - this snippet is a bit magic and benefits from some explanation:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;Multiplication by {02} for a single byte can be implemeted using the C code:&lt;/p&gt;
&lt;/blockquote&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;uint8_t c, cc;
cc = (c &amp;lt;&amp;lt; 1) ^ ((c &amp;amp; 0x80) ? 0x1d : 0);
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;How does this work? Well:&lt;/p&gt;
&lt;p&gt;Say you have a binary number 0bNMMM MMMM. Mutiplication by 2 gives you 0bNMMMMMMM0, which is 9 bits. Now, there are two cases to consider.&lt;/p&gt;
&lt;p&gt;If your leading bit (N) is 0, your product doesn't have an x^8 term, so we don't need to reduce it modulo the irreducible polynomial.&lt;/p&gt;
&lt;p&gt;If your leading bit is 1 however, your product is x^8 + something, which does need to be reduced. Fortunately, because we took an 8 bit number and multiplied it by 2, the largest term is x^8, so we only need to reduce it once. So we xor our number with our polynomial to subtract it.&lt;/p&gt;
&lt;p&gt;We implement this by letting the top bit overflow out and then xoring the lower 8 bits with the low 8 bits of the polynomial (0x1d)&lt;/p&gt;
&lt;p&gt;So, back to the original statement:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;(c &amp;lt;&amp;lt; 1) ^ ((c &amp;amp; 0x80) ? 0x1d : 0)
    |          |          |     |
    &amp;gt; multiply by 2       |     |
               |          |     |
               &amp;gt; is the high bit set - will the product have an x^8 term?
                          |     |
                          &amp;gt; if so, reduce by the polynomial
                                |
                                &amp;gt; otherwise, leave alone
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Hopefully that makes sense.&lt;/p&gt;
&lt;h3&gt;Key points&lt;/h3&gt;
&lt;p&gt;It's critical you understand the section on Altivec (the vperm stuff), so let's cover it in a bit more detail.&lt;/p&gt;
&lt;p&gt;Say you want to do A * V, where A is a constant and V is an 8-bit variable. We can express V as V_a + V_b, where V_a is the top 4 bits of V, and V_b is the bottom 4 bits. A * V = A * V_a + A * V_b&lt;/p&gt;
&lt;p&gt;We can then make lookup tables for multiplication by A.&lt;/p&gt;
&lt;p&gt;If we did this in the most obvious way, we would need a 256 entry lookup table. But by splitting things into the top and bottom halves, we can reduce that to two 16 entry tables. For example, say A = 02.&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;V_a&lt;/th&gt;
&lt;th&gt;A * V_a&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;00&lt;/td&gt;
&lt;td&gt;00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;01&lt;/td&gt;
&lt;td&gt;02&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;02&lt;/td&gt;
&lt;td&gt;04&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0f&lt;/td&gt;
&lt;td&gt;1e&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;V_b&lt;/th&gt;
&lt;th&gt;A * V_b&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;00&lt;/td&gt;
&lt;td&gt;00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;40&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;f0&lt;/td&gt;
&lt;td&gt;fd&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;We then use vperm to look up entries in these tables and vxor to combine our results.&lt;/p&gt;
&lt;p&gt;So - and this is a key point - for each A value we wish to multiply by, we need to generate a new lookup table.&lt;/p&gt;
&lt;p&gt;So if we wanted A = 03:&lt;/p&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;V_a&lt;/th&gt;
&lt;th&gt;A * V_a&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;00&lt;/td&gt;
&lt;td&gt;00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;01&lt;/td&gt;
&lt;td&gt;03&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;02&lt;/td&gt;
&lt;td&gt;06&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;0f&lt;/td&gt;
&lt;td&gt;11&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;table&gt;
&lt;thead&gt;
&lt;tr&gt;
&lt;th&gt;V_b&lt;/th&gt;
&lt;th&gt;A * V_b&lt;/th&gt;
&lt;/tr&gt;
&lt;/thead&gt;
&lt;tbody&gt;
&lt;tr&gt;
&lt;td&gt;00&lt;/td&gt;
&lt;td&gt;00&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;10&lt;/td&gt;
&lt;td&gt;30&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;20&lt;/td&gt;
&lt;td&gt;60&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;td&gt;...&lt;/td&gt;
&lt;/tr&gt;
&lt;tr&gt;
&lt;td&gt;f0&lt;/td&gt;
&lt;td&gt;0d&lt;/td&gt;
&lt;/tr&gt;
&lt;/tbody&gt;
&lt;/table&gt;
&lt;p&gt;One final thing is that Power8 adds a vpermxor instruction, so we can reduce the entire 4 instruction sequence in the paper:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;vsrb v1, v0, v14
vperm v2, v12, v12, v0
vperm v1, v13, v13, v1
vxor v1, v2, v1
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;to 1 vpermxor:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;vpermxor v1, v12, v13, v0
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Isn't POWER grand?&lt;/p&gt;
&lt;h2&gt;OK, but how does this relate to erasure codes?&lt;/h2&gt;
&lt;p&gt;I'm glad you asked.&lt;/p&gt;
&lt;p&gt;Galois Field arithmetic, and its application in RAID 6 is the basis for erasure coding. (It's also the basis for CRCs - two for the price of one!)&lt;/p&gt;
&lt;p&gt;But, that's all to come in part 2, which will definitely be published before 7 April!&lt;/p&gt;
&lt;p&gt;Many thanks to Sarah Axtens who reviewed the mathematical content of this post and suggested significant improvements. All errors and gross oversimplifications remain my own. Thanks also to the OzLabs crew for their feedback and comments.&lt;/p&gt;</content><category term="erasure"></category><category term="raid"></category><category term="storage"></category></entry><entry><title>Kernel interfaces and vDSO test</title><link href="http://sthbrx.github.io/blog/2016/06/24/kernel-interfaces-and-vdso-test/" rel="alternate"></link><published>2016-06-24T16:30:00+10:00</published><updated>2016-06-24T16:30:00+10:00</updated><author><name>Cyril Bur</name></author><id>tag:sthbrx.github.io,2016-06-24:/blog/2016/06/24/kernel-interfaces-and-vdso-test/</id><summary type="html">&lt;h3&gt;Getting Suckered&lt;/h3&gt;
&lt;p&gt;Last week a colleague of mine came up to me and showed me some of the
&lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; on PowerPC and asked why on earth does it fail
&lt;a href="https://github.com/nlynch-mentor/vdsotest"&gt;vdsotest&lt;/a&gt;. I should come
clean at this point and admit that I knew very little about the &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt;
and hadn't heard of vdsotest. I had to admit to this colleague that I
had no idea everything looked super sane.&lt;/p&gt;
&lt;p&gt;Unfortunately (for me) I got hooked, vdsotest was saying it was
getting '22' instead of '-1' and it was the case where the &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; would
call into the kernel. It plagued me all night, 22 is so suspicious.
Right before I got to work the next morning I had an epiphany, "I bet
22 is EINVAL".&lt;/p&gt;
&lt;h3&gt;Virtual Dynamically linked Shared Objects&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/VDSO"&gt;&lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt;&lt;/a&gt; is a mechanism to
expose some kernel functionality into userspace to avoid the cost of a
context switch into kernel mode. This is a great feat of engineering,
avoiding the context switch can have a dramatic speedup for userspace
code. Obviously not all kernel functionality can be placed into
userspace and even for the functionality which can,
there may be edge cases in which the &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; needs to ask the kernel.&lt;/p&gt;
&lt;p&gt;Who tests the &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt;? For the portion that lies exclusively in userspace it
will escape all testing of the syscall interface which is really what
kernel developers are so focused on not breaking. Enter Nathan Lynch
with &lt;a href="https://github.com/nlynch-mentor/vdsotest"&gt;vdsotest&lt;/a&gt; who has
done some great work!&lt;/p&gt;
&lt;h3&gt;The Kernel&lt;/h3&gt;
&lt;p&gt;When the &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; can't get the correct value without the kernel, it
simply calls into the kernel because the kernel is the definitive
reference for every syscall. On PowerPC something like this happens
(sorry, our &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; is 100% asm):
&lt;sup id="fnref-1"&gt;&lt;a class="footnote-ref" href="#fn-1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;/*&lt;/span&gt;
 &lt;span class="err"&gt;*&lt;/span&gt; &lt;span class="nf"&gt;Exact&lt;/span&gt; &lt;span class="no"&gt;prototype&lt;/span&gt; &lt;span class="no"&gt;of&lt;/span&gt; &lt;span class="no"&gt;clock_gettime&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
 &lt;span class="err"&gt;*&lt;/span&gt;
 &lt;span class="err"&gt;*&lt;/span&gt; &lt;span class="nf"&gt;int&lt;/span&gt; &lt;span class="no"&gt;__kernel_clock_gettime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="no"&gt;clockid_t&lt;/span&gt; &lt;span class="no"&gt;clock_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="no"&gt;struct&lt;/span&gt; &lt;span class="no"&gt;timespec&lt;/span&gt; &lt;span class="p"&gt;*&lt;/span&gt;&lt;span class="no"&gt;tp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="c"&gt;;&lt;/span&gt;
 &lt;span class="p"&gt;*&lt;/span&gt;
 &lt;span class="err"&gt;*/&lt;/span&gt;
&lt;span class="nf"&gt;V_FUNCTION_BEGIN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="no"&gt;__kernel_clock_gettime&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="na"&gt;.cfi_startproc&lt;/span&gt;
    &lt;span class="err"&gt;/*&lt;/span&gt; &lt;span class="nf"&gt;Check&lt;/span&gt; &lt;span class="no"&gt;for&lt;/span&gt; &lt;span class="no"&gt;supported&lt;/span&gt; &lt;span class="no"&gt;clock&lt;/span&gt; &lt;span class="no"&gt;IDs&lt;/span&gt; &lt;span class="p"&gt;*&lt;/span&gt;&lt;span class="err"&gt;/&lt;/span&gt;
    &lt;span class="nf"&gt;cmpwi&lt;/span&gt;   &lt;span class="no"&gt;cr0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="no"&gt;r3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="no"&gt;CLOCK_REALTIME&lt;/span&gt;
    &lt;span class="nf"&gt;cmpwi&lt;/span&gt;   &lt;span class="no"&gt;cr1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="no"&gt;r3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="no"&gt;CLOCK_MONOTONIC&lt;/span&gt;
    &lt;span class="nf"&gt;cror&lt;/span&gt;    &lt;span class="no"&gt;cr0&lt;/span&gt;&lt;span class="p"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="err"&gt;+&lt;/span&gt;&lt;span class="no"&gt;eq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="no"&gt;cr0&lt;/span&gt;&lt;span class="p"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="err"&gt;+&lt;/span&gt;&lt;span class="no"&gt;eq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="no"&gt;cr1&lt;/span&gt;&lt;span class="p"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="err"&gt;+&lt;/span&gt;&lt;span class="no"&gt;eq&lt;/span&gt;
    &lt;span class="nf"&gt;bne&lt;/span&gt; &lt;span class="no"&gt;cr0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;99&lt;/span&gt;&lt;span class="no"&gt;f&lt;/span&gt;

    &lt;span class="err"&gt;/*&lt;/span&gt; &lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="nf"&gt;snip&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;*&lt;/span&gt;&lt;span class="err"&gt;/&lt;/span&gt;

    &lt;span class="err"&gt;/*&lt;/span&gt;
     &lt;span class="err"&gt;*&lt;/span&gt; &lt;span class="nf"&gt;syscall&lt;/span&gt; &lt;span class="no"&gt;fallback&lt;/span&gt;
     &lt;span class="err"&gt;*/&lt;/span&gt;
&lt;span class="err"&gt;99:&lt;/span&gt;
    &lt;span class="nf"&gt;li&lt;/span&gt;  &lt;span class="no"&gt;r0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="no"&gt;__NR_clock_gettime&lt;/span&gt;
    &lt;span class="nf"&gt;sc&lt;/span&gt;
    &lt;span class="nf"&gt;blr&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For those not familiar, this couldn't be more simple. The start checks
to see if it is a clock id that the &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; can handle and if not it jumps
to the 99 label. From here simply load the syscall number, jump to the
kernel and branch to link register aka 'return'.  In this case the
'return' statement would return to the userspace code which called the
&lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; function.&lt;/p&gt;
&lt;p&gt;Wait, having the &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; calling into the kernel call gets us the wrong
result? Or course it should, vdsotest is assuming a C ABI with return
values and errno but the kernel doesn't do that, the kernel ABI is
different. How does this even work on x86? Ohhhhh vdsotest does &lt;sup id="fnref-2"&gt;&lt;a class="footnote-ref" href="#fn-2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;record_syscall_result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;syscall_result&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;sr_ret&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;sr_errno&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="cm"&gt;/* Calling the vDSO directly instead of through libc can lead to:&lt;/span&gt;
&lt;span class="cm"&gt;     * - The vDSO code punts to the kernel (e.g. unrecognized clock id).&lt;/span&gt;
&lt;span class="cm"&gt;     * - The kernel returns an error (e.g. -22 (-EINVAL))&lt;/span&gt;
&lt;span class="cm"&gt;     * So we need to recognize this situation and fix things up.&lt;/span&gt;
&lt;span class="cm"&gt;     * Fortunately we&amp;#39;re dealing only with syscalls that return -ve values&lt;/span&gt;
&lt;span class="cm"&gt;     * on error.&lt;/span&gt;
&lt;span class="cm"&gt;     */&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sr_ret&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;sr_errno&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;sr_errno&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;sr_ret&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="n"&gt;sr_ret&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;syscall_result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sr_ret&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sr_ret&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sr_errno&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sr_errno&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;};&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That little hack isn't working on PowerPC and here's why:&lt;/p&gt;
&lt;p&gt;The kernel puts the return value in the ABI specified return register
(r3) and uses a condition register bit (condition register field 0, SO
bit), so unlike x86 on error the return value isn't negative. To make
matters worse, the condition register is very difficult to access from
C. Depending on your definition of 'access from C' you might consider
it impossible, a fixup like that would be impossible.&lt;/p&gt;
&lt;h3&gt;Lessons learnt&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; supplied functions aren't quite the same as their libc
counterparts. Unless you have very good reason, and to be fair,
vdsotest does have a very good reason, always access the &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; through
libc&lt;/li&gt;
&lt;li&gt;Kernel interfaces aren't C interfaces, yep, they're close but they
  aren't the same&lt;/li&gt;
&lt;li&gt;22 is in fact EINVAL&lt;/li&gt;
&lt;li&gt;Different architectures are... Different!&lt;/li&gt;
&lt;li&gt;Variety is the spice of life&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;P.S I have a hacky patch waiting review&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn-1"&gt;
&lt;p&gt;arch/powerpc/kernel/vdso64/gettimeofday.S&amp;#160;&lt;a class="footnote-backref" href="#fnref-1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-2"&gt;
&lt;p&gt;src/vdsotest.h&amp;#160;&lt;a class="footnote-backref" href="#fnref-2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</summary><content type="html">&lt;h3&gt;Getting Suckered&lt;/h3&gt;
&lt;p&gt;Last week a colleague of mine came up to me and showed me some of the
&lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; on PowerPC and asked why on earth does it fail
&lt;a href="https://github.com/nlynch-mentor/vdsotest"&gt;vdsotest&lt;/a&gt;. I should come
clean at this point and admit that I knew very little about the &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt;
and hadn't heard of vdsotest. I had to admit to this colleague that I
had no idea everything looked super sane.&lt;/p&gt;
&lt;p&gt;Unfortunately (for me) I got hooked, vdsotest was saying it was
getting '22' instead of '-1' and it was the case where the &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; would
call into the kernel. It plagued me all night, 22 is so suspicious.
Right before I got to work the next morning I had an epiphany, "I bet
22 is EINVAL".&lt;/p&gt;
&lt;h3&gt;Virtual Dynamically linked Shared Objects&lt;/h3&gt;
&lt;p&gt;The &lt;a href="https://en.wikipedia.org/wiki/VDSO"&gt;&lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt;&lt;/a&gt; is a mechanism to
expose some kernel functionality into userspace to avoid the cost of a
context switch into kernel mode. This is a great feat of engineering,
avoiding the context switch can have a dramatic speedup for userspace
code. Obviously not all kernel functionality can be placed into
userspace and even for the functionality which can,
there may be edge cases in which the &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; needs to ask the kernel.&lt;/p&gt;
&lt;p&gt;Who tests the &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt;? For the portion that lies exclusively in userspace it
will escape all testing of the syscall interface which is really what
kernel developers are so focused on not breaking. Enter Nathan Lynch
with &lt;a href="https://github.com/nlynch-mentor/vdsotest"&gt;vdsotest&lt;/a&gt; who has
done some great work!&lt;/p&gt;
&lt;h3&gt;The Kernel&lt;/h3&gt;
&lt;p&gt;When the &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; can't get the correct value without the kernel, it
simply calls into the kernel because the kernel is the definitive
reference for every syscall. On PowerPC something like this happens
(sorry, our &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; is 100% asm):
&lt;sup id="fnref-1"&gt;&lt;a class="footnote-ref" href="#fn-1"&gt;1&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="err"&gt;/*&lt;/span&gt;
 &lt;span class="err"&gt;*&lt;/span&gt; &lt;span class="nf"&gt;Exact&lt;/span&gt; &lt;span class="no"&gt;prototype&lt;/span&gt; &lt;span class="no"&gt;of&lt;/span&gt; &lt;span class="no"&gt;clock_gettime&lt;/span&gt;&lt;span class="p"&gt;()&lt;/span&gt;
 &lt;span class="err"&gt;*&lt;/span&gt;
 &lt;span class="err"&gt;*&lt;/span&gt; &lt;span class="nf"&gt;int&lt;/span&gt; &lt;span class="no"&gt;__kernel_clock_gettime&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="no"&gt;clockid_t&lt;/span&gt; &lt;span class="no"&gt;clock_id&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="no"&gt;struct&lt;/span&gt; &lt;span class="no"&gt;timespec&lt;/span&gt; &lt;span class="p"&gt;*&lt;/span&gt;&lt;span class="no"&gt;tp&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;&lt;span class="c"&gt;;&lt;/span&gt;
 &lt;span class="p"&gt;*&lt;/span&gt;
 &lt;span class="err"&gt;*/&lt;/span&gt;
&lt;span class="nf"&gt;V_FUNCTION_BEGIN&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="no"&gt;__kernel_clock_gettime&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
  &lt;span class="na"&gt;.cfi_startproc&lt;/span&gt;
    &lt;span class="err"&gt;/*&lt;/span&gt; &lt;span class="nf"&gt;Check&lt;/span&gt; &lt;span class="no"&gt;for&lt;/span&gt; &lt;span class="no"&gt;supported&lt;/span&gt; &lt;span class="no"&gt;clock&lt;/span&gt; &lt;span class="no"&gt;IDs&lt;/span&gt; &lt;span class="p"&gt;*&lt;/span&gt;&lt;span class="err"&gt;/&lt;/span&gt;
    &lt;span class="nf"&gt;cmpwi&lt;/span&gt;   &lt;span class="no"&gt;cr0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="no"&gt;r3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="no"&gt;CLOCK_REALTIME&lt;/span&gt;
    &lt;span class="nf"&gt;cmpwi&lt;/span&gt;   &lt;span class="no"&gt;cr1&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="no"&gt;r3&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="no"&gt;CLOCK_MONOTONIC&lt;/span&gt;
    &lt;span class="nf"&gt;cror&lt;/span&gt;    &lt;span class="no"&gt;cr0&lt;/span&gt;&lt;span class="p"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="err"&gt;+&lt;/span&gt;&lt;span class="no"&gt;eq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="no"&gt;cr0&lt;/span&gt;&lt;span class="p"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="err"&gt;+&lt;/span&gt;&lt;span class="no"&gt;eq&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="no"&gt;cr1&lt;/span&gt;&lt;span class="p"&gt;*&lt;/span&gt;&lt;span class="mi"&gt;4&lt;/span&gt;&lt;span class="err"&gt;+&lt;/span&gt;&lt;span class="no"&gt;eq&lt;/span&gt;
    &lt;span class="nf"&gt;bne&lt;/span&gt; &lt;span class="no"&gt;cr0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="mi"&gt;99&lt;/span&gt;&lt;span class="no"&gt;f&lt;/span&gt;

    &lt;span class="err"&gt;/*&lt;/span&gt; &lt;span class="err"&gt;[&lt;/span&gt;&lt;span class="nf"&gt;snip&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="p"&gt;*&lt;/span&gt;&lt;span class="err"&gt;/&lt;/span&gt;

    &lt;span class="err"&gt;/*&lt;/span&gt;
     &lt;span class="err"&gt;*&lt;/span&gt; &lt;span class="nf"&gt;syscall&lt;/span&gt; &lt;span class="no"&gt;fallback&lt;/span&gt;
     &lt;span class="err"&gt;*/&lt;/span&gt;
&lt;span class="err"&gt;99:&lt;/span&gt;
    &lt;span class="nf"&gt;li&lt;/span&gt;  &lt;span class="no"&gt;r0&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;&lt;span class="no"&gt;__NR_clock_gettime&lt;/span&gt;
    &lt;span class="nf"&gt;sc&lt;/span&gt;
    &lt;span class="nf"&gt;blr&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For those not familiar, this couldn't be more simple. The start checks
to see if it is a clock id that the &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; can handle and if not it jumps
to the 99 label. From here simply load the syscall number, jump to the
kernel and branch to link register aka 'return'.  In this case the
'return' statement would return to the userspace code which called the
&lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; function.&lt;/p&gt;
&lt;p&gt;Wait, having the &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; calling into the kernel call gets us the wrong
result? Or course it should, vdsotest is assuming a C ABI with return
values and errno but the kernel doesn't do that, the kernel ABI is
different. How does this even work on x86? Ohhhhh vdsotest does &lt;sup id="fnref-2"&gt;&lt;a class="footnote-ref" href="#fn-2"&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;static&lt;/span&gt; &lt;span class="kr"&gt;inline&lt;/span&gt; &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="nf"&gt;record_syscall_result&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;syscall_result&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
                     &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;sr_ret&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;sr_errno&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt;
&lt;span class="p"&gt;{&lt;/span&gt;
    &lt;span class="cm"&gt;/* Calling the vDSO directly instead of through libc can lead to:&lt;/span&gt;
&lt;span class="cm"&gt;     * - The vDSO code punts to the kernel (e.g. unrecognized clock id).&lt;/span&gt;
&lt;span class="cm"&gt;     * - The kernel returns an error (e.g. -22 (-EINVAL))&lt;/span&gt;
&lt;span class="cm"&gt;     * So we need to recognize this situation and fix things up.&lt;/span&gt;
&lt;span class="cm"&gt;     * Fortunately we&amp;#39;re dealing only with syscalls that return -ve values&lt;/span&gt;
&lt;span class="cm"&gt;     * on error.&lt;/span&gt;
&lt;span class="cm"&gt;     */&lt;/span&gt;
    &lt;span class="k"&gt;if&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="n"&gt;sr_ret&lt;/span&gt; &lt;span class="o"&gt;&amp;lt;&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&amp;amp;&lt;/span&gt; &lt;span class="n"&gt;sr_errno&lt;/span&gt; &lt;span class="o"&gt;==&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="n"&gt;sr_errno&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="n"&gt;sr_ret&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="n"&gt;sr_ret&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="o"&gt;-&lt;/span&gt;&lt;span class="mi"&gt;1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
    &lt;span class="p"&gt;}&lt;/span&gt;

    &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;res&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;syscall_result&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sr_ret&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sr_ret&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
        &lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;sr_errno&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;sr_errno&lt;/span&gt;&lt;span class="p"&gt;,&lt;/span&gt;
    &lt;span class="p"&gt;};&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;That little hack isn't working on PowerPC and here's why:&lt;/p&gt;
&lt;p&gt;The kernel puts the return value in the ABI specified return register
(r3) and uses a condition register bit (condition register field 0, SO
bit), so unlike x86 on error the return value isn't negative. To make
matters worse, the condition register is very difficult to access from
C. Depending on your definition of 'access from C' you might consider
it impossible, a fixup like that would be impossible.&lt;/p&gt;
&lt;h3&gt;Lessons learnt&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;&lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; supplied functions aren't quite the same as their libc
counterparts. Unless you have very good reason, and to be fair,
vdsotest does have a very good reason, always access the &lt;abbr title="Virtual Dynamically linked Shared Objects"&gt;vDSO&lt;/abbr&gt; through
libc&lt;/li&gt;
&lt;li&gt;Kernel interfaces aren't C interfaces, yep, they're close but they
  aren't the same&lt;/li&gt;
&lt;li&gt;22 is in fact EINVAL&lt;/li&gt;
&lt;li&gt;Different architectures are... Different!&lt;/li&gt;
&lt;li&gt;Variety is the spice of life&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;P.S I have a hacky patch waiting review&lt;/p&gt;
&lt;div class="footnote"&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id="fn-1"&gt;
&lt;p&gt;arch/powerpc/kernel/vdso64/gettimeofday.S&amp;#160;&lt;a class="footnote-backref" href="#fnref-1" title="Jump back to footnote 1 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id="fn-2"&gt;
&lt;p&gt;src/vdsotest.h&amp;#160;&lt;a class="footnote-backref" href="#fnref-2" title="Jump back to footnote 2 in the text"&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;</content><category term="kernel"></category><category term="asm"></category><category term="vdso"></category></entry><entry><title>Using the Atom editor for Linux kernel development</title><link href="http://sthbrx.github.io/blog/2016/06/07/using-the-atom-editor-for-linux-kernel-development/" rel="alternate"></link><published>2016-06-07T17:03:00+10:00</published><updated>2016-06-07T17:03:00+10:00</updated><author><name>Russell Currey</name></author><id>tag:sthbrx.github.io,2016-06-07:/blog/2016/06/07/using-the-atom-editor-for-linux-kernel-development/</id><summary type="html">&lt;p&gt;&lt;a href="https://atom.io"&gt;Atom&lt;/a&gt; is a text editor.  It's new, it's shiny, and it has a lot of good and bad sides.  I work in a lab full of kernel developers, and in the kernel, there are no IDEs.  There's no real metadata you can get out of your compiler (given the kernel isn't very clang friendly), there's certainly nothing like that you can get out of your build system, so "plain old" text editors reign supreme.  It's a vim or Emacs show.&lt;/p&gt;
&lt;p&gt;And so Atom comes along.  Unlike other shiny new text editors to emerge in the past 10 or so years, it's open source (unlike Sublime Text), it works well on Linux, and it's very configurable.  When it first came out, Atom was an absolute mess.  There was a noticeable delay whenever you typed a key.  That has gone, but the sour impression that comes from replacing a native application with a web browser in a frame remains.&lt;/p&gt;
&lt;p&gt;Like the curious person I am, I'm always trying out new things to see if they're any good.  I'm not particularly tied to any editor; I prefer modal editing, but I'm no vim wizard.  I eventually settled on using Emacs with evil-mode (which I assumed would make both Emacs and vim people like me, but the opposite happened), which was decent.  It was configurable, it was good, but it had issues.&lt;/p&gt;
&lt;p&gt;&lt;img alt="The Atom editor" src="/images/ruscur/36lOiMT.png"&gt;&lt;/p&gt;
&lt;p&gt;So, let's have a look at how Atom stacks up for low-level work.  First of all, it's X only.  You wouldn't use it to change one line of a file in /etc/, and a lot of kernel developers only edit code inside a terminal emulator.  Most vim people do this since gvim is a bit wonky, and Emacs people can double-dip; using Emacs without X for small things and Emacs with X for programming.  You don't want to do that with Atom, if nothing else because of its slow startup time.&lt;/p&gt;
&lt;p&gt;Now let's look at configurability.  In my opinion, no editor will ever match the level of configurability of Emacs, however the barrier to entry is much lower here.  Atom has lots of options exposed in a config file, and you can set them there or you can use an equivalent GUI.  In addition, a perk of being a browser in a frame is that you can customise a lot of UI things with CSS, for those inclined.  Overall, I'd say Emacs &amp;gt; Atom &amp;gt; vim here, but for a newbie, it's probably Atom &amp;gt; Emacs &amp;gt; vim.&lt;/p&gt;
&lt;p&gt;Okay, package management.  Atom is the clear winner here.  The package repository is very easy to use, for users and developers.  I wrote my own package, typed &lt;code&gt;apm publish&lt;/code&gt; and within a minute a friend could install it.  For kernel development though, you don't really need to install anything, Atom is pretty batteries-included.  This includes good syntax highlighting, ctags support, and a few themes.  In this respect, Atom feels like an editor that was created this century.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Atom's inbuilt package management" src="/images/ruscur/DAx7GqD.png"&gt;&lt;/p&gt;
&lt;p&gt;What about actually editing text?  Well, I only use modal editing, and Atom is very far from being the best vim.  I think evil-mode in Emacs is the best vim, followed closely by vim itself.  Atom has a vim-mode, and it's fine for insert/normal/visual mode, but anything involving a : is a no-go.  There's a plugin that's entirely useless.  If I tried to do a replacement with :s, Atom would lock up &lt;em&gt;and&lt;/em&gt; fail to replace the text.  vim replaced thousands of occurrences with in a second.  Other than that, Atom's pretty good.  I can move around pretty much just as well as I could in vim or Emacs, but not quite.  Also, it support ligatures!  The first kernel-usable editor that does.&lt;/p&gt;
&lt;p&gt;Autocompletions feel very good in Atom.  It completes within a local scope automatically, without any knowledge of the type of file you're working on.  As far as intelligence goes, Atom's support for tags outside of ctags is very lacking, and ctags is stupid.  Go-to definition &lt;em&gt;sometimes&lt;/em&gt; works, but it lags when dealing with something as big as the Linux kernel.  Return-from definition is very good, though.  Another downside is that it can complete from any open buffer, which is a huge problem if you're writing Rust in one tab and C in the other.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Atom's fuzzy file matching is pretty good" src="/images/ruscur/0PRiIUS.png"&gt;&lt;/p&gt;
&lt;p&gt;An experience I've had with Atom that I haven't had with other editors is actually writing a plugin.  It was really easy, mostly because I stole a lot of it from an existing plugin, but it was easy.  I wrote a syntax highlighting package for POWER assembly, which was much more fighting with regular expressions than it was fighting with anything in Atom.  Once I had it working, it was very easy to publish; just push to GitHub and run a command.&lt;/p&gt;
&lt;p&gt;Sometimes, Atom can get too clever for its own good.  For some completely insane reason, it automatically "fixes" whitespace in every file you open, leading to a huge amount of git changes you didn't intend.  That's easy to disable, but I don't want my editor doing that, it'd be much better if it highlighted whitespace it didn't like by default, like you can get vim and Emacs to do.  For an editor designed around git, I can't comprehend that decision.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Atom can also fuzzy match its commands" src="/images/ruscur/arbWXHx.png"&gt;&lt;/p&gt;
&lt;p&gt;Speaking of git, the editor pretty much has everything you'd expect for an editor written at GitHub.  The sidebar shows you what lines you've added, removed and modified, and the gutter shows you what branch you're on and how much you've changed all-up.  There's no in-built support for doing git things inside the editor, but there's a package for it.  It's pretty nice to get something "for free" that you'd have to tinker with in other editors.&lt;/p&gt;
&lt;p&gt;Overall, Atom has come a long way and still has a long way to go.  I've been using it for a few weeks and I'll continue to use it.  I'll encourage new developers to use it, but it needs to be better for experienced programmers who are used to their current workflow to consider switching.  If you're in the market for a new editor, Atom might just be for you.&lt;/p&gt;</summary><content type="html">&lt;p&gt;&lt;a href="https://atom.io"&gt;Atom&lt;/a&gt; is a text editor.  It's new, it's shiny, and it has a lot of good and bad sides.  I work in a lab full of kernel developers, and in the kernel, there are no IDEs.  There's no real metadata you can get out of your compiler (given the kernel isn't very clang friendly), there's certainly nothing like that you can get out of your build system, so "plain old" text editors reign supreme.  It's a vim or Emacs show.&lt;/p&gt;
&lt;p&gt;And so Atom comes along.  Unlike other shiny new text editors to emerge in the past 10 or so years, it's open source (unlike Sublime Text), it works well on Linux, and it's very configurable.  When it first came out, Atom was an absolute mess.  There was a noticeable delay whenever you typed a key.  That has gone, but the sour impression that comes from replacing a native application with a web browser in a frame remains.&lt;/p&gt;
&lt;p&gt;Like the curious person I am, I'm always trying out new things to see if they're any good.  I'm not particularly tied to any editor; I prefer modal editing, but I'm no vim wizard.  I eventually settled on using Emacs with evil-mode (which I assumed would make both Emacs and vim people like me, but the opposite happened), which was decent.  It was configurable, it was good, but it had issues.&lt;/p&gt;
&lt;p&gt;&lt;img alt="The Atom editor" src="/images/ruscur/36lOiMT.png"&gt;&lt;/p&gt;
&lt;p&gt;So, let's have a look at how Atom stacks up for low-level work.  First of all, it's X only.  You wouldn't use it to change one line of a file in /etc/, and a lot of kernel developers only edit code inside a terminal emulator.  Most vim people do this since gvim is a bit wonky, and Emacs people can double-dip; using Emacs without X for small things and Emacs with X for programming.  You don't want to do that with Atom, if nothing else because of its slow startup time.&lt;/p&gt;
&lt;p&gt;Now let's look at configurability.  In my opinion, no editor will ever match the level of configurability of Emacs, however the barrier to entry is much lower here.  Atom has lots of options exposed in a config file, and you can set them there or you can use an equivalent GUI.  In addition, a perk of being a browser in a frame is that you can customise a lot of UI things with CSS, for those inclined.  Overall, I'd say Emacs &amp;gt; Atom &amp;gt; vim here, but for a newbie, it's probably Atom &amp;gt; Emacs &amp;gt; vim.&lt;/p&gt;
&lt;p&gt;Okay, package management.  Atom is the clear winner here.  The package repository is very easy to use, for users and developers.  I wrote my own package, typed &lt;code&gt;apm publish&lt;/code&gt; and within a minute a friend could install it.  For kernel development though, you don't really need to install anything, Atom is pretty batteries-included.  This includes good syntax highlighting, ctags support, and a few themes.  In this respect, Atom feels like an editor that was created this century.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Atom's inbuilt package management" src="/images/ruscur/DAx7GqD.png"&gt;&lt;/p&gt;
&lt;p&gt;What about actually editing text?  Well, I only use modal editing, and Atom is very far from being the best vim.  I think evil-mode in Emacs is the best vim, followed closely by vim itself.  Atom has a vim-mode, and it's fine for insert/normal/visual mode, but anything involving a : is a no-go.  There's a plugin that's entirely useless.  If I tried to do a replacement with :s, Atom would lock up &lt;em&gt;and&lt;/em&gt; fail to replace the text.  vim replaced thousands of occurrences with in a second.  Other than that, Atom's pretty good.  I can move around pretty much just as well as I could in vim or Emacs, but not quite.  Also, it support ligatures!  The first kernel-usable editor that does.&lt;/p&gt;
&lt;p&gt;Autocompletions feel very good in Atom.  It completes within a local scope automatically, without any knowledge of the type of file you're working on.  As far as intelligence goes, Atom's support for tags outside of ctags is very lacking, and ctags is stupid.  Go-to definition &lt;em&gt;sometimes&lt;/em&gt; works, but it lags when dealing with something as big as the Linux kernel.  Return-from definition is very good, though.  Another downside is that it can complete from any open buffer, which is a huge problem if you're writing Rust in one tab and C in the other.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Atom's fuzzy file matching is pretty good" src="/images/ruscur/0PRiIUS.png"&gt;&lt;/p&gt;
&lt;p&gt;An experience I've had with Atom that I haven't had with other editors is actually writing a plugin.  It was really easy, mostly because I stole a lot of it from an existing plugin, but it was easy.  I wrote a syntax highlighting package for POWER assembly, which was much more fighting with regular expressions than it was fighting with anything in Atom.  Once I had it working, it was very easy to publish; just push to GitHub and run a command.&lt;/p&gt;
&lt;p&gt;Sometimes, Atom can get too clever for its own good.  For some completely insane reason, it automatically "fixes" whitespace in every file you open, leading to a huge amount of git changes you didn't intend.  That's easy to disable, but I don't want my editor doing that, it'd be much better if it highlighted whitespace it didn't like by default, like you can get vim and Emacs to do.  For an editor designed around git, I can't comprehend that decision.&lt;/p&gt;
&lt;p&gt;&lt;img alt="Atom can also fuzzy match its commands" src="/images/ruscur/arbWXHx.png"&gt;&lt;/p&gt;
&lt;p&gt;Speaking of git, the editor pretty much has everything you'd expect for an editor written at GitHub.  The sidebar shows you what lines you've added, removed and modified, and the gutter shows you what branch you're on and how much you've changed all-up.  There's no in-built support for doing git things inside the editor, but there's a package for it.  It's pretty nice to get something "for free" that you'd have to tinker with in other editors.&lt;/p&gt;
&lt;p&gt;Overall, Atom has come a long way and still has a long way to go.  I've been using it for a few weeks and I'll continue to use it.  I'll encourage new developers to use it, but it needs to be better for experienced programmers who are used to their current workflow to consider switching.  If you're in the market for a new editor, Atom might just be for you.&lt;/p&gt;</content><category term="education"></category><category term="kernel"></category><category term="development"></category><category term="tools"></category></entry><entry><title>SROP Mitigation</title><link href="http://sthbrx.github.io/blog/2016/05/13/srop-mitigation/" rel="alternate"></link><published>2016-05-13T22:22:00+10:00</published><updated>2016-05-13T22:22:00+10:00</updated><author><name>Rashmica Gupta</name></author><id>tag:sthbrx.github.io,2016-05-13:/blog/2016/05/13/srop-mitigation/</id><summary type="html">&lt;h2&gt;What is SROP?&lt;/h2&gt;
&lt;p&gt;Sigreturn Oriented Programming - a general technique that can be used as an exploit, or as a backdoor to exploit another vulnerability.&lt;/p&gt;
&lt;h2&gt;Okay, but what is it?&lt;/h2&gt;
&lt;p&gt;Yeah... Let me take you through some relevant background info, where I skimp on the details and give you the general picture.&lt;/p&gt;
&lt;p&gt;In Linux, software interrupts are called signals. More about signals &lt;a href="http://www.thegeekstuff.com/2012/03/linux-signals-fundamentals/"&gt;here&lt;/a&gt;! Generally a signal will convey some information from the kernel and so most signals will have a specific signal handler (some code that deals with the signal) setup.&lt;/p&gt;
&lt;p&gt;Signals are asynchronous - ie they can be sent to a process/program at anytime. When a signal arrives for a process, the kernel suspends the process. The kernel then saves the 'context' of the process - all the general purpose registers (GPRs), the stack pointer, the next-instruction pointer etc - into a structure called a 'sigframe'. The sigframe is stored on the stack, and then the kernel runs the signal handler. At the very end of the signal handler, it calls a special system call called 'sigreturn' - indicating to the kernel that the signal has been dealt with. The kernel then grabs the sigframe from the stack, restores the process's context and resumes the execution of the process.&lt;/p&gt;
&lt;p&gt;This is the rough mental picture you should have:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Double Format" src="/images/rashmica/picture.png"&gt;&lt;/p&gt;
&lt;h2&gt;Okay... but you still haven't explained what SROP is..?&lt;/h2&gt;
&lt;p&gt;Well, if you insist...&lt;/p&gt;
&lt;p&gt;The above process was designed so that the kernel does not need to keep track of what signals it has delivered. The kernel assumes that the sigframe it takes off the stack was legitimately put there by the kernel because of a signal. This is where we can trick the kernel!&lt;/p&gt;
&lt;p&gt;If we can construct a fake sigframe, put it on the stack, and call sigreturn, the kernel will assume that the sigframe is one it put there before and will load the contents of the fake context into the CPU's registers and 'resume' execution from where the fake sigframe tells it to. And that is what SROP is!&lt;/p&gt;
&lt;h2&gt;Well that sounds cool, show me!&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Firstly&lt;/strong&gt; we have to set up a (valid) sigframe:&lt;/p&gt;
&lt;p&gt;By valid sigframe, I mean a sigframe that the kernel will not reject. Luckily most architectures only examine a few parts of the sigframe to determine the validity of it. Unluckily, you will have to dive into the source code to find out which parts of the sigframe you need to set up for your architecture. Have a look in the function which deals with the syscall sigreturn (probably something like sys_sigreturn() ).&lt;/p&gt;
&lt;p&gt;For a real time signal on a little endian powerpc 64bit machine, the sigframe looks something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;rt_sigframe&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;ucontext&lt;/span&gt; &lt;span class="n"&gt;uc&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;_unused&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
        &lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;tramp&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;TRAMP_SIZE&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
        &lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;siginfo&lt;/span&gt; &lt;span class="n"&gt;__user&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;pinfo&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;__user&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;puc&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;siginfo&lt;/span&gt; &lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;user_cookie&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="cm"&gt;/* New 64 bit little-endian ABI allows redzone of 512 bytes below sp */&lt;/span&gt;
        &lt;span class="kt"&gt;char&lt;/span&gt; &lt;span class="n"&gt;abigap&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;USER_REDZONE_SIZE&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="n"&gt;__attribute__&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;aligned&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)));&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The most important part of the sigframe is the context or ucontext as this contains all the register values that will be written into the CPU's registers when the kernel loads in the sigframe. To minimise potential issues we can copy valid values from the current GPRs into our fake ucontext:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;register&lt;/span&gt; &lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;r1&lt;/span&gt; &lt;span class="nf"&gt;asm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;r1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="k"&gt;register&lt;/span&gt; &lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;r13&lt;/span&gt; &lt;span class="nf"&gt;asm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;r13&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;ucontext&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="p"&gt;};&lt;/span&gt;

&lt;span class="cm"&gt;/* We need a system thread id so copy the one from this process */&lt;/span&gt;
&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uc_mcontext&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gp_regs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;PT_R13&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r13&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="cm"&gt;/*  Set the context&amp;#39;s stack pointer to where the current stack pointer is pointing */&lt;/span&gt;
&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uc_mcontext&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gp_regs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;PT_R1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We also need to tell the kernel where to resume execution from. As this is just a test to see if we can successfully get the kernel to resume execution from a fake sigframe we will just point it to a function that prints out some text.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/* Set the next instruction pointer (NIP) to the code that we want executed */&lt;/span&gt;
&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uc_mcontext&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gp_regs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;PT_NIP&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;test_function&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For some reason the sys_rt_sigreturn() on little endian powerpc 64bit checks the endianess bit of the ucontext's MSR register, so we need to set that:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/* Set MSR bit if LE */&lt;/span&gt;
&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uc_mcontext&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gp_regs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;PT_MSR&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mh"&gt;0x01&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Fun fact: not doing this or setting it to 0 results in the CPU switching from little endian to big endian! For a powerpc machine sys_rt_sigreturn() only examines ucontext, so we do not need to set up a full sigframe.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Secondly&lt;/strong&gt; we have to put it on the stack:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/* Set current stack pointer to our fake context */&lt;/span&gt;
&lt;span class="n"&gt;r1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Thirdly&lt;/strong&gt;, we call sigreturn:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/* Syscall - NR_rt_sigreturn */&lt;/span&gt;
&lt;span class="k"&gt;asm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;li 0, 172&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="k"&gt;asm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;sc&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;When the kernel receives the sigreturn call, it looks at the userspace stack pointer for the ucontext and loads this in. As we have put valid values in the ucontext, the kernel assumes that this is a valid sigframe that it set up earlier and loads the contents of the ucontext in the CPU's registers "and resumes" execution of the process from the address we pointed the NIP to.&lt;/p&gt;
&lt;p&gt;Obviously, you need something worth executing at this address, but sadly that next part is not in my job description. This is a nice gateway into the kernel though and would pair nicely with another kernel vulnerability.  If you are interested in some more in depth examples, have a read of &lt;a href="http://www.cs.vu.nl/~herbertb/papers/srop_sp14.pdf"&gt;this&lt;/a&gt; paper.&lt;/p&gt;
&lt;h2&gt;So how can we mitigate this?&lt;/h2&gt;
&lt;p&gt;Well, I'm glad you asked. We need some way of distinguishing between sigframes that were put there legitimately by the kernel and 'fake' sigframes. The current idea that is being thrown around is cookies, and you can see the x86 discussion &lt;a href="https://lkml.org/lkml/2016/3/29/788"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The proposed solution is to give every sighand struct a randomly generated value. When the kernel constructs a sigframe for a process, it stores a 'cookie' with the sigframe. The cookie is a hash of the cookie's location and the random value stored in the sighand struct for the process. When the kernel receives a sigreturn, it hashes the location where the cookie should be with the randomly generated number in sighand struct - if this matches the cookie, the cookie is zeroed,  the sigframe is valid and the kernel will restore this context.  If the cookies do not match, the sigframe is not restored.&lt;/p&gt;
&lt;p&gt;Potential issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multithreading: Originally the random number was suggested to be stored in the task struct. However, this would break multi-threaded applications as every thread has its own task struct. As the sighand struct is shared by threads, this should not adversely affect multithreaded applications.&lt;/li&gt;
&lt;li&gt;Cookie location: At first I put the cookie on top of the sigframe. However some code in userspace assumed that all the space between the signal handler and the sigframe  was essentially up for grabs and would zero the cookie before I could read the cookie value. Putting the cookie below the sigframe was also a no-go due to the ABI-gap (a gap below the stack pointer that signal code cannot touch) being a part of the sigframe. Putting the cookie inside the sigframe, just above the ABI gap has been fine with all the tests I have run so far!&lt;/li&gt;
&lt;li&gt;Movement of sigframe: If you move the sigframe on the stack, the cookie value will no longer be valid... I don't think that this is something that you should be doing, and have not yet come across a scenario that does this. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a more in-depth explanation of SROP, click &lt;a href="https://lwn.net/Articles/676803/"&gt;here&lt;/a&gt;.&lt;/p&gt;</summary><content type="html">&lt;h2&gt;What is SROP?&lt;/h2&gt;
&lt;p&gt;Sigreturn Oriented Programming - a general technique that can be used as an exploit, or as a backdoor to exploit another vulnerability.&lt;/p&gt;
&lt;h2&gt;Okay, but what is it?&lt;/h2&gt;
&lt;p&gt;Yeah... Let me take you through some relevant background info, where I skimp on the details and give you the general picture.&lt;/p&gt;
&lt;p&gt;In Linux, software interrupts are called signals. More about signals &lt;a href="http://www.thegeekstuff.com/2012/03/linux-signals-fundamentals/"&gt;here&lt;/a&gt;! Generally a signal will convey some information from the kernel and so most signals will have a specific signal handler (some code that deals with the signal) setup.&lt;/p&gt;
&lt;p&gt;Signals are asynchronous - ie they can be sent to a process/program at anytime. When a signal arrives for a process, the kernel suspends the process. The kernel then saves the 'context' of the process - all the general purpose registers (GPRs), the stack pointer, the next-instruction pointer etc - into a structure called a 'sigframe'. The sigframe is stored on the stack, and then the kernel runs the signal handler. At the very end of the signal handler, it calls a special system call called 'sigreturn' - indicating to the kernel that the signal has been dealt with. The kernel then grabs the sigframe from the stack, restores the process's context and resumes the execution of the process.&lt;/p&gt;
&lt;p&gt;This is the rough mental picture you should have:&lt;/p&gt;
&lt;p&gt;&lt;img alt="Double Format" src="/images/rashmica/picture.png"&gt;&lt;/p&gt;
&lt;h2&gt;Okay... but you still haven't explained what SROP is..?&lt;/h2&gt;
&lt;p&gt;Well, if you insist...&lt;/p&gt;
&lt;p&gt;The above process was designed so that the kernel does not need to keep track of what signals it has delivered. The kernel assumes that the sigframe it takes off the stack was legitimately put there by the kernel because of a signal. This is where we can trick the kernel!&lt;/p&gt;
&lt;p&gt;If we can construct a fake sigframe, put it on the stack, and call sigreturn, the kernel will assume that the sigframe is one it put there before and will load the contents of the fake context into the CPU's registers and 'resume' execution from where the fake sigframe tells it to. And that is what SROP is!&lt;/p&gt;
&lt;h2&gt;Well that sounds cool, show me!&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Firstly&lt;/strong&gt; we have to set up a (valid) sigframe:&lt;/p&gt;
&lt;p&gt;By valid sigframe, I mean a sigframe that the kernel will not reject. Luckily most architectures only examine a few parts of the sigframe to determine the validity of it. Unluckily, you will have to dive into the source code to find out which parts of the sigframe you need to set up for your architecture. Have a look in the function which deals with the syscall sigreturn (probably something like sys_sigreturn() ).&lt;/p&gt;
&lt;p&gt;For a real time signal on a little endian powerpc 64bit machine, the sigframe looks something like this:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;rt_sigframe&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt;
        &lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;ucontext&lt;/span&gt; &lt;span class="n"&gt;uc&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;_unused&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="mi"&gt;2&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
        &lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;int&lt;/span&gt; &lt;span class="n"&gt;tramp&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;TRAMP_SIZE&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
        &lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;siginfo&lt;/span&gt; &lt;span class="n"&gt;__user&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;pinfo&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="kt"&gt;void&lt;/span&gt; &lt;span class="n"&gt;__user&lt;/span&gt; &lt;span class="o"&gt;*&lt;/span&gt;&lt;span class="n"&gt;puc&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;siginfo&lt;/span&gt; &lt;span class="n"&gt;info&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;user_cookie&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
        &lt;span class="cm"&gt;/* New 64 bit little-endian ABI allows redzone of 512 bytes below sp */&lt;/span&gt;
        &lt;span class="kt"&gt;char&lt;/span&gt; &lt;span class="n"&gt;abigap&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;USER_REDZONE_SIZE&lt;/span&gt;&lt;span class="p"&gt;];&lt;/span&gt;
&lt;span class="p"&gt;}&lt;/span&gt; &lt;span class="n"&gt;__attribute__&lt;/span&gt; &lt;span class="p"&gt;((&lt;/span&gt;&lt;span class="n"&gt;aligned&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="mi"&gt;16&lt;/span&gt;&lt;span class="p"&gt;)));&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;The most important part of the sigframe is the context or ucontext as this contains all the register values that will be written into the CPU's registers when the kernel loads in the sigframe. To minimise potential issues we can copy valid values from the current GPRs into our fake ucontext:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="k"&gt;register&lt;/span&gt; &lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;r1&lt;/span&gt; &lt;span class="nf"&gt;asm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;r1&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="k"&gt;register&lt;/span&gt; &lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt; &lt;span class="n"&gt;r13&lt;/span&gt; &lt;span class="nf"&gt;asm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;r13&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="k"&gt;struct&lt;/span&gt; &lt;span class="n"&gt;ucontext&lt;/span&gt; &lt;span class="n"&gt;ctx&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;{&lt;/span&gt; &lt;span class="mi"&gt;0&lt;/span&gt; &lt;span class="p"&gt;};&lt;/span&gt;

&lt;span class="cm"&gt;/* We need a system thread id so copy the one from this process */&lt;/span&gt;
&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uc_mcontext&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gp_regs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;PT_R13&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r13&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;

&lt;span class="cm"&gt;/*  Set the context&amp;#39;s stack pointer to where the current stack pointer is pointing */&lt;/span&gt;
&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uc_mcontext&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gp_regs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;PT_R1&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="n"&gt;r1&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;We also need to tell the kernel where to resume execution from. As this is just a test to see if we can successfully get the kernel to resume execution from a fake sigframe we will just point it to a function that prints out some text.&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/* Set the next instruction pointer (NIP) to the code that we want executed */&lt;/span&gt;
&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uc_mcontext&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gp_regs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;PT_NIP&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="n"&gt;test_function&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;For some reason the sys_rt_sigreturn() on little endian powerpc 64bit checks the endianess bit of the ucontext's MSR register, so we need to set that:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/* Set MSR bit if LE */&lt;/span&gt;
&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;uc_mcontext&lt;/span&gt;&lt;span class="p"&gt;.&lt;/span&gt;&lt;span class="n"&gt;gp_regs&lt;/span&gt;&lt;span class="p"&gt;[&lt;/span&gt;&lt;span class="n"&gt;PT_MSR&lt;/span&gt;&lt;span class="p"&gt;]&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="mh"&gt;0x01&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;Fun fact: not doing this or setting it to 0 results in the CPU switching from little endian to big endian! For a powerpc machine sys_rt_sigreturn() only examines ucontext, so we do not need to set up a full sigframe.&lt;/p&gt;
&lt;p&gt;&lt;strong&gt;Secondly&lt;/strong&gt; we have to put it on the stack:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/* Set current stack pointer to our fake context */&lt;/span&gt;
&lt;span class="n"&gt;r1&lt;/span&gt; &lt;span class="o"&gt;=&lt;/span&gt; &lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="kt"&gt;unsigned&lt;/span&gt; &lt;span class="kt"&gt;long&lt;/span&gt;&lt;span class="p"&gt;)&lt;/span&gt; &lt;span class="o"&gt;&amp;amp;&lt;/span&gt;&lt;span class="n"&gt;ctx&lt;/span&gt;&lt;span class="p"&gt;;&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;&lt;strong&gt;Thirdly&lt;/strong&gt;, we call sigreturn:&lt;/p&gt;
&lt;div class="highlight"&gt;&lt;pre&gt;&lt;span&gt;&lt;/span&gt;&lt;span class="cm"&gt;/* Syscall - NR_rt_sigreturn */&lt;/span&gt;
&lt;span class="k"&gt;asm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;li 0, 172&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;span class="k"&gt;asm&lt;/span&gt;&lt;span class="p"&gt;(&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;sc&lt;/span&gt;&lt;span class="se"&gt;\n&lt;/span&gt;&lt;span class="s"&gt;&amp;quot;&lt;/span&gt;&lt;span class="p"&gt;);&lt;/span&gt;
&lt;/pre&gt;&lt;/div&gt;


&lt;p&gt;When the kernel receives the sigreturn call, it looks at the userspace stack pointer for the ucontext and loads this in. As we have put valid values in the ucontext, the kernel assumes that this is a valid sigframe that it set up earlier and loads the contents of the ucontext in the CPU's registers "and resumes" execution of the process from the address we pointed the NIP to.&lt;/p&gt;
&lt;p&gt;Obviously, you need something worth executing at this address, but sadly that next part is not in my job description. This is a nice gateway into the kernel though and would pair nicely with another kernel vulnerability.  If you are interested in some more in depth examples, have a read of &lt;a href="http://www.cs.vu.nl/~herbertb/papers/srop_sp14.pdf"&gt;this&lt;/a&gt; paper.&lt;/p&gt;
&lt;h2&gt;So how can we mitigate this?&lt;/h2&gt;
&lt;p&gt;Well, I'm glad you asked. We need some way of distinguishing between sigframes that were put there legitimately by the kernel and 'fake' sigframes. The current idea that is being thrown around is cookies, and you can see the x86 discussion &lt;a href="https://lkml.org/lkml/2016/3/29/788"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;The proposed solution is to give every sighand struct a randomly generated value. When the kernel constructs a sigframe for a process, it stores a 'cookie' with the sigframe. The cookie is a hash of the cookie's location and the random value stored in the sighand struct for the process. When the kernel receives a sigreturn, it hashes the location where the cookie should be with the randomly generated number in sighand struct - if this matches the cookie, the cookie is zeroed,  the sigframe is valid and the kernel will restore this context.  If the cookies do not match, the sigframe is not restored.&lt;/p&gt;
&lt;p&gt;Potential issues:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;Multithreading: Originally the random number was suggested to be stored in the task struct. However, this would break multi-threaded applications as every thread has its own task struct. As the sighand struct is shared by threads, this should not adversely affect multithreaded applications.&lt;/li&gt;
&lt;li&gt;Cookie location: At first I put the cookie on top of the sigframe. However some code in userspace assumed that all the space between the signal handler and the sigframe  was essentially up for grabs and would zero the cookie before I could read the cookie value. Putting the cookie below the sigframe was also a no-go due to the ABI-gap (a gap below the stack pointer that signal code cannot touch) being a part of the sigframe. Putting the cookie inside the sigframe, just above the ABI gap has been fine with all the tests I have run so far!&lt;/li&gt;
&lt;li&gt;Movement of sigframe: If you move the sigframe on the stack, the cookie value will no longer be valid... I don't think that this is something that you should be doing, and have not yet come across a scenario that does this. &lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;For a more in-depth explanation of SROP, click &lt;a href="https://lwn.net/Articles/676803/"&gt;here&lt;/a&gt;.&lt;/p&gt;</content><category term="SROP"></category><category term="mitigation"></category><category term="kernel"></category></entry><entry><title>The Elegance of the Plaintext Patch</title><link href="http://sthbrx.github.io/blog/2016/03/22/the-elegance-of-the-plaintext-patch/" rel="alternate"></link><published>2016-03-22T13:53:00+11:00</published><updated>2016-03-22T13:53:00+11:00</updated><author><name>Russell Currey</name></author><id>tag:sthbrx.github.io,2016-03-22:/blog/2016/03/22/the-elegance-of-the-plaintext-patch/</id><summary type="html">&lt;p&gt;I've only been working on the Linux kernel for a few months.  Before that, I worked with proprietary source control at work and common tools like GitHub at home.  The concept of the mailing list seemed obtuse to me.  If I noticed a problem with some program, I'd be willing to open an issue on GitHub but not to send an email to a mailing list.  Who still uses those, anyway?&lt;/p&gt;
&lt;p&gt;Starting out with the kernel meant I had to figure this email thing out.  &lt;code&gt;git format-patch&lt;/code&gt; and &lt;code&gt;git send-email&lt;/code&gt; take most of the pain out of formatting and submitting a patch, which is nice.  The patch files generated by &lt;code&gt;format-patch&lt;/code&gt; open nicely in Emacs by default, showing all whitespace and letting you pick up any irregularities.  &lt;code&gt;send-email&lt;/code&gt; means you can send it to yourself or a friend first, finding anything that looks stupid before being exposed to the public.&lt;/p&gt;
&lt;p&gt;And then what?  You've sent an email.  It gets sent to hundreds or thousands of people.  Nowhere near that many will read it.  Some might miss it due to their mail server going down, or the list flagging your post as spam, or requiring moderation.  Some recipients will be bots that archive mail on the list, or publish information about the patch.  If you haven't formatted it correctly, someone will let you know quickly.  If your patch is important or controversial, you'll have all sorts of responses.  If your patch is small or niche, you might not ever hear anything back.&lt;/p&gt;
&lt;p&gt;I remember when I sent my first patch.  I was talking to a former colleague who didn't understand the patch/mailing list workflow at all.  I sent him a link to my patch on a mail archive.  I explained it like a pull request - here's my code, you can find the responses.  What's missing from a GitHub-esque pull request?  We don't know what tests it passed.  We don't know if it's been merged yet, or if the maintainer has looked at it.  It takes a bit of digging around to find out who's commented on it.  If it's part of a series, that's awkward to find out as well.  What about revisions of a series?  That's another pain point.&lt;/p&gt;
&lt;p&gt;Luckily, these problems do have solutions.  &lt;a href="http://jk.ozlabs.org/projects/patchwork/"&gt;Patchwork&lt;/a&gt;, written by fellow OzLabs member &lt;a href="http://jk.ozlabs.org"&gt;Jeremy Kerr&lt;/a&gt;, changes the way we work with patches.  Project maintainers rely on Pathwork instances, such as &lt;a href="https://patchwork.ozlabs.org"&gt;https://patchwork.ozlabs.org&lt;/a&gt;, for their day-to-day workflow: tagging reviewers, marking the status of patches, keeping track of tests, acks, reviews and comments in one place.  Missing from this picture is support for series and revisions, which is a feature that's being developed by the &lt;a href="https://www.freedesktop.org/wiki/"&gt;freedesktop&lt;/a&gt; project.  You can check out their changes in action &lt;a href="https://patchwork.freedesktop.org"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So, Patchwork helps patches and email catch up to what GitHub has in terms of ease of information.  We're still missing testing and other hooks.  What about review?  What can we do with email, compared to GitHub and the like?&lt;/p&gt;
&lt;p&gt;In my opinion, the biggest feature of email is the ease of review.  Just reply inline and you're done.  There's inline commenting on GitHub and GitLab, which works well but is a bit tacky, people commenting on the same thing overlap and conflict, each comment generates a notification (which can be an email until you turn that off).  Plus, since it's email, it's really easy to bring in additional people to the conversation as necessary.  If there's a super lengthy technical discussion in the kernel, it might just take Linus to resolve.&lt;/p&gt;
&lt;p&gt;There are alternatives to just replying to email, too, such as &lt;a href="https://www.gerritcodereview.com/"&gt;Gerrit&lt;/a&gt;.  Gerrit's pretty popular, and has a huge amount of features.  I understand why people use it, though I'm not much of a fan.  Reason being, it doesn't add to the email workflow, it replaces it.  Plaintext email is supported on pretty much any device, with a bunch of different programs.  From the goals of Patchwork: "patchwork should supplement mailing lists, not replace them".&lt;/p&gt;
&lt;p&gt;Linus Torvalds famously explained why he prefers email over GitHub pull requests &lt;a href="https://github.com/torvalds/linux/pull/17"&gt;here&lt;/a&gt;, using &lt;a href="https://groups.google.com/forum/#!topic/linux.kernel/w957vpu3PPU"&gt;this&lt;/a&gt; pull request from Ben Herrenschmidt as an example of why git's own pull request format is superior to that of GitHub.  Damien Lespiau, who is working on the freedesktop Patchwork fork, &lt;a href="http://damien.lespiau.name/2016/02/augmenting-mailing-lists-with-patchwork.html"&gt;outlines on his blog&lt;/a&gt; all the issues he has with mailing list workflows and why he thinks mailing lists are a relic of the past.  His work on Patchwork has gone a long way to help fix those problems, however I don't think mailing lists are outdated and superceded, I think they are timeless.  They are a technology-agnostic, simple and free system that will still be around if GitHub dies or alienates its community.&lt;/p&gt;
&lt;p&gt;That said, there's still the case of the missing features.  What about automated testing?  What about developer feedback?  What about making a maintainer's life easier?  We've been working on improving these issues, and I'll outline how we're approaching them in a future post.&lt;/p&gt;</summary><content type="html">&lt;p&gt;I've only been working on the Linux kernel for a few months.  Before that, I worked with proprietary source control at work and common tools like GitHub at home.  The concept of the mailing list seemed obtuse to me.  If I noticed a problem with some program, I'd be willing to open an issue on GitHub but not to send an email to a mailing list.  Who still uses those, anyway?&lt;/p&gt;
&lt;p&gt;Starting out with the kernel meant I had to figure this email thing out.  &lt;code&gt;git format-patch&lt;/code&gt; and &lt;code&gt;git send-email&lt;/code&gt; take most of the pain out of formatting and submitting a patch, which is nice.  The patch files generated by &lt;code&gt;format-patch&lt;/code&gt; open nicely in Emacs by default, showing all whitespace and letting you pick up any irregularities.  &lt;code&gt;send-email&lt;/code&gt; means you can send it to yourself or a friend first, finding anything that looks stupid before being exposed to the public.&lt;/p&gt;
&lt;p&gt;And then what?  You've sent an email.  It gets sent to hundreds or thousands of people.  Nowhere near that many will read it.  Some might miss it due to their mail server going down, or the list flagging your post as spam, or requiring moderation.  Some recipients will be bots that archive mail on the list, or publish information about the patch.  If you haven't formatted it correctly, someone will let you know quickly.  If your patch is important or controversial, you'll have all sorts of responses.  If your patch is small or niche, you might not ever hear anything back.&lt;/p&gt;
&lt;p&gt;I remember when I sent my first patch.  I was talking to a former colleague who didn't understand the patch/mailing list workflow at all.  I sent him a link to my patch on a mail archive.  I explained it like a pull request - here's my code, you can find the responses.  What's missing from a GitHub-esque pull request?  We don't know what tests it passed.  We don't know if it's been merged yet, or if the maintainer has looked at it.  It takes a bit of digging around to find out who's commented on it.  If it's part of a series, that's awkward to find out as well.  What about revisions of a series?  That's another pain point.&lt;/p&gt;
&lt;p&gt;Luckily, these problems do have solutions.  &lt;a href="http://jk.ozlabs.org/projects/patchwork/"&gt;Patchwork&lt;/a&gt;, written by fellow OzLabs member &lt;a href="http://jk.ozlabs.org"&gt;Jeremy Kerr&lt;/a&gt;, changes the way we work with patches.  Project maintainers rely on Pathwork instances, such as &lt;a href="https://patchwork.ozlabs.org"&gt;https://patchwork.ozlabs.org&lt;/a&gt;, for their day-to-day workflow: tagging reviewers, marking the status of patches, keeping track of tests, acks, reviews and comments in one place.  Missing from this picture is support for series and revisions, which is a feature that's being developed by the &lt;a href="https://www.freedesktop.org/wiki/"&gt;freedesktop&lt;/a&gt; project.  You can check out their changes in action &lt;a href="https://patchwork.freedesktop.org"&gt;here&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;So, Patchwork helps patches and email catch up to what GitHub has in terms of ease of information.  We're still missing testing and other hooks.  What about review?  What can we do with email, compared to GitHub and the like?&lt;/p&gt;
&lt;p&gt;In my opinion, the biggest feature of email is the ease of review.  Just reply inline and you're done.  There's inline commenting on GitHub and GitLab, which works well but is a bit tacky, people commenting on the same thing overlap and conflict, each comment generates a notification (which can be an email until you turn that off).  Plus, since it's email, it's really easy to bring in additional people to the conversation as necessary.  If there's a super lengthy technical discussion in the kernel, it might just take Linus to resolve.&lt;/p&gt;
&lt;p&gt;There are alternatives to just replying to email, too, such as &lt;a href="https://www.gerritcodereview.com/"&gt;Gerrit&lt;/a&gt;.  Gerrit's pretty popular, and has a huge amount of features.  I understand why people use it, though I'm not much of a fan.  Reason being, it doesn't add to the email workflow, it replaces it.  Plaintext email is supported on pretty much any device, with a bunch of different programs.  From the goals of Patchwork: "patchwork should supplement mailing lists, not replace them".&lt;/p&gt;
&lt;p&gt;Linus Torvalds famously explained why he prefers email over GitHub pull requests &lt;a href="https://github.com/torvalds/linux/pull/17"&gt;here&lt;/a&gt;, using &lt;a href="https://groups.google.com/forum/#!topic/linux.kernel/w957vpu3PPU"&gt;this&lt;/a&gt; pull request from Ben Herrenschmidt as an example of why git's own pull request format is superior to that of GitHub.  Damien Lespiau, who is working on the freedesktop Patchwork fork, &lt;a href="http://damien.lespiau.name/2016/02/augmenting-mailing-lists-with-patchwork.html"&gt;outlines on his blog&lt;/a&gt; all the issues he has with mailing list workflows and why he thinks mailing lists are a relic of the past.  His work on Patchwork has gone a long way to help fix those problems, however I don't think mailing lists are outdated and superceded, I think they are timeless.  They are a technology-agnostic, simple and free system that will still be around if GitHub dies or alienates its community.&lt;/p&gt;
&lt;p&gt;That said, there's still the case of the missing features.  What about automated testing?  What about developer feedback?  What about making a maintainer's life easier?  We've been working on improving these issues, and I'll outline how we're approaching them in a future post.&lt;/p&gt;</content><category term="development"></category><category term="education"></category><category term="kernel"></category><category term="patches"></category></entry></feed>